SubDomain Number,SubDomainName,question,option A,option B,option C,option D,option E,option F,answer,_multanswer,_stateNew,_stateWrong,_stateRight,_stateMastered,_countRight,_countWrong,_LabAssignment,_StudyRef,_AICoaching,_MarkedQuestions,_SubDomainNum,_SubDomainName, ,_OriginalNumber,_ExamNumber,_ExamName,_ImageURL,_Explanation
1.1,CloudTrail & GuardDuty,A company has an AWS account that hosts a production application. The company receives an email notiﬁcation that Amazon GuardDuty has detected an Impact:IAMUser/AnomalousBehavior ﬁnding in the account. A security engineer needs to run the investigation playbook for this security incident and must collect and analyze the information without affecting the application. Which solution will meet these requirements MOST quickly?,Log in to the AWS account by using read-only credentials. Review the GuardDuty ﬁnding for details about the IAM credentials that were used. Use the IAM console to add a DenyAll policy to the IAM principal.,Log in to the AWS account by using read-only credentials. Review the GuardDuty ﬁnding to determine which API calls initiated the ﬁnding. Use Amazon Detective to review the API calls in context.,Log in to the AWS account by using administrator credentials. Review the GuardDuty ﬁnding for details about the IAM credentials that were used. Use the IAM console to add a DenyAll policy to the IAM principal.,Log in to the AWS account by using read-only credentials. Review the GuardDuty ﬁnding to determine which API calls initiated the ﬁnding. Use AWS CloudTrail Insights and AWS CloudTrail Lake to review the API calls in context.,,,B,0,1,,,,0,0,,,,,1.2,Detection & Investigation,,9,SCS-C02,AWS Certified Security - Specialty,,"Using read-only access avoids making changes that could disrupt the application, while Amazon Detective is purpose-built to analyze GuardDuty findings and CloudTrail activity with entity graph context for rapid investigation. Options A and C involve modifying IAM (potentially impacting production), and D typically requires additional setup and querying in CloudTrail Lake, making it slower than Detective."
1.1,CloudTrail & GuardDuty,A company uses identity federation to authenticate users into an identity account (987654321987) where the users assume an IAM role named IdentityRole. The users then assume an IAM role named JobFunctionRole in the target AWS account (123456789123) to perform their job functions. A user is unable to assume the IAM role in the target account. The policy attached to the role in the identity account is: What should be done to enable the user to assume the appropriate role in the target account?,Update the IAM policy attached to the role in the identity account to be:,Update the trust policy on the role in the target account to be:,Update the trust policy on the role in the identity account to be:,Update the IAM policy attached to the role in the target account to be:,,,B,0,1,,,,0,0,,,,,4.2,Cross-Account & Federation,,34,SCS-C02,AWS Certified Security - Specialty,,"Cross-account role assumption requires the target role’s trust policy to allow the source role as a trusted principal. Since users assume IdentityRole in the identity account, the JobFunctionRole in the target account must trust arn:aws:iam::987654321987:role/IdentityRole; updating that trust policy enables sts:AssumeRole to succeed."
1.1,CloudTrail & GuardDuty,"A company recently had a security audit in which the auditors identiﬁed multiple potential threats. These potential threats can cause usage pattern changes such as DNS access peak, abnormal instance traﬃc, abnormal network interface traﬃc, and unusual Amazon S3 API calls. The threats can come from different sources and can occur at any time. The company needs to implement a solution to continuously monitor its system and identify all these incoming threats in near-real time. Which solution will meet these requirements?","Enable AWS CloudTrail logs, VPC ﬂow logs, and DNS logs. Use Amazon CloudWatch Logs to manage these logs from a centralized account.","Enable AWS CloudTrail logs, VPC ﬂow logs, and DNS logs. Use Amazon Macie to monitor these logs from a centralized account.","Enable Amazon GuardDuty from a centralized account. Use GuardDuty to manage AWS CloudTrail logs, VPC ﬂow logs, and DNS logs.","Enable Amazon Inspector from a centralized account. Use Amazon Inspector to manage AWS CloudTrail logs, VPC ﬂow logs, and DNS logs.",,,C,0,1,,,,0,0,,,,,2.3,Monitoring & Detection,,40,SCS-C02,AWS Certified Security - Specialty,,"Amazon GuardDuty is a managed threat detection service that continuously analyzes AWS CloudTrail events, VPC Flow Logs, and DNS logs to detect suspicious activity in near real time. It provides centralized, account-wide monitoring and alerts for anomalies like unusual S3 API calls and abnormal traffic patterns. Other options either only aggregate logs (CloudWatch Logs) or focus on data classification (Macie) or vulnerability scanning (Inspector), not continuous threat detection."
1.1,CloudTrail & GuardDuty,"A security engineer logs in to the AWS Lambda console with administrator permissions. The security engineer is trying to view logs in Amazon CloudWatch for a Lambda function that is named myFunction. When the security engineer chooses the option in the Lambda console to view logs in CloudWatch, an ""error loading Log Streams"" message appears. The IAM policy for the Lambda function's execution role contains the following: How should the security engineer correct the error?",Move the logs:CreateLogGroup action to the second Allow statement.,Add the logs:PutDestination action to the second Allow statement.,Add the logs:GetLogEvents action to the second Allow statement.,Add the logs:CreateLogStream action to the second Allow statement.,,,D,0,1,,,,0,0,,,,,4.1,Authentication & Authorization,,59,SCS-C02,AWS Certified Security - Specialty,,"Lambda’s execution role must be able to create a log stream in the function’s CloudWatch Logs group; without logs:CreateLogStream, the function can’t write logs, leading the console to show an error when loading log streams. The engineer already has admin permissions to read logs, so adding read actions isn’t the fix. Therefore, adding logs:CreateLogStream to the execution role resolves the issue."
1.1,CloudTrail & GuardDuty,A security engineer must use AWS Key Management Service (AWS KMS) to design a key management solution for a set of Amazon Elastic Block Store (Amazon EBS) volumes that contain sensitive data. The solution needs to ensure that the key material automatically expires in 90 days. Which solution meets these criteria?,A customer managed key that uses customer provided key material,A customer managed key that uses AWS provided key material,An AWS managed key,Operating system encryption that uses GnuPG,,,A,0,1,,,,0,0,,,,,5.3,Key Management,,84,SCS-C02,AWS Certified Security - Specialty,,"Only a customer managed KMS key with imported (customer provided) key material supports setting an expiration time, allowing the key material to automatically expire after 90 days. AWS-provided key material (customer managed or AWS managed) cannot be configured to expire—rotation is annual and does not render old key material unusable. GnuPG is outside AWS KMS and not suitable for EBS encryption integration."
1.1,CloudTrail & GuardDuty,A company's security engineer is developing an incident response plan to detect suspicious activity in an AWS account for VPC hosted resources. The security engineer needs to provide visibility for as many AWS Regions as possible. Which combination of steps will meet these requirements MOST cost-effectively? (Choose two.),Turn on VPC Flow Logs for all VPCs in the account.,Activate Amazon GuardDuty across all AWS Regions.,Activate Amazon Detective across all AWS Regions.,Create an Amazon Simple Notiﬁcation Service (Amazon SNS) topic. Create an Amazon EventBridge rule that responds to ﬁndings and publishes the ﬁndings to the SNS topic.,Create an AWS Lambda function. Create an Amazon EventBridge rule that invokes the Lambda function to publish ﬁndings to Amazon Simple Email Service (Amazon SES).,,"B, D",1,0,1,0,0,0,2,,,,,1.1,Preparation & Runbooks,,90,SCS-C02,AWS Certified Security - Specialty,,"Enabling Amazon GuardDuty across all Regions provides managed, cost‑effective threat detection for VPC‑hosted resources using existing telemetry (CloudTrail, VPC Flow Logs, DNS) and scales globally. Using EventBridge to route GuardDuty findings to an SNS topic delivers near‑real‑time, low‑cost notifications without custom code. VPC Flow Logs (A) and Detective (C) add cost and are not required for initial detection, and Lambda/SES (E) is more complex and costly than SNS."
1.1,CloudTrail & GuardDuty,"A company is using AWS WAF to protect a customized public API service that is based on Amazon EC instances. The API uses an Application Load Balancer. The AWS WAF web ACL is conﬁgured with an AWS Managed Rules rule group. After a software upgrade to the API and the client application, some types of requests are no longer working and are causing application stability issues. A security engineer discovers that AWS WAF logging is not turned on for the web ACL. The security engineer needs to immediately return the application to service, resolve the issue, and ensure that logging is not turned off in the future. The security engineer turns on logging for the web ACL and speciﬁes Amazon CloudWatch Logs as the destination. Which additional set of steps should the security engineer take to meet the requirements?",Edit the rules in the web ACL to include rules with Count actions. Review the logs to determine which rule is blocking the request. Modify the IAM policy of all AWS WAF administrators so that they cannot remove the logging conﬁguration for any AWS WAF web ACLs.,Edit the rules in the web ACL to include rules with Count actions. Review the logs to determine which rule is blocking the request. Modify the AWS WAF resource policy so that AWS WAF administrators cannot remove the logging conﬁguration for any AWS WAF web ACLs.,Edit the rules in the web ACL to include rules with Count and Challenge actions. Review the logs to determine which rule is blocking the request. Modify the AWS WAF resource policy so that AWS WAF administrators cannot remove the logging conﬁguration for any AWS WAF web ACLs.,Edit the rules in the web ACL to include rules with Count and Challenge actions. Review the logs to determine which rule is blocking the request. Modify the IAM policy of all AWS WAF administrators so that they cannot remove the logging conﬁguration for any AWS WAF web ACLs.,,,A,0,1,,,,0,0,,,,,2.1,Logging Configuration,,110,SCS-C02,AWS Certified Security - Specialty,,"Switching suspect rules to Count lets legitimate traffic pass immediately while still recording matches, so you can identify the offending rule from the CloudWatch Logs without causing outages. The Challenge action is unnecessary and could still impede clients. To ensure logging can’t be turned off, restrict administrators’ IAM permissions (e.g., deny wafv2:DeleteLoggingConfiguration), which is enforced via IAM, not a WAF resource policy."
1.1,CloudTrail & GuardDuty,"A company has many member accounts in an organization in AWS Organizations. The company is concerned about the potential for misuse of the AWS account root user credentials for member accounts in the organization. To address this potential misuse, the company wants to ensure that even if the account root user credentials are compromised the account is still protected. Which solution will meet this requirement?",Block service access by using SCPs for the root user,Remove the password for the root user,Delete access keys for the root user,Create an Amazon EventBridge rule to detect any AWS account root user API events,,,A,0,1,,,,0,0,,,,,4.1,Authentication & Authorization,,124,SCS-C02,AWS Certified Security - Specialty,,"SCPs in AWS Organizations define the maximum permissions for all principals in member accounts, including the root user, so you can explicitly deny actions and effectively neutralize a compromised root account. This provides preventative control, whereas removing passwords/keys can be reversed or incomplete, and EventBridge rules only detect, not block, misuse."
1.1,CloudTrail & GuardDuty,"A company deploys a distributed web application on a ﬂeet of Amazon EC2 instances. The ﬂeet is behind an Application Load Balancer (ALB) that will be conﬁgured to terminate the TLS connection. All TLS traﬃc to the ALB must stay secure, even if the certiﬁcate private key is compromised. How can a security engineer meet this requirement?",Create an HTTPS listener that uses a certiﬁcate that is managed by AWS Certiﬁcate Manager (ACM).,Create an HTTPS listener that uses a security policy that uses a cipher suite with perfect forward secrecy (PFS).,Create an HTTPS listener that uses the Server Order Preference security feature.,Create a TCP listener that uses a custom security policy that allows only cipher suites with perfect forward secrecy (PFS).,,,B,0,1,,,,0,0,,,,,5.1,Encryption at Rest,,129,SCS-C02,AWS Certified Security - Specialty,,"Using a security policy with perfect forward secrecy (PFS) ensures each session uses ephemeral keys that are not derived from the server’s private key, so past traffic remains secure even if the private key is compromised. ACM or Server Order Preference do not guarantee PFS, and a TCP listener would not terminate TLS at the ALB or apply TLS security policies."
1.1,CloudTrail & GuardDuty,A company uses an organization in AWS Organizations to manage hundreds of AWS accounts. Some of the accounts provide access to external AWS principals through cross-account IAM roles and Amazon S3 bucket policies. The company needs to identify which external principals have access to which accounts. Which solution will provide this information?,Enable AWS Identity and Access Management Access Analyzer for the organization. Conﬁgure the organization as a zone of trust. Filter ﬁndings by AWS account ID.,Create a custom AWS Conﬁg rule to monitor IAM roles in each account. Deploy an AWS Conﬁg aggregator to a central account. Filter ﬁndings by AWS account ID.,Activate Amazon Inspector. Integrate Amazon Inspector with AWS Security Hub. Filter ﬁndings by AWS account ID for the IAM role resource type and the S3 bucket policy resource type.,Conﬁgure the organization to use Amazon GuardDuty. Filter ﬁndings by AWS account ID for the Discovery:IAMUser/AnomalousBehavior ﬁnding type.,,,A,0,1,,,,0,0,,,,,4.2,Cross-Account & Federation,,157,SCS-C02,AWS Certified Security - Specialty,,"IAM Access Analyzer can be enabled at the organization level with the org as a zone of trust, automatically analyzing IAM roles and S3 bucket policies across all accounts to surface findings where resources are shared with external principals. You can then filter findings by AWS account ID to see which external principals have access to which accounts. The other options do not enumerate external access paths from policies (Inspector/GuardDuty) or require custom, less targeted implementations (Config)."
1.1,CloudTrail & GuardDuty,"A company has a batch-processing system that uses Amazon S3, Amazon EC2, and AWS Key Management Service (AWS KMS). The system uses two AWS accounts: Account A and Account B. Account A hosts an S3 bucket that stores the objects that will be processed. The S3 bucket also stores the results of the processing. All the S3 bucket objects are encrypted by a KMS key that is managed in Account A. Account B hosts a VPC that has a ﬂeet of EC2 instances that access the S3 bucket in Account A by using statements in the bucket policy. The VPC was created with DNS hostnames enabled and DNS resolution enabled. A security engineer needs to update the design of the system without changing any of the system's code. No AWS API calls from the batch- processing EC2 instances can travel over the internet. Which combination of steps will meet these requirements? (Choose two.)","In the Account B VPC, create a gateway VPC endpoint for Amazon S3. For the gateway VPC endpoint, create a resource policy that allows the s3:GetObject, s3:ListBucket, s3:PutObject, and s3:PutObjectAcl actions for the S3 bucket.","In the Account B VPC, create an interface VPC endpoint for Amazon S3. For the interface VPC endpoint, create a resource policy that allows the s3:GetObject, s3:ListBucket, s3:PutObject, and s3:PutObjectAcl actions for the S3 bucket.","In the Account B VPC, create an interface VPC endpoint for AWS KMS. For the interface VPC endpoint, create a resource policy that allows the kms:Encrypt, kms:Decrypt, and kms:GenerateDataKey actions for the KMS key. Ensure that private DNS is turned on for the endpoint.","In the Account B VPC, create an interface VPC endpoint for AWS KMS. For the interface VPC endpoint, create a resource policy that allows the kms:Encrypt, kms:Decrypt, and kms:GenerateDataKey actions for the KMS key. Ensure that private DNS is turned off for the endpoint.","In the Account B VPC, verify that the S3 bucket policy allows the s3:PutObjectAcl action for cross-account use. In the Account B VPC, create a gateway VPC endpoint for Amazon S3. For the gateway VPC endpoint, create a resource policy that allows the s3:GetObject, s3:ListBucket, and s3:PutObject actions for the S3 bucket.",,"A, C",1,1,,,,0,0,,,,,4.2,Cross-Account & Federation,,176,SCS-C02,AWS Certified Security - Specialty,,"A uses an S3 gateway VPC endpoint (the correct endpoint type for S3 data) so all S3 traffic stays on the AWS network, and its endpoint policy explicitly allows the needed S3 actions, including PutObjectAcl, avoiding endpoint-policy filtering issues. C creates a KMS interface VPC endpoint with private DNS so KMS encryption/decryption/data key calls from the EC2 instances stay private and are permitted by the endpoint policy; an S3 interface endpoint is not required for standard S3 access."
1.1,CloudTrail & GuardDuty,"A company’s security engineer wants to receive an email alert whenever Amazon GuardDuty, AWS Identity and Access Management Access Analyzer, or Amazon Macie generate a high-severity security ﬁnding. The company uses AWS Control Tower to govern all of its accounts. The company also uses AWS Security Hub with all of the AWS service integrations turned on. Which solution will meet these requirements with the LEAST operational overhead?","Set up separate AWS Lambda functions for GuardDuty, IAM Access Analyzer, and Macie to call each service's public API to retrieve high- severity ﬁndings. Use Amazon Simple Notiﬁcation Service (Amazon SNS) to send the email alerts. Create an Amazon EventBridge rule to invoke the functions on a schedule.",Create an Amazon EventBridge rule with a pattern that matches Security Hub ﬁndings events with high severity. Conﬁgure the rule to send the ﬁndings to a target Amazon Simple Notiﬁcation Service (Amazon SNS) topic. Subscribe the desired email addresses to the SNS topic.,Create an Amazon EventBridge rule with a pattern that matches AWS Control Tower events with high severity. Conﬁgure the rule to send the ﬁndings to a target Amazon Simple Notiﬁcation Service (Amazon SNS) topic. Subscribe the desired email addresses to the SNS topic.,"Host an application on Amazon EC2 to call the GuardDuty. IAM Access Analyzer, and Macie APIs. Within the application, use the Amazon Simple Notiﬁcation Service (Amazon SNS) API to retrieve high-severity ﬁndings and to send the ﬁndings to an SNS topic. Subscribe the desired email addresses to the SNS topic.",,,B,0,1,,,,0,0,,,,,2.4,Alerting & Remediation,,199,SCS-C02,AWS Certified Security - Specialty,,"Security Hub already aggregates findings from GuardDuty, IAM Access Analyzer, and Macie, so filtering for high-severity findings at the Security Hub event level in EventBridge and routing to SNS requires no custom code. This approach is fully managed, scalable, and minimizes operational overhead compared to building Lambda/EC2 collectors or using Control Tower events, which are not the source of these findings."
1.1,CloudTrail & GuardDuty,A security engineer needs to analyze Apache web server access logs that are stored in an Amazon S3 bucket. Amazon EC2 instance web servers generated the logs. The EC2 instances have the Amazon CloudWatch agent installed and conﬁgured to report their access logs. The security engineer needs to use a query in Amazon Athena to analyze the logs. The query must identify IP addresses that have attempted and failed to access restricted web server content held at the /admin URL path. The query also must identify the URLs that the IP addresses attempted to access. Which query will meet these requirements?,"SELECT client_ip, client_request FROM logs WHERE client_request LIKE '%/admin%!’ AND server_status = '403’",SELECT client_ip FROM logs WHERE client_request CONTAINS '%/admin%’ AND server_status = '401' GROUP BY client_ip,"SELECT DISTINCT (client_ip), client_request, client_id FROM logs WHERE server status = ‘403’ LIMIT 1000","SELECT DISTINCT (client_ip), client_request FROM logs WHERE user_id <> ‘admin’ AND server_status = ‘401!’",,,A,0,1,,,,0,0,,,,,2.3,Monitoring & Detection,,217,SCS-C02,AWS Certified Security - Specialty,,"Option A filters for the /admin path and 403 (Forbidden) responses, which indicate failed attempts to access restricted content. It returns both client_ip and client_request, providing the source IPs and the exact URLs attempted. Other options use the wrong status code, invalid functions, or don’t include the URL."
1.1,CloudTrail & GuardDuty,"A security engineer has designed a VPC to segment private traﬃc from public traﬃc. The VPC includes two Availability Zones. The security engineer has provisioned each Availability Zone with one private subnet and one public subnet. The security engineer has created three route tables for use with the environment. One route table is for the public subnets, and two route tables are for the private subnets (one route table for the private subnet in each Availability Zone). The security engineer discovers that all four subnets are attempting to route traﬃc out through the internet gateway that is attached to the VPC. Which combination of steps should the security engineer take to remediate this scenario? (Choose two.)",Verify that a NAT gateway has been provisioned in the public subnet in each Availability Zone.,Verify that a NAT gateway has been provisioned in the private subnet in each Availability Zone.,Modify the route tables that are associated with each of the public subnets. Create a new route for local destinations to the VPC CIDR range.,Modify the route tables that are associated with each of the private subnets. Create a new route for the destination 0.0.0.0/0. Specify the NAT gateway in the public subnet of the same Availability Zone as the target of the route.,Modify the route tables that are associated with each of the private subnets. Create a new route for the destination 0.0.0.0/0. Specify the internet gateway in the public subnet of the same Availability Zone as the target of the route.,,"A, D",1,1,,,,0,0,,,,,3.1,Network Architecture Security,,234,SCS-C02,AWS Certified Security - Specialty,,"Private subnets should not route directly to the internet gateway; they must send 0.0.0.0/0 traffic to a NAT gateway in a public subnet. A ensures NAT gateways exist in each AZ’s public subnet for high availability, and D updates the private route tables to target the NAT gateway in the same AZ. Options B and E are incorrect because NAT gateways belong in public subnets, and private subnets must not route to the internet gateway."
1.1,CloudTrail & GuardDuty,A company ﬁnds that one of its Amazon EC2 instances suddenly has a high CPU usage. The company does not know whether the EC2 instance is compromised or whether the operating system is performing background cleanup. Which combination of steps should a security engineer take before investigating the issue? (Choose three.),Disable termination protection for the EC2 instance if termination protection has not been disabled.,Enable termination protection for the EC2 instance if termination protection has not been enabled.,Take snapshots of the Amazon Elastic Block Store (Amazon EBS) data volumes that are attached to the EC2 instance.,Remove all snapshots of the Amazon Elastic Block Store (Amazon EBS) data volumes that are attached to the EC2 instance.,"Capture the EC2 instance metadata, and then tag the EC2 instance as under quarantine.",Immediately remove any entries in the EC2 instance metadata that contain sensitive information.,"B, C, E",1,1,,,,0,0,,,,,1.2,Detection & Investigation,,239,SCS-C02,AWS Certified Security - Specialty,,"Enable termination protection (B) to prevent accidental termination and preserve evidence during investigation. Take EBS snapshots (C) to capture immutable, point‑in‑time copies of the disks for forensic analysis. Capture instance metadata and tag the instance as quarantined (E) to preserve context (roles, user data, networking) and signal responders; deleting snapshots or altering metadata would risk destroying evidence."
1.1,CloudTrail & GuardDuty,"A company uses Amazon Elastic Kubernetes Service (Amazon EKS) clusters to run its Kubernetes-based applications. The company uses Amazon GuardDuty to protect the applications. EKS Protection is enabled in GuardDuty. However, the corresponding GuardDuty feature is not monitoring the Kubernetes-based applications. Which solution will cause GuardDuty to monitor the Kubernetes-based applications?",Enable VPC ﬂow logs for the VPC that hosts the EKS clusters.,Assign the CloudWatchEventsFullAccess AWS managed policy to the EKS clusters.,Ensure that the AmazonGuardDutyFullAccess AWS managed policy is attached to the GuardDuty service role.,Enable the control plane logs in Amazon EKS. Ensure that the logs are ingested into Amazon CloudWatch.,,,D,0,1,,,,0,0,,,,,3.4,Container & Serverless Security,,272,SCS-C02,AWS Certified Security - Specialty,,"GuardDuty’s EKS Protection (audit log monitoring) relies on Amazon EKS control plane audit logs being sent to CloudWatch. Without control plane logs, GuardDuty cannot analyze Kubernetes API activity, so enabling and ingesting these logs allows GuardDuty to monitor the Kubernetes-based applications. Other options (VPC flow logs, EventBridge permissions, or GuardDuty service role policy) do not enable EKS audit log visibility."
1.1,CloudTrail & GuardDuty,A security engineer needs to implement a solution to determine whether a company’s Amazon EC2 instances are being used to mine cryptocurrency. The solution must provide notiﬁcations of cryptocurrency-related activity to an Amazon Simple Notiﬁcation Service (Amazon SNS) topic. Which solution will meet these requirements?,Create AWS Conﬁg custom rules by using Guard custom policy. Conﬁgure the AWS Conﬁg rules to detect when an EC2 instance queries a DNS domain name that is associated with cryptocurrency-related activity. Conﬁgure AWS Conﬁg to initiate alerts to the SNS topic.,Enable Amazon GuardDuty. Create an Amazon EventBridge rule to send alerts to the SNS topic when GuardDuty creates a ﬁnding that is associated with cryptocurrency-related activity.,Enable Amazon Inspector. Create an Amazon EventBridge rule to send alerts to the SNS topic when Amazon Inspector creates a ﬁnding that is associated with cryRtocurrency-related activity.,Enable VPC ﬂow logs. Send the ﬂow logs to an Amazon S3 bucket. Set up a query in Amazon Athena to detect when an EC2 instance queries a DNS domain name that is associated with cryptocurrency-related activity. Conﬁgure the Athena query to initiate alerts to the SNS topic.,,,B,0,1,,,,0,0,,,,,2.3,Monitoring & Detection,,289,SCS-C02,AWS Certified Security - Specialty,,"Amazon GuardDuty has built-in threat detections for cryptocurrency mining on EC2 (e.g., suspicious DNS, C2 traffic) and produces findings that can be routed to Amazon SNS via EventBridge. The other options either don’t detect this runtime behavior (AWS Config, Amazon Inspector) or require custom analytics and don’t provide DNS query visibility from VPC Flow Logs."
1.2,Security Hub & Detective,"A company is migrating one of its legacy systems from an on-premises data center to AWS. The application server will run on AWS, but the database must remain in the on-premises data center for compliance reasons. The database is sensitive to network latency. Additionally, the data that travels between the on-premises data center and AWS must have IPsec encryption. Which combination of AWS solutions will meet these requirements? (Choose two.)",AWS Site-to-Site VPN,AWS Direct Connect,AWS VPN CloudHub,VPC peering,NAT gateway,,"A, B",1,1,,,,0,0,,,,,3.1,Network Architecture Security,,5,SCS-C02,AWS Certified Security - Specialty,,"Use AWS Direct Connect to provide a dedicated, low-latency, high-bandwidth connection between the on-premises data center and AWS. Add an AWS Site-to-Site VPN to provide IPsec encryption (which can run over Direct Connect), meeting the encryption requirement; other options (VPC peering, VPN CloudHub, NAT gateway) do not address both latency and encryption for on-prem to AWS connectivity."
1.2,Security Hub & Detective,"A company that uses AWS Organizations is using AWS IAM Identity Center (AWS Single Sign-On) to administer access to AWS accounts. A security engineer is creating a custom permission set in IAM Identity Center. The company will use the permission set across multiple accounts. An AWS managed policy and a customer managed policy are attached to the permission set. The security engineer has full administrative permissions and is operating in the management account. When the security engineer attempts to assign the permission set to an IAM Identity Center user who has access to multiple accounts, the assignment fails. What should the security engineer do to resolve this failure?",Create the customer managed policy in every account where the permission set is assigned. Give the customer managed policy the same name and same permissions in each account.,Remove either the AWS managed policy or the customer managed policy from the permission set. Create a second permission set that includes the removed policy. Apply the permission sets separately to the user.,Evaluate the logic of the AWS managed policy and the customer managed policy. Resolve any policy conﬂicts in the permission set before deployment.,"Do not add the new permission set to the user. Instead, edit the user's existing permission set to include the AWS managed policy and the customer managed policy.",,,A,0,1,,,,0,0,,,,,4.1,Authentication & Authorization,,41,SCS-C02,AWS Certified Security - Specialty,,"IAM customer managed policies are account-scoped and cannot be attached across accounts. When IAM Identity Center provisions the role in target accounts, the referenced customer managed policy must already exist in each target account (same name/path), otherwise the assignment fails. AWS managed policies work globally, but customer managed ones must be created per account."
1.2,Security Hub & Detective,A company is using AWS to run a long-running analysis process on data that is stored in Amazon S3 buckets. The process runs on a ﬂeet of Amazon EC2 instances that are in an Auto Scaling group. The EC2 instances are deployed in a private subnet of a VPC that does not have internet access. The EC2 instances and the S3 buckets are in the same AWS account. The EC2 instances access the S3 buckets through an S3 gateway endpoint that has the default access policy. Each EC2 instance is associated with an instance proﬁle role that has a policy that explicitly allows the s3:GetObject action and the s3:PutObject action for only the required S3 buckets. The company learns that one or more of the EC2 instances are compromised and are exﬁltrating data to an S3 bucket that is outside the company's organization in AWS Organizations. A security engineer must implement a solution to stop this exﬁltration of data and to keep the EC2 processing job functional. Which solution will meet these requirements?,Update the policy on the S3 gateway endpoint to allow the S3 actions only if the values of the aws:ResourceOrgID and aws:PrincipalOrgID condition keys match the company's values.,Update the policy on the instance proﬁle role to allow the S3 actions only if the value of the aws:ResourceOrgID condition key matches the company's value.,Add a network ACL rule to the subnet of the EC2 instances to block outgoing connections on port 443.,Apply an SCP on the AWS account to allow the S3 actions only if the values of the aws:ResourceOrgID and aws:PrincipalOrgID condition keys match the company's values.,,,A,0,1,,,,0,0,,,,,4.1,Authentication & Authorization,,55,SCS-C02,AWS Certified Security - Specialty,,"Because the instances reach S3 only via the VPC gateway endpoint, the endpoint policy is the correct choke point to control which S3 buckets can be accessed. Adding aws:ResourceOrgID and aws:PrincipalOrgID conditions to the endpoint policy restricts access to buckets and principals within the company’s organization, stopping exfiltration to external buckets while preserving needed internal S3 access. The other options either don’t apply effectively (identity policy/SCP) or would break all S3 access (NACL on port 443)."
1.2,Security Hub & Detective,A company stores images for a website in an Amazon S3 bucket. The company is using Amazon CloudFront to serve the images to end users. The company recently discovered that the images are being accessed from countries where the company does not have a distribution license. Which actions should the company take to secure the images to limit their distribution? (Choose two.),Update the S3 bucket policy to restrict access to a CloudFront origin access control (OAC).,Update the website DNS record to use an Amazon Route 53 geolocation record deny list of countries where the company lacks a license.,Add a CloudFront geo restriction deny list of countries where the company lacks a license.,Update the S3 bucket policy with a deny list of countries where the company lacks a license.,Enable the Restrict Viewer Access option in CloudFront to create a deny list of countries where the company lacks a license.,,"A, C",1,1,,,,0,0,,,,,3.2,Edge & Web Protection,,91,SCS-C02,AWS Certified Security - Specialty,,"Use CloudFront geo restriction to deny requests from specific countries at the edge so users in unlicensed regions cannot access the content. Configure S3 access via a CloudFront origin access control (OAC) so objects can only be retrieved through CloudFront, preventing direct S3 URL access that would bypass geo restrictions."
1.2,Security Hub & Detective,A company has AWS accounts that are in an organization in AWS Organizations. An Amazon S3 bucket in one of the accounts is publicly accessible. A security engineer must change the conﬁguration so that the S3 bucket is no longer publicly accessible. The security engineer also must ensure that the S3 bucket cannot be made publicly accessible in the future. Which solution will meet these requirements?,Conﬁgure the S3 bucket to use an AWS Key Management Service (AWS KMS) key. Encrypt all objects in the S3 bucket by creating a bucket policy that enforces encryption. Conﬁgure an SCP to deny the s3:GetObject action for the OU that contains the AWS account.,Enable the PublicAccessBlock conﬁguration on the S3 bucket. Conﬁgure an SCP to deny the s3:GetObject action for the OU that contains the AWS account.,Enable the PublicAccessBlock conﬁguration on the S3 bucket. Conﬁgure an SCP to deny the s3:PutPublicAccessBlock action for the OU that contains the AWS account.,Conﬁgure the S3 bucket to use S3 Object Lock in governance mode. Conﬁgure an SCP to deny the s3:PutPublicAccessBlock action for the OU that contains the AWS account.,,,C,0,1,,,,0,0,,,,,4.1,Authentication & Authorization,,119,SCS-C02,AWS Certified Security - Specialty,,"Enabling S3 Block Public Access on the bucket blocks public ACLs and bucket policies, making the bucket no longer publicly accessible. An SCP that denies s3:PutPublicAccessBlock prevents anyone from changing or disabling that protection, ensuring it cannot be made public in the future. Other options either don’t address public exposure (KMS/Object Lock) or only block reads without preventing public configuration."
1.2,Security Hub & Detective,"A company uses Amazon Elastic Container Service (Amazon ECS) containers that have the Fargate launch type. The containers run web and mobile applications that are written in Java and Node.js. To meet network segmentation requirements, each of the company’s business units deploys applications in its own dedicated AWS account. Each business unit stores container images in an Amazon Elastic Container Registry (Amazon ECR) private registry in its own account. A security engineer must recommend a solution to scan ECS containers and ECR registries for vulnerabilities in operating systems and programming language libraries. The company’s audit team must be able to identify potential vulnerabilities that exist in any of the accounts where applications are deployed. Which solution will meet these requirements?","In each account, update the ECR registry to use Amazon Inspector instead of the default scanning service. Conﬁgure Amazon Inspector to forward vulnerability ﬁndings to AWS Security Hub in a central security account. Provide access for the audit team to use Security Hub to review the ﬁndings.","In each account, conﬁgure AWS Conﬁg to monitor the conﬁguration of the ECS containers and the ECR registry. Conﬁgure AWS Conﬁg conformance packs for vulnerability scanning. Create an AWS Conﬁg aggregator in a central account to collect conﬁguration and compliance details from all accounts. Provide the audit team with access to AWS Conﬁg in the account where the aggregator is conﬁgured.","In each account, conﬁgure AWS Audit Manager to scan the ECS containers and the ECR registry. Conﬁgure Audit Manager to forward vulnerability ﬁndings to AWS Security Hub in a central security account. Provide access for the audit team to use Security Hub to review the ﬁndings.","In each account, conﬁgure Amazon GuardDuty to scan the ECS containers and the ECR registry. Conﬁgure GuardDuty to forward vulnerability ﬁndings to AWS Security Hub in a central security account. Provide access for the audit team to use Security Hub to review the ﬁndings.",,,A,0,1,,,,0,0,,,,,3.4,Container & Serverless Security,,126,SCS-C02,AWS Certified Security - Specialty,,"Amazon Inspector provides enhanced ECR image scanning and container vulnerability detection, including OS and application dependency (e.g., Java, Node.js) vulnerabilities, and can be enabled instead of ECR’s basic scanner. It natively integrates with AWS Security Hub for multi-account aggregation, allowing a central security account to provide the audit team visibility. AWS Config, Audit Manager, and GuardDuty do not perform container image vulnerability scanning."
1.2,Security Hub & Detective,A company is using an AWS Key Management Service (AWS KMS) AWS owned key in its application to encrypt ﬁles in an AWS account. The company's security team wants the ability to change to new key material for new ﬁles whenever a potential key breach occurs. A security engineer must implement a solution that gives the security team the ability to change the key whenever the team wants to do so. Which solution will meet these requirements?,Create a new customer managed key. Add a key rotation schedule to the key. Invoke the key rotation schedule every time the security team requests a key change.,Create a new AWS managed key. Add a key rotation schedule to the key. Invoke the key rotation schedule every time the security team requests a key change.,Create a key alias. Create a new customer managed key every time the security team requests a key change. Associate the alias with the new key.,Create a key alias. Create a new AWS managed key every time the security team requests a key change. Associate the alias with the new key.,,,C,0,1,,,,0,0,,,,,5.3,Key Management,,138,SCS-C02,AWS Certified Security - Specialty,,"Customer managed keys allow you to create new keys on demand and control which key the application uses via a stable KMS alias. Rotating to a new CMK and repointing the alias ensures new files use the new key while old files remain decryptable; AWS owned/managed keys can’t be rotated on demand, and KMS rotation schedules can’t be invoked manually."
1.2,Security Hub & Detective,A security administrator has enabled AWS Security Hub for all the AWS accounts in an organization in AWS Organizations. The security team wants near-real-time response and remediation for deployed AWS resources that do not meet security standards. All changes must be centrally logged for auditing purposes. The organization has reached the quotas for the number of SCPs attached to an OU and SCP document size. The team wants to avoid making any changes to any of the SCPs. The solution must maximize scalability and cost-effectiveness. Which combination of actions should the security administrator take to meet these requirements? (Choose three.),Create an AWS Conﬁg custom rule to detect conﬁguration changes to AWS resources. Create an AWS Lambda function to remediate the AWS resources in the delegated administrator AWS account.,Use AWS Systems Manager Change Manager to track conﬁguration changes to AWS resources. Create a Systems Manager document to remediate the AWS resources in the delegated administrator AWS account.,Create a Security Hub custom action to reference in an Amazon EventBridge event rule in the delegated administrator AWS account.,Create an Amazon EventBridge event rule to Invoke an AWS Lambda function that will take action on AWS resources.,Create an Amazon EventBridge event rule to invoke an AWS Lambda function that will evaluate AWS resource conﬁguration for a set of API requests and create a ﬁnding for noncompllant AWS resources.,Create an Amazon EventBridge event rule to invoke an AWS Lambda function on a schedule to assess speciﬁc AWS Conﬁg rules.,"A, C, D",1,1,,,,0,0,,,,,2.1,Logging Configuration,,166,SCS-C02,AWS Certified Security - Specialty,,"AWS Config custom rules (A) provide near-real-time detection of noncompliant resource changes, and Security Hub custom actions (C) can be used with EventBridge rules to route findings in the delegated admin account. EventBridge invoking Lambda (D) enables automated, scalable remediation with centralized logging in CloudWatch for auditing, avoiding any SCP changes. Other options either are not near-real-time or duplicate native capabilities, reducing scalability and cost-effectiveness."
1.2,Security Hub & Detective,A company suspects that an attacker has exploited an overly permissive role to export credentials from Amazon EC2 instance metadata. The company uses Amazon GuardDuty and AWS Audit Manager. The company has enabled AWS CloudTrail logging and Amazon CloudWatch logging for all of its AWS accounts. A security engineer must determine if the credentials were used to access the company's resources from an external account. Which solution will provide this information?,Review GuardDuty ﬁndings to ﬁnd InstanceCredentialExﬁltration events.,Review assessment reports in the Audit Manager console to ﬁnd InstanceCredentialExﬁltration events.,Review CloudTrail logs for GetSessionToken API calls to AWS Security Token Service (AWS STS) that come from an account ID from outside the company.,Review CloudWatch logs for GetSessionToken API calls to AWS Security Token Service (AWS STS) that come from an account ID from outside the company.,,,A,0,1,,,,0,0,,,,,2.3,Monitoring & Detection,,173,SCS-C02,AWS Certified Security - Specialty,,"Amazon GuardDuty specifically detects and generates InstanceCredentialExfiltration findings when credentials obtained from EC2 instance metadata are used from an external IP or account. This directly answers whether stolen instance role credentials were used outside the company’s environment. Audit Manager is for compliance, and searching CloudTrail/CloudWatch for GetSessionToken would miss typical exfiltration scenarios that use temporary role credentials (e.g., AssumeRole), not STS GetSessionToken."
1.2,Security Hub & Detective,"A company uses HTTP Live Streaming (HLS) to stream live video content to paying subscribers by using Amazon CloudFront. HLS splits the video content into chunks so that the user can request the right chunk based on different conditions. Because the video events last for several hours, the total video is made up of thousands of chunks. The origin URL is not disclosed, and every user is forced to access the CloudFront URL. The company has a web application that authenticates the paying users against an internal repository and a CloudFront key pair that is already issued. What is the simplest and MOST effective way to protect the content?",Develop the application to use the CloudFront key pair to create signed URLs that users will use to access the content.,Develop the application to use the CloudFront key pair to set the signed cookies that users will use to access the content.,Develop the application to issue a security token that Lambda@Edge will receive to authenticate and authorize access to the content.,"Keep the CloudFront URL encrypted inside the application, and use AWS KMS to resolve the URL on-the-ﬂy after the user is authenticated.",,,B,0,1,,,,0,0,,,,,4.1,Authentication & Authorization,,189,SCS-C02,AWS Certified Security - Specialty,,"HLS generates thousands of small segment files; signed cookies let you authorize access to many objects (e.g., a path/prefix) with a single policy, avoiding the need to create a signed URL for every segment. Since a CloudFront key pair already exists and origin is hidden, signed cookies are the simplest, scalable way to protect and deliver all chunks to authenticated users. Alternatives like Lambda@Edge or KMS add unnecessary complexity."
1.2,Security Hub & Detective,A company needs to securely deploy resources and workloads across AWS accounts. The accounts are in an organization in AWS Organizations. The company needs to use AWS CloudFormation for infrastructure as code (IaC) management of approved architectural patterns. The company also must enforce tagging requirements and speciﬁc guidelines for resource and workload conﬁguration and creation. Which solution will meet these requirements?,Use CloudFormation stack policies to prevent the creation of resources that do not meet the tagging or conﬁguration requirements. Use Amazon EventBridge rules to detect API calls that attempt to create resources outside of CloudFormation.,Use an AWS CodePipeline pipeline to test and deploy IaC deﬁned workloads through CloudFormation into the accounts. Use AWS Conﬁg rules to enforce the tagging requirements. Apply an SCP to prevent the creation of misconﬁgured resources in all OUs.,Create an IAM permissions boundary to prevent the creation of misconﬁgured resources through CloudFormation and to enforce the tagging requirements. Apply the permissions boundary to all account roles. Use AWS Conﬁg rules to identify existing resources that are in a misconﬁgured state.,Use AWS Service Catalog with CloudFormation to manage access to approved architecture conﬁgurations. Provision Service Catalog portfolios to the accounts across the organization. Use AWS Conﬁg rules to enforce the tagging requirements and other resource conﬁguration policies across accounts.,,,D,0,1,,,,0,0,,,,,4.2,Cross-Account & Federation,,220,SCS-C02,AWS Certified Security - Specialty,,"Service Catalog lets you centrally curate and distribute approved CloudFormation-based architectures (products/portfolios) across AWS Organizations, ensuring teams can only deploy standardized, vetted patterns via IaC. Pairing this with AWS Config rules enforces tagging and resource configuration policies organization-wide. Alternatives either don’t constrain deployments to approved patterns or cannot reliably enforce granular tag/config requirements."
1.2,Security Hub & Detective,"A company wants to start processing sensitive data on Amazon EC2 instances. The company will use Amazon CloudWatch Logs to monitor, store, and access log ﬁles from the EC2 instances. The company’s developers use CloudWatch Logs for troubleshooting. A security engineer must implement a solution that prevents the developers from viewing the sensitive data. The solution must automatically apply to any new log groups that are created in the account in the future. Which solution will meet these requirements?",Create a CloudWatch Logs account-wide data protection policy. Specify the appropriate data identiﬁers for the policy. Ensure that the developers do not have the logs:Unmask IAM permission.,Export the CloudWatch Logs data to an Amazon S3 bucket. Set up automated discovery by using Amazon Macie on the S3 bucket. Create a custom data identiﬁer for the sensitive data. Remove the developers’ access to CloudWatch Logs. Grant permissions for the developers to view the exported log data in Amazon S3.,Export the CloudWatch Logs data to an Amazon S3 bucket. Set up automated discovery by using Amazon Macie on the S3 bucket. Specify the appropriate managed data identiﬁers. Remove the developers’ access to CloudWatch Logs. Grant permissions for the developers to view the exported log data in Amazon S3.,Create a CloudWatch Logs data protection policy for each log group. Specify the appropriate data identiﬁers for the policy. Ensure that the developers do not have the logs:Unmask IAM permission.,,,A,0,1,,,,0,0,,,,,4.1,Authentication & Authorization,,229,SCS-C02,AWS Certified Security - Specialty,,"A CloudWatch Logs account-wide data protection policy automatically scans and masks sensitive data across all current and future log groups, meeting the requirement for automatic application. By ensuring developers lack the logs:Unmask permission, they cannot reveal masked data while still using CloudWatch Logs for troubleshooting. Other options either require per-log-group setup or rely on exporting to S3/Macie, which doesn’t satisfy the requirement to continue using CloudWatch Logs and auto-apply controls."
1.2,Security Hub & Detective,A consultant agency needs to perform a security audit for a company’s production AWS account. Several consultants need access to the account. The consultant agency already has its own AWS account. The company requires multi-factor authentication (MFA) for all access to its production account. The company also forbids the use of long- term credentials. Which solution will provide the consultant agency with access that meets these requirements?,Create an IAM group. Create an IAM user for each consultant. Add each user to the group. Turn on MFA for each consultant.,Conﬁgure Amazon Cognito on the company’s production account to authenticate against the consultant agency’s identity provider (IdP). Add MFA to a Cognito user pool.,"Create an IAM role in the consultant agency’s AWS account. Deﬁne a trust policy that requires MFA. In the trust policy, specify the company’s production account as the principal. Attach the trust policy to the role.","Create an IAM role in the company’s production account. Deﬁne a trust policy that requires MFA. In the trust policy, specify the consultant agency’s AWS account as the principal. Attach the trust policy to the role.",,,D,0,1,,,,0,0,,,,,4.1,Authentication & Authorization,,248,SCS-C02,AWS Certified Security - Specialty,,"Create a role in the company’s production account that trusts the consultant agency’s AWS account and requires MFA, so consultants can assume it with MFA to receive temporary STS credentials. This enforces MFA, avoids long‑term credentials, and cleanly enables cross‑account, least‑privilege access."
1.2,Security Hub & Detective,A company wants to automate the creation of a security report. The company has an AWS Lambda function that gathers data from Amazon Inspector ﬁndings stored in AWS Security Hub in the us-west-2 Region. The Lambda function then needs to create a daily report by using an Amazon EventBridge schedule. A security engineer discovers that the Lambda function is failing to create the report. The security engineer must implement a solution that corrects the issue and provides least privilege permissions. Which solution will meet these requirements?,Create a resource-based policy that allows Security Hub access to the ARN of the Lambda function.,Attach the AWSSecurityHubReadOnlyAccess AWS managed policy to the Lambda function’s execution role.,Grant the Lambda function’s execution role read-only permissions to access Amazon Inspector and Security Hub.,"Create a custom IAM policy that grants the Security Hub Get*, List*, Batch*, and Describe* permissions on the arn:aws:securityhub:us- west-2::product/aws/inspector/* resource. Attach the policy to the Lambda function’s execution role.",,,B,0,1,,,,0,0,,,,,4.1,Authentication & Authorization,,281,SCS-C02,AWS Certified Security - Specialty,,"The Lambda function only needs read-only access to Security Hub to fetch Inspector findings for the report. Attaching the AWSSecurityHubReadOnlyAccess managed policy to the function’s execution role provides the minimal Get/List/Describe permissions required. Other options either grant unnecessary services (C), misconfigure trust/resource policies (A), or use incorrect/incomplete resource scoping and actions (D)."
1.2,Security Hub & Detective,A company is investigating actions that an IAM role performed. The company must ﬁnd out when the role last accessed AWS Security Hub and when the role last used the DeleteInsight action in Security Hub. Which solution will provide this information?,Use the checks for the security category in AWS Trusted Advisor. Search for the role and examine the actions taken.,Use the Access Advisor tab in AWS Identity and Access Management (IAM). Search for Security Hub and the actions taken.,Use AWS Identity and Access Management (IAM) to generate a credential report. Search the report for Security Hub activity.,Create an analyzer in AWS Identity and Access Management Access Analyzer. Examine the ﬁndings for the role’s actions in Security Hub.,,,B,0,1,,,,0,0,,,,,2.3,Monitoring & Detection,,286,SCS-C02,AWS Certified Security - Specialty,,"The IAM Access Advisor tab shows service- and action-level last accessed information for an IAM role, letting you see when the role last used AWS Security Hub and specific API actions such as DeleteInsight. Other options (Trusted Advisor, credential reports, Access Analyzer) do not provide last-used timestamps for services or actions."
1.2,Security Hub & Detective,A security engineer has created an Amazon GuardDuty detector in several AWS accounts. The accounts are in an organization in AWS Organizations. The security engineer needs centralized visibility of the security ﬁndings from the detectors. Which solution will meet this requirement?,Conﬁgure Amazon CloudWatch Logs Insights.,Create an Amazon CloudWatch dashboard.,Conﬁgure AWS Security Hub integrations.,Query the ﬁndings by using Amazon Athena.,,,C,0,1,,,,0,0,,,,,2.2,Centralization & Retention,,298,SCS-C02,AWS Certified Security - Specialty,,"AWS Security Hub natively aggregates and centralizes security findings from services like GuardDuty across multiple accounts in an AWS Organization via a delegated administrator. This provides a single pane of glass for organization-wide visibility with minimal setup. CloudWatch or Athena would require custom exports and queries and do not offer native, cross-account security finding aggregation."
1.3,Macie & Inspector,A company has an AWS Lambda function that creates image thumbnails from larger images. The Lambda function needs read and write access to an Amazon S3 bucket in the same AWS account. Which solutions will provide the Lambda function this access? (Choose two.),Create an IAM user that has only programmatic access. Create a new access key pair. Add environmental variables to the Lambda function with the access key ID and secret access key. Modify the Lambda function to use the environmental variables at run time during communication with Amazon S3.,Generate an Amazon EC2 key pair. Store the private key in AWS Secrets Manager. Modify the Lambda function to retrieve the private key from Secrets Manager and to use the private key during communication with Amazon S3.,Create an IAM role for the Lambda function. Attach an IAM policy that allows access to the S3 bucket.,Create an IAM role for the Lambda function. Attach a bucket policy to the S3 bucket to allow access. Specify the function's IAM role as the principal.,Create a security group. Attach the security group to the Lambda function. Attach a bucket policy that allows access to the S3 bucket through the security group ID.,, ,1,1,,,,0,0,,,,,4.1,Authentication & Authorization,,1,SCS-C02,AWS Certified Security - Specialty,,"Lambda should access S3 using its execution role; attaching an IAM policy to the role (C) and/or adding a bucket policy that grants that role access (D) are the correct authorization patterns. Embedding long‑term keys (A) or using EC2 key pairs (B) is insecure/irrelevant, and security groups (E) are not used as principals for S3 access."
1.3,Macie & Inspector,A security engineer is designing an IAM policy to protect AWS API operations. The policy must enforce multi-factor authentication (MFA) for IAM users to access certain services in the AWS production account. Each session must remain valid for only 2 hours. The current version of the IAM policy is as follows: Which combination of conditions must the security engineer add to the IAM policy to meet these requirements? (Choose two.),"""Bool"": {""aws:MultiFactorAuthPresent"": ""true""}","""Bool"": {""aws:MultiFactorAuthPresent"": ""false""}","""NumericLessThan"": {""aws:MultiFactorAuthAge"": ""7200""}","""NumericGreaterThan"": {""aws:MultiFactorAuthAge"": ""7200""}","""NumericLessThan"": {""MaxSessionDuration"": ""7200""}",,"A, C",1,1,,,,0,0,,,,,4.1,Authentication & Authorization,,13,SCS-C02,AWS Certified Security - Specialty,,"A enforces that the request is made with MFA by requiring aws:MultiFactorAuthPresent to be true. C limits the session to 2 hours by allowing access only when aws:MultiFactorAuthAge is less than 7200 seconds (i.e., MFA was performed within the last 2 hours). The other options either disable MFA, require MFA older than 2 hours, or use an invalid condition key for this purpose."
1.3,Macie & Inspector,A company uses AWS Organizations to manage a multi-account AWS environment in a single AWS Region. The organization's management account is named management-01. The company has turned on AWS Conﬁg in all accounts in the organization. The company has designated an account named security-01 as the delegated administrator for AWS Conﬁg. All accounts report the compliance status of each account's rules to the AWS Conﬁg delegated administrator account by using an AWS Conﬁg aggregator. Each account administrator can conﬁgure and manage the account's own AWS Conﬁg rules to handle each account's unique compliance requirements. A security engineer needs to implement a solution to automatically deploy a set of 10 AWS Conﬁg rules to all existing and future AWS accounts in the organization. The solution must turn on AWS Conﬁg automatically during account creation. Which combination of steps will meet these requirements? (Choose two.),Create an AWS CloudFormation template that contains the 10 required AWS Conﬁg rules. Deploy the template by using CloudFormation StackSets in the security-01 account.,Create a conformance pack that contains the 10 required AWS Conﬁg rules. Deploy the conformance pack from the security-01 account.,Create a conformance pack that contains the 10 required AWS Conﬁg rules. Deploy the conformance pack from the management-01 account.,Create an AWS CloudFormation template that will activate AWS Conﬁg. Deploy the template by using CloudFormation StackSets in the security-01 account.,Create an AWS CloudFormation template that will activate AWS Conﬁg. Deploy the template by using CloudFormation StackSets in the management-01 account.,,"B, E",1,1,,,,0,0,,,,,6.1,Governance Frameworks,,20,SCS-C02,AWS Certified Security - Specialty,,"Use an organization conformance pack to centrally deploy the same 10 AWS Config rules across all existing and future accounts; the delegated administrator (security-01) can deploy this, satisfying the rule deployment requirement (B). To ensure AWS Config is enabled automatically in every new account, use CloudFormation StackSets with service-managed permissions from the management account (management-01), which supports automatic rollout to new org accounts (E)."
1.3,Macie & Inspector,"A company is expanding its group of stores. On the day that each new store opens, the company wants to launch a customized web application for that store. Each store's application will have a non-production environment and a production environment. Each environment will be deployed in a separate AWS account. The company uses AWS Organizations and has an OU that is used only for these accounts. The company distributes most of the development work to third-party development teams. A security engineer needs to ensure that each team follows the company's deployment plan for AWS resources. The security engineer also must limit access to the deployment plan to only the developers who need access. The security engineer already has created an AWS CloudFormation template that implements the deployment plan. What should the security engineer do next to meet the requirements in the MOST secure way?",Create an AWS Service Catalog portfolio in the organization's management account. Upload the CloudFormation template. Add the template to the portfolio's product list. Share the portfolio with the OU.,"Use the CloudFormation CLI to create a module from the CloudFormation template. Register the module as a private extension in the CloudFormation registry. Publish the extension. In the OU, create an SCP that allows access to the extension.",Create an AWS Service Catalog portfolio in the organization's management account. Upload the CloudFormation template. Add the template to the portfolio's product list. Create an IAM role that has a trust policy that allows cross-account access to the portfolio for users in the OU accounts. Attach the AWSServiceCatalogEndUserFullAccess managed policy to the role.,Use the CloudFormation CLI to create a module from the CloudFormation template. Register the module as a private extension in the CloudFormation registry. Publish the extension. Share the extension with the OU.,,,A,0,1,,,,0,0,,,,,4.1,Authentication & Authorization,,28,SCS-C02,AWS Certified Security - Specialty,,"AWS Service Catalog lets you centrally curate approved CloudFormation templates and share them with an OU, enforcing governance, versioning, and constraints while allowing account admins to grant access only to specific developers. Option A uses native portfolio sharing for least-privilege access, whereas Option C adds a broad end-user policy and cross-account role, and B/D (CloudFormation modules) don’t provide the same access control and deployment governance as Service Catalog."
1.3,Macie & Inspector,A company has a single AWS account and uses an Amazon EC2 instance to test application code. The company recently discovered that the instance was compromised. The instance was serving up malware. The analysis of the instance showed that the instance was compromised 35 days ago. A security engineer must implement a continuous monitoring solution that automatically notiﬁes the company's security team about compromised instances through an email distribution list for high severity ﬁndings. The security engineer must implement the solution as soon as possible. Which combination of steps should the security engineer take to meet these requirements? (Choose three.),Enable AWS Security Hub in the AWS account.,Enable Amazon GuardDuty in the AWS account.,Create an Amazon Simple Notiﬁcation Service (Amazon SNS) topic. Subscribe the security team's email distribution list to the topic.,Create an Amazon Simple Queue Service (Amazon SQS) queue. Subscribe the security team's email distribution list to the queue.,Create an Amazon EventBridge rule for GuardDuty ﬁndings of high severity. Conﬁgure the rule to publish a message to the topic.,Create an Amazon EventBridge rule for Security Hub ﬁndings of high severity. Conﬁgure the rule to publish a message to the queue.,"B, C, E",1,1,,,,0,0,,,,,2.3,Monitoring & Detection,,33,SCS-C02,AWS Certified Security - Specialty,,"GuardDuty provides continuous threat detection for resources like EC2 and identifies compromised instances, so enabling it is the fastest way to detect such events. SNS supports direct email notifications, so creating a topic and subscribing the security team’s distribution list enables alerts. An EventBridge rule can filter high-severity GuardDuty findings and publish them to the SNS topic for immediate notification; Security Hub and SQS are not required for this use case."
1.3,Macie & Inspector,"A company's security engineer has been tasked with restricting a contractor's IAM account access to the company’s Amazon EC2 console without providing access to any other AWS services. The contractor's IAM account must not be able to gain access to any other AWS service, even if the IAM account is assigned additional permissions based on IAM group membership. What should the security engineer do to meet these requirements?",Create an inline IAM user policy that allows for Amazon EC2 access for the contractor's IAM user.,Create an IAM permissions boundary policy that allows Amazon EC2 access. Associate the contractor's IAM account with the IAM permissions boundary policy.,Create an IAM group with an attached policy that allows for Amazon EC2 access. Associate the contractor's IAM account with the IAM group.,Create a IAM role that allows for EC2 and explicitly denies all other services. Instruct the contractor to always assume this role.,,,B,0,1,,,,0,0,,,,,4.1,Authentication & Authorization,,38,SCS-C02,AWS Certified Security - Specialty,,"An IAM permissions boundary sets the maximum permissions a user can ever have, regardless of any additional policies from groups or inline policies. By creating a boundary that only allows EC2, the contractor’s access is constrained to EC2 even if other permissions are later attached. Inline or group policies (A, C) could be expanded later, and a role (D) relies on user behavior and doesn’t restrict the user’s base credentials."
1.3,Macie & Inspector,A company has recently recovered from a security incident that required the restoration of Amazon EC2 instances from snapshots. The company uses an AWS Key Management Service (AWS KMS) customer managed key to encrypt all Amazon Elastic Block Store (Amazon EBS) snapshots. The company performs a gap analysis of its disaster recovery procedures and backup strategies. A security engineer needs to implement a solution so that the company can recover the EC2 instances if the AWS account is compromised and the EBS snapshots are deleted. Which solution will meet this requirement?,Create a new Amazon S3 bucket. Use EBS lifecycle policies to move EBS snapshots to the new S3 bucket. Use lifecycle policies to move snapshots to the S3 Glacier Instant Retrieval storage class. Use S3 Object Lock to prevent deletion of the snapshots.,Use AWS Systems Manager to distribute a conﬁguration that backs up all attached disks to Amazon S3.,Create a new AWS account that has limited privileges. Allow the new account to access the KMS key that encrypts the EBS snapshots. Copy the encrypted snapshots to the new account on a recurring basis.,Use AWS Backup to copy EBS snapshots to Amazon S3. Use S3 Object Lock to prevent deletion of the snapshots.,,,C,0,0,1,0,0,0,1,,,,,1.4,Post-Incident Activities,,45,SCS-C02,AWS Certified Security - Specialty,,"Creating cross-account copies of encrypted EBS snapshots provides isolation so that backups persist even if the source account is compromised and snapshots are deleted. Granting the destination account access to the KMS key (or re-encrypting on copy) ensures the snapshots remain usable for recovery. The S3-based options are invalid for EBS snapshots, and Systems Manager is not a resilient, supported method for EBS snapshot backups."
1.3,Macie & Inspector,A company's AWS CloudTrail logs are all centrally stored in an Amazon S3 bucket. The security team controls the company's AWS account. The security team must prevent unauthorized access and tampering of the CloudTrail logs. Which combination of steps should the security team take? (Choose three.),Conﬁgure server-side encryption with AWS KMS managed encryption keys (SSE-KMS).,Compress log ﬁles with secure gzip.,Create an Amazon EventBridge rule to notify the security team of any modiﬁcations on CloudTrail log ﬁles.,Implement least privilege access to the S3 bucket by conﬁguring a bucket policy.,Conﬁgure CloudTrail log ﬁle integrity validation.,Conﬁgure Access Analyzer for S3.,"A, D, E",1,1,,,,0,0,,,,,2.1,Logging Configuration,,51,SCS-C02,AWS Certified Security - Specialty,,ERR: Error: API error: upstream connect error or disconnect/reset before headers. reset reason: connection termination
1.3,Macie & Inspector,"An application is running on an Amazon EC2 instance that has an IAM role attached. The IAM role provides access to an AWS Key Management Service (AWS KMS) customer managed key and an Amazon S3 bucket. The key is used to access 2 TB of sensitive data that is stored in the S3 bucket. A security engineer discovers a potential vulnerability on the EC2 instance that could result in the compromise of the sensitive data. Due to other critical operations, the security engineer cannot immediately shut down the EC2 instance for vulnerability patching. What is the FASTEST way to prevent the sensitive data from being exposed?",Download the data from the existing S3 bucket to a new EC2 instance. Then delete the data from the S3 bucket. Re-encrypt the data with a client-based key. Upload the data to a new S3 bucket.,Block access to the public range of S3 endpoint IP addresses by using a host-based ﬁrewall. Ensure that internet-bound traﬃc from the affected EC2 instance is routed through the host-based ﬁrewall.,Revoke the IAM role's active session permissions. Update the S3 bucket policy to deny access to the IAM role. Remove the IAM role from the EC2 instance proﬁle.,"Disable the current key. Create a new KMS key that the IAM role does not have access to, and re-encrypt all the data with the new key. Schedule the compromised key for deletion.",,,C,0,1,,,,0,0,,,,,4.1,Authentication & Authorization,,63,SCS-C02,AWS Certified Security - Specialty,,"Option C immediately cuts off the instance’s access by revoking the role’s active session, applying an explicit Deny in the S3 bucket policy (which overrides any Allow), and removing the role so no new credentials are issued. This is the fastest, least disruptive way to stop data access without moving or re-encrypting data. Options A and D are slow (data copy/re-encryption), and B is unreliable for blocking S3 access."
1.3,Macie & Inspector,"A company has a group of Amazon EC2 instances in a single private subnet of a VPC with no internet gateway attached. A security engineer has installed the Amazon CloudWatch agent on all instances in that subnet to capture logs from a speciﬁc application. To ensure that the logs ﬂow securely, the company's networking team has created VPC endpoints for CloudWatch monitoring and CloudWatch logs. The networking team has attached the endpoints to the VPC. The application is generating logs However, when the security engineer queries CloudWatch, the logs do not appear. Which combination of steps should the security engineer take to troubleshoot this issue? (Choose three.)",Ensure that the EC2 instance proﬁle that is attached to the EC2 instances has permissions to create log streams and write logs.,Create a metric ﬁlter on the logs so that they can be viewed in the AWS Management Console.,Check the CloudWatch agent conﬁguration ﬁle on each EC2 instance to make sure that the CloudWatch agent is collecting the proper log ﬁles.,Check the VPC endpoint policies of both VPC endpoints to ensure that the EC2 instances have permissions to use them.,Create a NAT gateway in the subnet so that the EC2 instances can communicate with CloudWatch.,Ensure that the security groups allow all the EC2 instances to communicate with each other to aggregate logs before sending.,"A, C, D",1,1,,,,0,0,,,,,2.3,Monitoring & Detection,,70,SCS-C02,AWS Certified Security - Specialty,,"Logs will not appear if the instance profile lacks permissions (e.g., logs:CreateLogStream, logs:PutLogEvents), if the CloudWatch agent is misconfigured and not collecting the intended files, or if restrictive VPC endpoint policies block those API calls. Metric filters are not required to view raw logs, a NAT gateway is unnecessary with VPC endpoints, and instances don’t need to communicate with each other to send logs."
1.3,Macie & Inspector,A company plans to create individual child accounts within an existing organization in AWS Organizations for each of its DevOps teams. AWS CloudTrail has been enabled and conﬁgured on all accounts to write audit logs to an Amazon S3 bucket in a centralized AWS account. A security engineer needs to ensure that DevOps team members are unable to modify or disable this conﬁguration. How can the security engineer meet these requirements?,Create an IAM policy that prohibits changes to the speciﬁc CloudTrail trail and apply the policy to the AWS account root user.,Create an S3 bucket policy in the speciﬁed destination account for the CloudTrail trail that prohibits conﬁguration changes from the AWS account root user in the source account.,Create an SCP that prohibits changes to the speciﬁc CloudTrail trail and apply the SCP to the appropriate organizational unit or account in Organizations.,Create an IAM policy that prohibits changes to the speciﬁc CloudTrail trail and apply the policy to a new IAM group. Have team members use individual IAM accounts that are members of the new IAM group.,,,C,0,1,,,,0,0,,,,,4.3,Permission Boundaries,,78,SCS-C02,AWS Certified Security - Specialty,,"An SCP with explicit Deny on CloudTrail configuration actions at the OU/account level enforces organization-wide guardrails that apply to all principals, including administrators and the root user. This prevents disabling or modifying CloudTrail regardless of IAM policies in the child accounts. IAM or S3 bucket policies (A, B, D) cannot reliably restrict the root user or prevent account admins from changing CloudTrail configuration."
1.3,Macie & Inspector,"A security engineer is trying to use Amazon EC2 Image Builder to create an image of an EC2 instance. The security engineer has conﬁgured the pipeline to send logs to an Amazon S3 bucket. When the security engineer runs the pipeline, the build fails with the following error: ""AccessDenied: Access Denied status code: 403"". The security engineer must resolve the error by implementing a solution that complies with best practices for least privilege access. Which combination of steps will meet these requirements? (Choose two.)","Ensure that the following policies are attached to the IAM role that the security engineer is using·EC2InstanceProﬁleForImageBuilder, EC2InstanceProﬁleForImageBuilderECRContainerBuilds, and AmazonSSMManagedInstanceCore.","Ensure that the following policies are attached to the instance proﬁle for the EC2 instance: EC2InstanceProﬁleForImageBuilder, EC2InstanceProﬁleForImageBuilderECRContainerBuilds, and AmazonSSMManagedInstanceCore.",Ensure that the AWSImageBuilderFullAccess policy is attached to the instance proﬁle for the EC2 instance.,Ensure that the security engineer's IAM role has the s3:PutObject permission for the S3 bucket.,Ensure that the instance proﬁle for the EC2 instance has the s3:PutObject permission for the S3 bucket.,,"B, E",1,1,,,,0,0,,,,,4.1,Authentication & Authorization,,83,SCS-C02,AWS Certified Security - Specialty,,"Image Builder runs builds on EC2 instances using an instance profile, so the necessary Image Builder/SSM/ECR permissions must be attached to that instance profile, not the engineer’s IAM role (B). Since the build instance writes logs to S3, that same instance profile must have s3:PutObject on the target bucket (E). Granting permissions to the user or using AWSImageBuilderFullAccess is either ineffective or overly permissive."
1.3,Macie & Inspector,A company needs to improve its ability to identify and prevent IAM policies that grant public access or cross-account access to resources. The company has implemented AWS Organizations and has started using AWS Identity and Access Management Access Analyzer to reﬁne overly broad access to accounts in the organization. A security engineer must automate a response in the company's organization for any newly created policies that are overly permissive. The automation must remediate external access and must notify the company's security team. Which combination of steps should the security engineer take to meet these requirements? (Choose three.),Create an AWS Step Functions state machine that checks the resource type in the ﬁnding and adds an explicit Deny statement in the trust policy for the IAM role. Conﬁgure the state machine to publish a notiﬁcation to an Amazon Simple Notiﬁcation Service (Amazon SNS) topic.,Create an AWS Batch job that forwards any resource type ﬁndings to an AWS Lambda function. Conﬁgure the Lambda function to add an explicit Deny statement in the trust policy for the IAM role. Conﬁgure the AWS Batch job to publish a notiﬁcation to an Amazon Simple Notiﬁcation Service (Amazon SNS) topic.,"In Amazon EventBridge, create an event rule that matches active IAM Access Analyzer ﬁndings and invokes AWS Step Functions for resolution.","In Amazon CloudWatch, create a metric ﬁlter that matches active IAM Access Analyzer ﬁndings and invokes AWS Batch for resolution.",Create an Amazon Simple Queue Service (Amazon SQS) queue. Conﬁgure the queue to forward a notiﬁcation to the security team that an external principal has been granted access to the speciﬁc IAM role and has been blocked.,Create an Amazon Simple Notiﬁcation Service (Amazon SNS) topic for external or cross-account access notices. Subscribe the security team's email addresses to the topic.,"A, C, F",1,1,,,,0,0,,,,,4.1,Authentication & Authorization,,88,SCS-C02,AWS Certified Security - Specialty,,"Access Analyzer findings can be captured with an EventBridge rule (C), which triggers a Step Functions workflow to automate remediation—such as adding an explicit Deny in the role trust policy (A). An SNS topic provides direct notifications to the security team (F). CloudWatch metric filters don’t target Access Analyzer findings, SQS alone doesn’t notify, and AWS Batch is unnecessary for this workflow."
1.3,Macie & Inspector,The security engineer is managing a traditional three-tier web application that is running on Amazon EC2 instances. The application has become the target of increasing numbers of malicious attacks from the internet. What steps should the security engineer take to check for known vulnerabilities and limit the attack surface? (Choose two.),Use AWS Certiﬁcate Manager to encrypt all traﬃc between the client and application servers.,Review the application security groups to ensure that only the necessary ports are open.,Use Elastic Load Balancing to oﬄoad Secure Sockets Layer encryption.,Use Amazon Inspector to periodically scan the backend instances.,Use AWS Key Management Service (AWS KMS) to encrypt all the traﬃc between the client and application servers.,,"B, D",1,1,,,,0,0,,,,,3.3,Host & Instance Hardening,,95,SCS-C02,AWS Certified Security - Specialty,,"Reviewing and tightening security groups (B) limits the application’s network exposure to only required ports, reducing the attack surface. Amazon Inspector (D) automatically assesses EC2 instances for known CVEs and unintended network exposure, helping identify vulnerabilities to remediate. By contrast, TLS termination or encryption services (A, C, E) protect data in transit but do not scan for vulnerabilities or reduce open attack vectors."
1.3,Macie & Inspector,"A company needs a forensic-logging solution for hundreds of applications running in Docker on Amazon EC2. The solution must perform real-time analytics on the logs, must support the replay of messages, and must persist the logs. Which AWS services should be used to meet these requirements? (Choose two.)",Amazon Athena,Amazon Kinesis,Amazon SQS,Amazon OpenSearch Service,Amazon EMR,,"B, D",1,1,,,,0,0,,,,,2.1,Logging Configuration,,123,SCS-C02,AWS Certified Security - Specialty,,"Amazon Kinesis Data Streams provides durable, ordered ingestion with configurable retention for message replay and real-time processing/analytics. Amazon OpenSearch Service persists, indexes, and enables real-time search and analytics on the logs. Other options are batch or lack real-time replay and analytics capabilities."
1.3,Macie & Inspector,"A company hosts an application on Amazon EC2 that is subject to speciﬁc rules for regulatory compliance. One rule states that traﬃc to and from the workload must be inspected for network-level attacks. This involves inspecting the whole packet. To comply with this regulatory rule, a security engineer must install intrusion detection software on a c5n.4xlarge EC2 instance. The engineer must then conﬁgure the software to monitor traﬃc to and from the application instances. What should the security engineer do next?",Place the network interface in promiscuous mode to capture the traﬃc,Conﬁgure VPC Flow Logs to send traﬃc to the monitoring EC2 instance using a Network Load Balancer.,Conﬁgure VPC traﬃc mirroring to send traﬃc to the monitoring EC2 instance using a Network Load Balancer.,Use Amazon Inspector to detect network-level attacks and trigger an AWS Lambda function to send the suspicious packets to the EC2 instance.,,,C,0,1,,,,0,0,,,,,3.1,Network Architecture Security,,128,SCS-C02,AWS Certified Security - Specialty,,"VPC Traffic Mirroring is designed for deep packet inspection by copying full network packets from selected ENIs to a monitoring appliance; it supports sending mirrored traffic to targets behind a Network Load Balancer. Promiscuous mode is not supported in VPC, VPC Flow Logs only provide metadata (not packet payloads), and Amazon Inspector does not perform network IDS packet inspection."
1.3,Macie & Inspector,"A company has created a set of AWS Lambda functions to automate incident response steps for incidents that occur on Amazon EC2 instances. The Lambda functions need to collect relevant artifacts, such as instance ID and security group conﬁguration. The Lambda functions must then write a summary to an Amazon S3 bucket. The company runs its workloads in a VPC that uses public subnets and private subnets. The public subnets use an internet gateway to access the internet. The private subnets use a NAT gateway to access the internet. All network traﬃc to Amazon S3 that is related to the incident response process must use the AWS network. This traﬃc must not travel across the internet. Which solution will meet these requirements?",Deploy the Lambda functions to a private subnet in the VPC. Conﬁgure the Lambda functions to access the S3 service through the NAT gateway.,Deploy the Lambda functions to a private subnet in the VPC. Create an S3 gateway endpoint to access the S3 service.,Deploy the S3 bucket and the Lambda functions in the same private subnet. Conﬁgure the Lambda functions to use the default endpoint for the S3 service.,Deploy an Amazon Simple Queue Service (Amazon SQS) queue and the Lambda functions in the same private subnet. Conﬁgure the Lambda functions to send data to the SQS queue. Conﬁgure the SQS queue to send data to the S3 bucket.,,,B,0,0,1,0,0,0,2,,,,,1.1,Preparation & Runbooks,,164,SCS-C02,AWS Certified Security - Specialty,,"An S3 gateway VPC endpoint enables private connectivity from subnets to Amazon S3 over the AWS network, ensuring no traffic traverses the public internet and removing the need for a NAT gateway. Deploying Lambda in private subnets with an S3 gateway endpoint meets the requirement, whereas the other options either route via the internet (NAT) or misunderstand that S3 isn’t deployed in subnets and don’t ensure private S3 access."
1.3,Macie & Inspector,"A company that uses AWS Organizations is migrating workloads to AWS. The company's application team determines that the workloads will use Amazon EC2 instances, Amazon S3 buckets, Amazon DynamoDB tables, and Application Load Balancers. For each resource type, the company mandates that deployments must comply with the following requirements: • All EC2 instances must be launched from approved AWS accounts. • All DynamoDB tables must be provisioned with a standardized naming convention. • All infrastructure that is provisioned in any accounts in the organization must be deployed by AWS CloudFormation templates. Which combination of steps should the application team take to meet these requirements? (Choose two.)",Create CloudFormation templates in an administrator AWS account. Share the stack sets with an application AWS account. Restrict the template to be used speciﬁcally by the application AWS account.,Create CloudFormation templates in an application AWS account. Share the output with an administrator AWS account ta review compliant resources. Restrict output to only the administrator AWS account.,Use permissions boundaries to prevent the application AWS account from provisioning speciﬁc resources unless conditions for the internal compliance requirements are met.,Use SCPs to prevent the application AWS account from provisioning speciﬁc resources unless conditions for the internal compliance requirements are met.,Activate AWS Conﬁg managed rules for each service in the application AWS account.,,"A, D",1,1,,,,0,0,,,,,4.1,Authentication & Authorization,,175,SCS-C02,AWS Certified Security - Specialty,,"A centralizes and distributes approved CloudFormation templates from the administrator account via StackSets, ensuring all infrastructure is provisioned through vetted templates. D applies preventative guardrails with SCPs to deny direct resource creation (or require aws:CalledVia=cloudformation.amazonaws.com), enforce naming/tag/tagging conditions (e.g., for DynamoDB), and restrict EC2 launches to only approved accounts."
1.3,Macie & Inspector,A security team is responsible for reviewing AWS API call activity in the cloud environment for security violations. These events must be recorded and retained in a centralized location for both current and future AWS regions. What is the SIMPLEST way to meet these requirements?,"Enable AWS Trusted Advisor security checks in the AWS Console, and report all security incidents for all regions.","Enable AWS CloudTrail by creating individual trails for each region, and specify a single Amazon S3 bucket to receive log ﬁles for later analysis.",Enable AWS CloudTrail by creating a new trail and applying the trail to all regions. Specify a single Amazon S3 bucket as the storage location.,"Enable Amazon CloudWatch logging for all AWS services across all regions, and aggregate them to a single Amazon S3 bucket for later analysis.",,,C,0,1,,,,0,0,,,,,2.2,Centralization & Retention,,194,SCS-C02,AWS Certified Security - Specialty,,"CloudTrail is the service that records AWS API calls, and a single trail set to “apply to all regions” will automatically capture events from current and future regions. Pointing the trail to one S3 bucket centralizes and retains the logs, making this the simplest compliant setup."
1.3,Macie & Inspector,A company runs workloads on Amazon EC2 instances. The company needs to continually scan the EC2 instances for software vulnerabilities and unintended network exposure. Which solution will meet these requirements?,Use Amazon Inspector. Set the scan mode to hybrid scanning.,Use Amazon GuardDuty. Enable the Malware Protection feature.,Use Amazon Inspector. Enable the Malware Protection feature.,Use Amazon GuardDuty. Enable the Runtime Monitoring feature.,,,A,0,1,,,,0,0,,,,,3.3,Host & Instance Hardening,,211,SCS-C02,AWS Certified Security - Specialty,,"Amazon Inspector continuously scans EC2 instances for software vulnerabilities (CVEs) and evaluates network reachability to identify unintended exposure. Hybrid scanning combines agentless and agent-based methods to maximize coverage and simplify deployment. GuardDuty features (Malware Protection or Runtime Monitoring) detect threats and malware, not ongoing vulnerability or reachability assessments."
1.3,Macie & Inspector,"A company has conﬁgured a gateway VPC endpoint in a VPC. Only Amazon EC2 instances that reside in a single subnet in the VPC can use the endpoint. The company has modiﬁed the route table for this single subnet to route traﬃc to Amazon S3 through the gateway VPC endpoint. The VPC provides internet access through an internet gateway. A security engineer attempts to use instance proﬁle credentials from an EC2 instance to retrieve an object from the S3 bucket, but the attempt fails. The security engineer veriﬁes that the EC2 instance has an IAM instance proﬁle with the correct permissions to access the S3 bucket and to retrieve objects. The security engineer also veriﬁes that the S3 bucket policy is allowing access properly. Additionally, the security engineer veriﬁes that the EC2 instance’s security group and the subnet's network ACLs allow the communication. What else should the security engineer check to determine why the request from the EC2 instance is failing?",Verify that the EC2 instance’s security group does not have an implicit inbound deny rule for Amazon S3.,Verify that the VPC endpoint’s security group does not have an explicit inbound deny rule for the EC2 instance.,Verify that the internet gateway is allowing traﬃc to Amazon S3.,Verify that the VPC endpoint policy is allowing access to Amazon S3.,,,D,0,1,,,,0,0,,,,,4.1,Authentication & Authorization,,227,SCS-C02,AWS Certified Security - Specialty,,"For S3 gateway VPC endpoints, an endpoint policy can explicitly allow or deny access and is evaluated in addition to the instance’s IAM role and the bucket policy. If the endpoint policy is restrictive, it will block the request even when IAM, bucket policy, security groups, and NACLs permit it. Gateway endpoints do not use security groups, and the internet gateway is bypassed due to the route to the endpoint."
1.3,Macie & Inspector,"Amazon CloudWatch Logs agent is successfully delivering logs to the CloudWatch Logs service. However, logs stop being delivered after the associated log stream has been active for a speciﬁc number of hours. What steps are necessary to identify the cause of this phenomenon? (Choose two.)",Ensure that ﬁle permissions for monitored ﬁles that allow the CloudWatch Logs agent to read the ﬁle have not been modiﬁed.,Verify that the OS Log rotation rules are compatible with the conﬁguration requirements for agent streaming.,Conﬁgure an Amazon Kinesis producer to ﬁrst put the logs into Amazon Kinesis Streams.,Create a CloudWatch Logs metric to isolate a value that changes at least once during the period before logging stops.,Use AWS CloudFormation to dynamically create and maintain the conﬁguration ﬁle for the CloudWatch Logs agent.,,"A, B",1,1,,,,0,0,,,,,2.3,Monitoring & Detection,,233,SCS-C02,AWS Certified Security - Specialty,,"Delivery often stops after a fixed time because log rotation kicks in, changing file names/inodes or compressing files, which the CloudWatch Logs agent may no longer track. Verifying OS log rotation settings (B) and ensuring file permissions remain readable by the agent after rotation (A) helps identify and resolve the issue. The other options do not address the agent’s file access or rotation behavior."
1.3,Macie & Inspector,A medical company recently completed an acquisition and inherited an existing AWS environment. The company has an upcoming audit and is concerned about the compliance posture of its acquisition. The company must identify personal health information inside Amazon S3 buckets and must identify S3 buckets that are publicly accessible. The company needs to prepare for the audit by collecting evidence in the environment. Which combination of steps will meet these requirements with the LEAST operational overhead? (Choose three.),Enable Amazon Macie. Run an on-demand sensitive data discovery job that uses the PERSONAL_INFORMATION managed data identiﬁer.,Use AWS Glue with the Detect PII transform to identify sensitive data and to mask the sensitive data.,Enable AWS Audit Manager. Create an assessment by using a supported framework.,Enable Amazon GuardDuty S3 Protection. Document any ﬁndings that are related to suspicious access of S3 buckets.,Enable AWS Security Hub. Use the AWS Foundational Security Best Practices standard. Review the controls dashboard for evidence of failed S3 Block Public Access controls.,Enable AWS Conﬁg. Set up the s3-bucket-public-write-prohibited AWS Conﬁg managed rule.,"A, C, E",1,1,,,,0,0,,,,,6.2,Compliance Validation,,238,SCS-C02,AWS Certified Security - Specialty,,"Amazon Macie can automatically discover and classify PHI/PII in S3 using managed data identifiers, satisfying the data-identification requirement with minimal setup. AWS Audit Manager streamlines audit preparation by collecting evidence and mapping it to supported compliance frameworks. AWS Security Hub with the Foundational Security Best Practices standard surfaces S3 public access misconfigurations as controls, providing centralized evidence with low operational overhead."
1.3,Macie & Inspector,A security engineer is working with a development team to design a supply chain application that stores sensitive inventory data in an Amazon S3 bucket. The application will use an AWS Key Management Service (AWS KMS) customer managed key to encrypt the data in Amazon S3. The inventory data in Amazon S3 will be shared with hundreds of vendors. All vendors will use AWS principals from their own AWS accounts to access the data in Amazon S3. The vendor list might change weekly. The security engineer needs to ﬁnd a solution that supports cross-account access. Which solution is the MOST operationally eﬃcient way to manage access control for the customer managed key?,Use KMS grants to manage key access. Programmatically create and revoke grants to manage vendor access.,Use am IAM role to manage key access. Programmatically update the IAM role policies to manage vendor access.,Use KMS key policies to manage key access. Programmatically update the KMS key policies to manage vendor access.,Use delegated access across AWS accounts by using IAM roles to manage key access. Programmatically update the IAM trust policy to manage cross-account vendor access.,,,A,0,1,,,,0,0,,,,,4.2,Cross-Account & Federation,,260,SCS-C02,AWS Certified Security - Specialty,,"KMS grants are purpose-built for granting and revoking fine-grained, cross-account KMS key permissions programmatically without editing the key policy, making them ideal when the set of external principals changes frequently. Using IAM roles or key policies would require continual policy/trust updates and is less scalable and more operationally burdensome than managing grants."
1.3,Macie & Inspector,A company runs workloads on Amazon EC2 instances. The company needs to continually monitor the EC2 instances for software vulnerabilities and must display the ﬁndings in AWS Security Hub. The company must not install agents on the EC2 instances. Which solution will meet these requirements?,Enable Amazon Inspector. Set the scan mode to hybrid scanning. Enable the integration for Amazon Inspector in Security Hub.,Use Security Hub to enable the AWS Foundational Security Best Practices standard. Wait for Security Hub to generate the ﬁndings.,Enable Amazon GuardDuty. Initiate on-demand malware scans by using GuardDuty Malware Protection. Enable the integration for GuardDuty in Security Hub.,Use AWS Conﬁg managed rules to detect EC2 software vulnerabilities. Ensure that Security Hub has the AWS Conﬁg integration enabled.,,,A,0,1,,,,0,0,,,,,2.3,Monitoring & Detection,,279,SCS-C02,AWS Certified Security - Specialty,,"Amazon Inspector (Inspector v2) supports agentless and hybrid scanning for EC2 instances to detect software vulnerabilities, satisfying the requirement to avoid installing agents. Inspector findings natively integrate with AWS Security Hub, enabling centralized visibility of vulnerabilities. Other options either don’t perform CVE-based vulnerability assessments (Security Hub/Config) or focus on threat/malware detection rather than software vulnerabilities (GuardDuty)."
1.3,Macie & Inspector,A company is migrating container workloads from a data center to Amazon Elastic Container Service (Amazon ECS) clusters. The company must implement a solution to detect potential threats in the workloads and to improve the security posture of the container clusters. Which solution will meet these requirements?,Conﬁgure Amazon Inspector on the VPC that is running the ECS clusters.,Enable Amazon GuardDuty Runtime Monitoring on the ECS clusters.,Audit Amazon ECS API access by using Amazon CloudWatch logs to identify unauthorized access.,Create container clusters in the same VPC. Use VPC ﬂow logs to centrally monitor network traﬃc.,,,B,0,1,,,,0,0,,,,,3.4,Container & Serverless Security,,288,SCS-C02,AWS Certified Security - Specialty,,"Amazon GuardDuty Runtime Monitoring provides managed threat detection for container workloads on Amazon ECS by analyzing runtime activity, processes, and network events to surface potential compromises. Amazon Inspector focuses on vulnerability scanning (not VPC-level setup), VPC flow logs only monitor network traffic, and auditing ECS API access doesn’t detect in-container runtime threats."
1.4,WAF & Shield,"A company is developing an ecommerce application. The application uses Amazon EC2 instances and an Amazon RDS MySQL database. For compliance reasons, data must be secured in transit and at rest. The company needs a solution that minimizes operational overhead and minimizes cost. Which solution meets these requirements?",Use TLS certiﬁcates from AWS Certiﬁcate Manager (ACM) with an Application Load Balancer. Deploy self-signed certiﬁcates on the EC2 instances. Ensure that the database client software uses a TLS connection to Amazon RDS. Enable encryption of the RDS DB instance. Enable encryption on the Amazon Elastic Block Store (Amazon EBS) volumes that support the EC2 instances.,Use TLS certiﬁcates from a third-party vendor with an Application Load Balancer. Install the same certiﬁcates on the EC2 instances. Ensure that the database client software uses a TLS connection to Amazon RDS. Use AWS Secrets Manager for client-side encryption of application data.,Use AWS CloudHSM to generate TLS certiﬁcates for the EC2 instances. Install the TLS certiﬁcates on the EC2 instances. Ensure that the database client software uses a TLS connection to Amazon RDS. Use the encryption keys from CloudHSM for client-side encryption of application data.,Use Amazon CloudFront with AWS WAF. Send HTTP connections to the origin EC2 instances. Ensure that the database client software uses a TLS connection to Amazon RDS. Use AWS Key Management Service (AWS KMS) for client-side encryption of application data before the data is stored in the RDS database.,,,A,0,1,,,,0,0,,,,,5.1,Encryption at Rest,,16,SCS-C02,AWS Certified Security - Specialty,,"Option A secures data in transit with ACM-managed TLS on the ALB and TLS to RDS, and secures data at rest by enabling RDS encryption and EBS volume encryption—meeting compliance with minimal management and cost. The other options add unnecessary complexity or cost (third‑party certs, CloudHSM, CloudFront) or misuse services (Secrets Manager for encryption) and even allow HTTP to origins."
1.4,WAF & Shield,"A company has hundreds of AWS accounts in an organization in AWS Organizations. The company operates out of a single AWS Region. The company has a dedicated security tooling AWS account in the organization. The security tooling account is conﬁgured as the organization's delegated administrator for Amazon GuardDuty and AWS Security Hub. The company has conﬁgured the environment to automatically enable GuardDuty and Security Hub for existing AWS accounts and new AWS accounts. The company is performing control tests on speciﬁc GuardDuty ﬁndings to make sure that the company's security team can detect and respond to security events. The security team launched an Amazon EC2 instance and attempted to run DNS requests against a test domain, example.com, to generate a DNS ﬁnding. However, the GuardDuty ﬁnding was never created in the Security Hub delegated administrator account. Why was the ﬁnding was not created in the Security Hub delegated administrator account?",VPC ﬂow logs were not turned on for the VPC where the EC2 instance was launched.,The VPC where the EC2 instance was launched had the DHCP option conﬁgured for a custom OpenDNS resolver.,The GuardDuty integration with Security Hub was never activated in the AWS account where the ﬁnding was generated.,Cross-Region aggregation in Security Hub was not conﬁgured.,,,B,0,1,,,,0,0,,,,,2.3,Monitoring & Detection,,31,SCS-C02,AWS Certified Security - Specialty,,"GuardDuty generates DNS-related findings only for queries that use the Amazon-provided VPC resolver (AmazonProvidedDNS). Because the VPC used a custom OpenDNS resolver via a DHCP options set, GuardDuty could not see the DNS queries, so no finding was created or forwarded to Security Hub."
1.4,WAF & Shield,"A startup company is using a single AWS account that has resources in a single AWS Region. A security engineer conﬁgures an AWS CloudTrail trail in the same Region to deliver log ﬁles to an Amazon S3 bucket by using the AWS CLI. Because of expansion, the company adds resources in multiple Regions. The security engineer notices that the logs from the new Regions are not reaching the S3 bucket. What should the security engineer do to ﬁx this issue with the LEAST amount of operational overhead?",Create a new CloudTrail trail. Select the new Regions where the company added resources.,Change the S3 bucket to receive notiﬁcations to track all actions from all Regions.,Create a new CloudTrail trail that applies to all Regions.,Change the existing CloudTrail trail so that it applies to all Regions.,,,D,0,1,,,,0,0,,,,,2.1,Logging Configuration,,47,SCS-C02,AWS Certified Security - Specialty,,"Update the existing CloudTrail trail to “apply to all Regions” so it automatically records events from every current and future Region to the same S3 bucket with minimal changes. Creating a new trail (A/C) is unnecessary, and S3 notifications (B) don’t control CloudTrail log delivery."
1.4,WAF & Shield,"A security engineer is conﬁguring account-based access control (ABAC) to allow only speciﬁc principals to put objects into an Amazon S3 bucket. The principals already have access to Amazon S3. The security engineer needs to conﬁgure a bucket policy that allows principals to put objects into the S3 bucket only if the value of the Team tag on the object matches the value of the Team tag that is associated with the principal. During testing, the security engineer notices that a principal can still put objects into the S3 bucket when the tag values do not match. Which combination of factors are causing the PutObject operation to succeed when the tag values are different? (Choose two.)",The principal's identity-based policy grants access to put objects into the S3 bucket with no conditions.,The principal's identity-based policy overrides the condition because the identity-based policy contains an explicit allow.,The S3 bucket's resource policy does not deny access to put objects.,The S3 bucket's resource policy cannot allow actions to the principal.,The bucket policy does not apply to principals in the same zone of trust.,,"A, C",1,1,,,,0,0,,,,,4.1,Authentication & Authorization,,66,SCS-C02,AWS Certified Security - Specialty,,"The principal’s identity-based policy includes an unconditional Allow for s3:PutObject, so IAM has an Allow regardless of the ABAC condition in the bucket policy. Because the bucket policy does not contain an explicit Deny to enforce the tag match, nothing overrides the identity-based Allow, allowing PutObject to succeed even when tags differ."
1.4,WAF & Shield,A company uses AWS Organizations. The company wants to implement short-term credentials for third-party AWS accounts to use to access accounts within the company's organization. Access is for the AWS Management Console and third-party software-as-a-service (SaaS) applications. Trust must be enhanced to prevent two external accounts from using the same credentials. The solution must require the least possible operational effort. Which solution will meet these requirements?,Use a bearer token authentication with OAuth or SAML to manage and share a central Amazon Cognito user pool across multiple Amazon API Gateway APIs.,"Implement AWS IAM Identity Center (AWS Single Sign-On), and use an identity source of choice. Grant access to users and groups from other accounts by using permission sets that are assigned by account.",Create a unique IAM role for each external account. Create a trust policy Use AWS Secrets Manager to create a random external key.,Create a unique IAM role for each external account. Create a trust policy that includes a condition that uses the sts:ExternalId condition key.,,,D,0,1,,,,0,0,,,,,4.2,Cross-Account & Federation,,81,SCS-C02,AWS Certified Security - Specialty,,"Using cross-account IAM roles with a trust policy that requires sts:ExternalId ensures only the intended third-party account can assume the role, preventing the confused-deputy problem where two external accounts could share credentials. Assigning a unique role and ExternalId per external account provides short-term STS credentials for console and SaaS access with minimal operational overhead. Other options don’t enforce this trust control or add unnecessary complexity."
1.4,WAF & Shield,"A company's data scientists want to create artiﬁcial intelligence and machine learning (AI/ML) training models by using Amazon SageMaker. The training models will use large datasets in an Amazon S3 bucket. The datasets contain sensitive information. On average, the data scientists need 30 days to train models. The S3 bucket has been secured appropriately. The company's data retention policy states that all data that is older than 45 days must be removed from the S3 bucket. Which action should a security engineer take to enforce this data retention policy?",Conﬁgure an S3 Lifecycle rule on the S3 bucket to delete objects after 45 days.,Create an AWS Lambda function to check the last-modiﬁed date of the S3 objects and delete objects that are older than 45 days. Create an S3 event notiﬁcation to invoke the Lambda function for each PutObject operation.,Create an AWS Lambda function to check the last-modiﬁed date of the S3 objects and delete objects that are older than 45 days. Create an Amazon EventBridge rule to invoke the Lambda function each month.,Conﬁgure S3 Intelligent-Tiering on the S3 bucket to automatically transition objects to another storage class.,,,A,0,1,,,,0,0,,,,,5.4,Data Classification & Access Control,,97,SCS-C02,AWS Certified Security - Specialty,,"An S3 Lifecycle rule natively enforces retention by automatically expiring (deleting) objects after a specified age, such as 45 days, with no custom code and at bucket scale. The Lambda-based options add unnecessary complexity and potential scheduling gaps, and S3 Intelligent-Tiering addresses cost optimization, not deletion."
1.4,WAF & Shield,A company has AWS accounts in an organization in AWS Organizations. The organization includes a dedicated security account. All AWS account activity across all member accounts must be logged and reported to the dedicated security account. The company must retain all the activity logs in a secure storage location within the dedicated security account for 2 years. No changes or deletions of the logs are allowed. Which combination of steps will meet these requirements with the LEAST operational overhead? (Choose two.),"In the dedicated security account, create an Amazon S3 bucket. Conﬁgure S3 Object Lock in compliance mode and a retention period of 2 years on the S3 bucket. Set the bucket policy to allow the organization's management account to write to the S3 bucket.","In the dedicated security account, create an Amazon S3 bucket. Conﬁgure S3 Object Lock in compliance mode and a retention period of 2 years on the S3 bucket. Set the bucket policy to allow the organization's member accounts to write to the S3 bucket.","In the dedicated security account, create an Amazon S3 bucket that has an S3 Lifecycle conﬁguration that expires objects after 2 years. Set the bucket policy to allow the organization's member accounts to write to the S3 bucket.",Create an AWS CloudTrail trail for the organization. Conﬁgure logs to be delivered to the logging Amazon S3 bucket in the dedicated security account.,Turn on AWS CloudTrail in each account. Conﬁgure logs to be delivered to an Amazon S3 bucket that is created in the organization's management account. Forward the logs to the S3 bucket in the dedicated security account by using AWS Lambda and Amazon Kinesis Data Firehose.,,"B, D",1,1,,,,0,0,,,,,2.2,Centralization & Retention,,112,SCS-C02,AWS Certified Security - Specialty,,"An organization-level CloudTrail centralizes logging for all member accounts and delivers logs to a single S3 bucket in the security account with minimal admin overhead. Enabling S3 Object Lock in compliance mode with a 2‑year retention enforces WORM so logs cannot be altered or deleted, and the bucket policy allowing organization member accounts (via org ID) ensures all accounts’ logs can be written."
1.4,WAF & Shield,"A company has a VPC that has no internet access and has the private DNS hostnames option enabled. An Amazon Aurora database is running inside the VPC. A security engineer wants to use AWS Secrets Manager to automatically rotate the credentials for the Aurora database. The security engineer conﬁgures the Secrets Manager default AWS Lambda rotation function to run inside the same VPC that the Aurora database uses. However, the security engineer determines that the password cannot be rotated properly because the Lambda function cannot communicate with the Secrets Manager endpoint. What is the MOST secure way that the security engineer can give the Lambda function the ability to communicate with the Secrets Manager endpoint?",Add a NAT gateway to the VPC to allow access to the Secrets Manager endpoint.,Add a gateway VPC endpoint to the VPC to allow access to the Secrets Manager endpoint.,Add an interface VPC endpoint to the VPC to allow access to the Secrets Manager endpoint.,Add an internet gateway for the VPC to allow access to the Secrets Manager endpoint.,,,C,0,1,,,,0,0,,,,,3.1,Network Architecture Security,,141,SCS-C02,AWS Certified Security - Specialty,,"Secrets Manager supports private connectivity via AWS PrivateLink, which uses interface VPC endpoints. Creating an interface VPC endpoint lets the Lambda function access Secrets Manager privately within the VPC without internet access. Gateway endpoints only support S3 and DynamoDB, and adding NAT or an internet gateway is less secure and unnecessary."
1.4,WAF & Shield,A development team is creating an open source toolset to manage a company's software as a service (SaaS) application. The company stores the code in a public repository so that anyone can view and download the toolset's code. The company discovers that the code contains an IAM access key and secret key that provide access to internal resources in the company’s AWS environment A security engineer must implement a solution to identify whether unauthorized usage of the exposed credentials has occurred. The solution also must prevent any additional usage of the exposed credentials. Which combination of steps will meet these requirements? (Choose two.),Use AWS Identity and Access Management Access Analyzer to determine which resources the exposed credentials accessed and who used them.,Deactivate the exposed IAM access key from the user’s IAM account.,Create a rule in Amazon GuardDuty to block the access key in the source code from being used.,Create a new IAM access key and secret key for the user whose credentials were exposed.,Generate an IAM credential report. Check the report to determine when the user that owns the access key last logged in.,,"A, B",1,1,,,,0,0,,,,,1.2,Detection & Investigation,,159,SCS-C02,AWS Certified Security - Specialty,,"Deactivate the exposed IAM access key to immediately prevent any further use (B). Use IAM Access Analyzer (with CloudTrail integration) to investigate which actions and resources were accessed with the exposed credentials and by whom (A). GuardDuty cannot block keys (detection only), creating a new key doesn’t revoke the compromised one, and the IAM credential report lacks detailed usage context."
1.4,WAF & Shield,A company uses AWS Organizations. The company has more than 100 AWS accounts and will increase the number of accounts. The company also uses an external corporate identity provider (IdP). The company needs to provide users with role-based access to the accounts. The solution must maximize scalability and operational eﬃciency. Which solution will meet these requirements?,"In each account, create a set of dedicated IAM users. Ensure that all users assume these IAM users through federation with the existing IdP.","Deploy an IAM role in a central identity account. Allow users to assume the role through federation with the existing IdP. In each account, deploy a set of IAM roles that match the desired access patterns. Include a trust policy that allows access from the central identity account. Edit the permissions policy for the role in each account to match user access requirements.",Enable AWS IAM Identity Center. Integrate IAM Identity Center with the company's existing IdP. Create permission sets that match the desired access patterns. Assign permissions to match user access requirements.,"In each account, deploy a set of IAM roles that match the desired access patterns. Create a trust policy with the existing IdP. Update each role's permissions policy to use SAML-based IAM condition keys that are based on user access requirements.",,,C,0,1,,,,0,0,,,,,4.2,Cross-Account & Federation,,161,SCS-C02,AWS Certified Security - Specialty,,"IAM Identity Center integrates directly with external IdPs and centrally manages role-based access using permission sets, enabling scalable, automated provisioning across many accounts in AWS Organizations. This minimizes per-account configuration and operational overhead compared to creating and managing IAM users/roles and trust policies in each account."
1.4,WAF & Shield,A security engineer uses Amazon Macie to scan a company’s Amazon S3 buckets for sensitive data. The company has many S3 buckets and many objects stored in the S3 buckets. The security engineer must identify S3 buckets that contain sensitive data and must perform additional scanning on those S3 buckets. Which solution will meet these requirements with the LEAST administrative overhead?,Conﬁgure S3 Cross-Region Replication (CRR) on the S3 buckets to replicate the objects to a second AWS Region. Conﬁgure Macie in the second Region to scan the replicated objects daily.,Create an AWS Lambda function as an S3 event destination for the S3 buckets. Conﬁgure the Lambda function to start a Macie scan of an object when the object is uploaded to an S3 bucket.,Conﬁgure Macie automated discovery to continuously sample data from the S3 buckets. Perform full scans of the S3 buckets where Macie discovers sensitive data.,Conﬁgure Macie scans to run on the S3 buckets. Aggregate the results of the scans in an Amazon DynamoDB table. Use the DynamoDB table for queries.,,,C,0,1,,,,0,0,,,,,5.4,Data Classification & Access Control,,209,SCS-C02,AWS Certified Security - Specialty,,"Macie automated discovery continuously and automatically samples data across all S3 buckets to flag those likely containing sensitive data, requiring minimal setup and management. You can then run full classification jobs only on the flagged buckets, reducing cost and effort compared to replication, custom Lambda triggers, or manual scan orchestration and aggregation."
1.4,WAF & Shield,A company needs to analyze access logs for an Application Load Balancer (ALB). The ALB directs traﬃc to the company’s online login portal. The company needs to use visualizations to identify login attempts by bots from a list of known IP sources. Which solution will meet these requirements?,Conﬁgure the ALB to send logs directly to Amazon CloudWatch Logs. Analyze and visualize the logs by using CloudWatch Logs Insights.,Conﬁgure the ALB to send logs directly to Amazon Redshift. Analyze the logs by using SQL queries. Visualize the logs by using custom reports.,Conﬁgure the ALB to send logs directly to Amazon OpenSearch Service. Analyze the logs by using OpenSearch dashboards. Visualize the logs by using custom OpenSearch dashboards.,Conﬁgure the ALB to send logs directly to an Amazon S3 bucket. Analyze the logs by using Amazon Athena. Visualize the logs by using Amazon QuickSight.,,,D,0,1,,,,0,0,,,,,2.3,Monitoring & Detection,,215,SCS-C02,AWS Certified Security - Specialty,,"ALB access logs are natively delivered to S3; you can query them with Athena and build visualizations in QuickSight, including filtering against a known list of bot IPs. ALBs cannot send access logs directly to CloudWatch Logs, Redshift, or OpenSearch, so those options are not feasible."
1.4,WAF & Shield,A company has conﬁgured an organization in AWS Organizations for its AWS accounts. AWS CloudTrail is enabled in all AWS Regions. A security engineer must implement a solution to prevent CloudTrail from being disabled. Which solution will meet this requirement?,Enable CloudTrail log ﬁle integrity validation from the organization’s management account.,Enable server-side encryption with AWS KMS keys (SSE-KMS) for CloudTrail logs. Create a KMS key. Attach a policy to the key to prevent decryption of the logs.,Create an SCP that includes an explicit Deny rule for the StopLogging action and the DeleteTrail action. Attach the SCP to the root OU.,Create IAM policies for all the company’s users to prevent the users from performing the DescribeTrails action and the GetTrailStatus action.,,,C,0,1,,,,0,0,,,,,4.1,Authentication & Authorization,,251,SCS-C02,AWS Certified Security - Specialty,,"An SCP with an explicit Deny on StopLogging and DeleteTrail at the root OU prevents any principal in any account from disabling or deleting CloudTrail, regardless of their IAM permissions. The other options do not stop trail disabling: integrity validation and SSE-KMS protect log integrity/confidentiality, and limiting Describe/Get actions doesn’t prevent stopping or deleting the trail."
1.4,WAF & Shield,A company has a new web-based account management system for an online game. Players create a unique username and password to log in to the system. The company has implemented an AWS WAF web ACL for the system. The web ACL includes the core rule set (CRS) AWS managed rule group on the Application Load Balancer that serves the system. The company’s security team ﬁnds that the system was the target of a credential stuﬃng attack. Credentials that were exposed in other breaches were used to try to log in to the system. The security team must implement a solution to reduce the chance of a successful credential stuﬃng attack in the future. The solution also must minimize impact on legitimate users of the system. Which combination of actions will meet these requirements? (Choose two.),Create an Amazon CloudWatch custom metric to analyze the number of successful login responses from a single IP address.,Add the account takeover prevention (ATP) AWS managed rule group to the web ACL. Conﬁgure the rule group to inspect login requests to the system. Block any requests that have the awswaf:managed:aws:atp:signal:credential_compromised label.,Conﬁgure a default web ACL action that requires all users to solve a CAPTCHA puzzle when they log in.,Implement IP-based match rules in the web ACL for any IP addresses that generate many successful login responses. Block any IP addresses that generate many successful logins.,Create a custom block response that redirects users to a secure workﬂow to reset their password inside the system.,,"A, B",1,1,,,,0,0,,,,,4.1,Authentication & Authorization,,274,SCS-C02,AWS Certified Security - Specialty,,"The AWS WAF Account Takeover Prevention managed rule group is purpose-built to detect and block credential stuffing by inspecting login requests and labeling compromised credentials, minimizing friction versus blanket CAPTCHAs or broad IP blocks. Creating a CloudWatch custom metric to track successful logins per IP helps detect anomalous patterns and informs tuning or automated responses, further reducing the chance of successful attacks while limiting impact on legitimate users."
1.4,WAF & Shield,"A company is operating an open-source software platform that is internet facing. The legacy software platform no longer receives security updates. The software platform operates using Amazon Route 53 weighted load balancing to send traﬃc to two Amazon EC2 instances that connect to an Amazon RDS cluster. A recent report suggests this software platform is vulnerable to SQL injection attacks, with samples of attacks provided. The company’s security engineer must secure this system against SQL injection attacks within 24 hours. The security engineer’s solution must involve the least amount of effort and maintain normal operations during implementation. What should the security engineer do to meet these requirements?","Create an Application Load Balancer with the existing EC2 instances as a target group. Create an AWS WAF web ACL containing rules that protect the application from this attack, then apply it to the ALB. Test to ensure the vulnerability has been mitigated, then redirect the Route 53 records to point to the ALB. Update security groups on the EC2 instances to prevent direct access from the internet.","Create an Amazon CloudFront distribution specifying one EC2 instance as an origin. Create an AWS WAF web ACL containing rules that protect the application from this attack, then apply it to the distribution. Test to ensure the vulnerability has been mitigated, then redirect the Route 53 records to point to CloudFront.","Obtain the latest source code for the platform and make the necessary updates. Test the updated code to ensure that the vulnerability has been mitigated, then deploy the patched version of the platform to the EC2 instances.","Update the security group that is attached to the EC2 instances, removing access from the internet to the TCP port used by the SQL database. Create an AWS WAF web ACL containing rules that protect the application from this attack, then apply it to the EC2 instances. Test to ensure the vulnerability has been mitigated, then restore the security group to the original setting.",,,A,0,1,,,,0,0,,,,,3.2,Edge & Web Protection,,276,SCS-C02,AWS Certified Security - Specialty,,"Placing an Application Load Balancer in front of the EC2 instances lets you attach AWS WAF with managed SQL injection rules to quickly mitigate the vulnerability without changing the application, and Route 53 can be switched to the ALB with no downtime. CloudFront adds unnecessary complexity for a dynamic origin and D is invalid because WAF cannot attach directly to EC2, while code fixes (C) are not feasible within 24 hours on a legacy platform. Locking down instance security groups ensures all traffic flows through the WAF-protected ALB."
1.4,WAF & Shield,A security engineer has noticed an unusually high amount of traﬃc coming from a single IP address. This was discovered by analyzing the Application Load Balancer’s access logs. How can the security engineer limit the number of requests from a speciﬁc IP address without blocking the IP address?,Add a rule to the Application Load Balancer to route the traﬃc originating from the IP address in question and show a static webpage.,Implement a rate-based rule with AWS WAF.,Use AWS Shield to limit the originating traﬃc hit rate.,Implement the GeoLocation feature in Amazon Route 53.,,,B,0,1,,,,0,0,,,,,2.3,Monitoring & Detection,,283,SCS-C02,AWS Certified Security - Specialty,,"AWS WAF supports rate-based rules that automatically throttle requests from IPs exceeding a defined request limit, reducing traffic without fully blocking the IP. This directly addresses per-IP request limiting, whereas ALB rules, AWS Shield, and Route 53 geolocation do not provide per-IP rate limiting controls."
1.4,WAF & Shield,A company is running its application on AWS. Malicious users exploited a recent promotion event and created many fake accounts. The application currently uses Amazon CloudFront in front of an Amazon API Gateway API. AWS Lambda functions serve the different API endpoints. The GET registration endpoint is behind the path of /store/registration. The URI for submission of the new account details is at /store/newaccount. A security engineer needs to design a solution that prevents similar exploitations for future promotion events. Which combination of steps will meet these requirements? (Choose two.),Create an AWS WAF web ACL. Add the AWSManagedRulesACFPRuleSet rule group to the web ACL. Associate the web ACL with the CloudFront distribution.,Create an AWS WAF web ACL. Add a rate limit rule to the web ACL. Include a RateBasedStatement entry that has a SearchString value that points to /store/registration.,Specify /store/registration as the registration page path. Specify /store/newaccount as the account creation path.,Enable AWS Shield Advanced for the account that hosts the CloudFront distribution. Conﬁgure a DNS-speciﬁc custom mitigation that uses the Shield Response Team (SRT) for /store/newaccount.,Enable Amazon GuardDuty for the account that hosts the CloudFront distribution. Enable Lambda Protection for the Lambda functions that answer calls to /store/registration and /store/newaccount.,,"A, B",1,1,,,,0,0,,,,,3.2,Edge & Web Protection,,301,SCS-C02,AWS Certified Security - Specialty,,"Attaching AWS WAF with the AWSManagedRulesACFPRuleSet at CloudFront blocks automated and fraudulent account-creation attempts at the edge. Adding a WAF rate-based rule scoped to the /store/registration path throttles spikes during promotions, limiting abuse. Shield Advanced and GuardDuty don’t mitigate application-layer fake signups, and the rate limit directly targets the registration endpoint."
1.5,Incident Response,A team is using AWS Secrets Manager to store an application database password. Only a limited number of IAM principals within the account can have access to the secret. The principals who require access to the secret change frequently. A security engineer must create a solution that maximizes ﬂexibility and scalability. Which solution will meet these requirements?,Use a role-based approach by creating an IAM role with an inline permissions policy that allows access to the secret. Update the IAM principals in the role trust policy as required.,Deploy a VPC endpoint for Secrets Manager. Create and attach an endpoint policy that speciﬁes the IAM principals that are allowed to access the secret. Update the list of IAM principals as required.,Use a tag-based approach by attaching a resource policy to the secret. Apply tags to the secret and the IAM principals. Use the aws:PrincipalTag and aws:ResourceTag IAM condition keys to control access.,Use a deny-by-default approach by using IAM policies to deny access to the secret explicitly. Attach the policies to an IAM group. Add all IAM principals to the IAM group. Remove principals from the group when they need access. Add the principals to the group again when access is no longer allowed.,,,C,0,0,0,1,0,1,0,,,,,4.1,Authentication & Authorization,,29,SCS-C02,AWS Certified Security - Specialty,,"Using ABAC with a resource policy on the secret and aws:PrincipalTag/aws:ResourceTag lets you grant access based on matching tags, so access changes are made by updating tags rather than editing policies per principal. This is more flexible and scalable than updating role trust policies, using endpoint policies, or managing explicit-deny groups."
1.5,Incident Response,"A company is using AWS Organizations to manage multiple AWS accounts for its human resources, ﬁnance, software development, and production departments. All the company's developers are part of the software development AWS account. The company discovers that developers have launched Amazon EC2 instances that were preconﬁgured with software that the company has not approved for use. The company wants to implement a solution to ensure that developers can launch EC2 instances with only approved software applications and only in the software development AWS account. Which solution will meet these requirements?","In the software development account, create AMIs of preconﬁgured instances that include only approved software. Include the AMI IDs in the condition section of an AWS CloudFormation template to launch the appropriate AMI based on the AWS Region. Provide the developers with the CloudFormation template to launch EC2 instances in the software development account.",Create an Amazon EventBridge rule that runs when any EC2 RunInstances API event occurs in the software development account. Specify AWS Systems Manager Run Command as a target of the rule. Conﬁgure Run Command to run a script that will install all approved software onto the instances that the developers launch.,Use an AWS Service Catalog portfolio that contains EC2 products with appropriate AMIs that include only approved software. Grant the developers permission to access only the Service Catalog portfolio to launch a product in the software development account.,"In the management account, create AMIs of preconﬁgured instances that include only approved software. Use AWS CloudFormation StackSets to launch the AMIs across any AWS account in the organization. Grant the developers permission to launch the stack sets within the management account.",,,C,0,0,0,1,0,1,0,,,,,4.1,Authentication & Authorization,,35,SCS-C02,AWS Certified Security - Specialty,,"AWS Service Catalog lets you publish curated EC2 products that use approved AMIs and enforce governance by granting developers access only to those products in the software development account. This prevents launching unapproved images while following least-privilege and account scoping. Alternatives either don’t enforce use (A), remediate after launch and don’t prevent unapproved software (B), or require elevated permissions in the management account and span accounts unnecessarily (D)."
1.5,Incident Response,"A company's policy requires that all API keys be encrypted and stored separately from source code in a centralized security account. This security account is managed by the company's security team. However, an audit revealed that an API key is stored with the source code of an AWS Lambda function in an AWS CodeCommit repository in the DevOps account. How should the security team securely store the API key?",Create a CodeCommit repository in the security account using AWS Key Management Service (AWS KMS) for encryption. Require the development team to migrate the Lambda source code to this repository.,"Store the API key in an Amazon S3 bucket in the security account using server-side encryption with Amazon S3 managed encryption keys (SSE-S3) to encrypt the key. Create a presigned URL for the S3 key, and specify the URL in a Lambda environmental variable in the AWS CloudFormation template. Update the Lambda function code to retrieve the key using the URL and call the API.",Create a secret in AWS Secrets Manager in the security account to store the API key using AWS Key Management Service (AWS KMS) for encryption. Grant access to the IAM role used by the Lambda function so that the function can retrieve the key from Secrets Manager and call the API.,Create an encrypted environment variable for the Lambda function to store the API key using AWS Key Management Service (AWS KMS) for encryption. Grant access to the IAM role used by the Lambda function so that the function can decrypt the key at runtime.,,,C,0,0,1,0,0,0,1,,,,,5.1,Encryption at Rest,,79,SCS-C02,AWS Certified Security - Specialty,,"Secrets Manager in the security account satisfies the requirement to centrally and separately store and KMS-encrypt API keys, while providing fine-grained cross-account access to the Lambda role. It enables secure retrieval at runtime and supports rotation, unlike environment variables or storing the secret alongside code. Other options either co-locate secrets with code or lack proper centralized, managed secret storage."
1.5,Incident Response,A security engineer is building a Java application that is running on Amazon EC2. The application communicates with an Amazon RDS instance and authenticates with a user name and password. Which combination of steps can the engineer take to protect the credentials and minimize downtime when the credentials are rotated? (Choose two.),Have a database administrator encrypt the credentials and store the ciphertext in Amazon S3. Grant permission to the instance role associated with the EC2 instance to read the object and decrypt the ciphertext.,Conﬁgure a scheduled job that updates the credential in AWS Systems Manager Parameter Store and notiﬁes the engineer that the application needs to be restarted.,Conﬁgure automatic rotation of credentials in AWS Secrets Manager.,Store the credential in an encrypted string parameter in AWS Systems Manager Parameter Store. Grant permission to the instance role associated with the EC2 instance to access the parameter and the AWS KMS key that is used to encrypt it.,Conﬁgure the Java application to catch a connection failure and make a call to AWS Secrets Manager to retrieve updated credentials when the password is rotated. Grant permission to the instance role associated with the EC2 instance to access Secrets Manager.,,"C, E",1,0,0,1,0,1,0,,,,,4.1,Authentication & Authorization,,85,SCS-C02,AWS Certified Security - Specialty,,"AWS Secrets Manager natively supports automatic rotation for RDS credentials, enabling secure, seamless updates without manual intervention (C). By handling connection failures and fetching the latest secret from Secrets Manager at runtime, the application can continue operating without restarts, minimizing downtime (E)."
1.5,Incident Response,An online media company has an application that customers use to watch events around the world. The application is hosted on a ﬂeet of Amazon EC2 instances that run Amazon Linux 2. The company uses AWS Systems Manager to manage the EC2 instances. The company applies patches and application updates by using the AWS-AmazonLinux2DefaultPatchBaseline patching baseline in Systems Manager Patch Manager. The company is concerned about potential attacks on the application during the week of an upcoming event. The company needs a solution that can immediately deploy patches to all the EC2 instances in response to a security incident or vulnerability. The solution also must provide centralized evidence that the patches were applied successfully. Which combination of steps will meet these requirements? (Choose two.),Create a new patching baseline in Patch Manager. Specify Amazon Linux 2 as the product. Specify Security as the classiﬁcation. Set the automatic approval for patches to 0 days. Ensure that the new patching baseline is the designated default for Amazon Linux 2.,Use the Patch Now option with the scan and install operation in the Patch Manager console to apply patches against the baseline to all nodes. Specify an Amazon S3 bucket as the patching log storage option.,Use the Clone function of Patch Manager to create a copy of the AWS-AmazonLmux2DefaultPatchBaseline built-in baseline. Set the automatic approval for patches to 1 day.,Create a patch policy that patches all managed nodes and sends a patch operation log output to an Amazon S3 bucket. Use a custom scan schedule to set Patch Manager to check every hour for new patches. Assign the baseline to the patch policy.,Use Systems Manager Application Manager to inspect the package versions that were installed on the EC2 instances. Additionally use Application Manager to validate that the patches were correctly installed.,,"A, B",1,0,1,0,0,0,2,,,,,1.1,Preparation & Runbooks,,144,SCS-C02,AWS Certified Security - Specialty,,"Creating a new baseline limited to Security patches with 0-day auto-approval (A) ensures any new security updates are immediately eligible for installation, meeting the urgent response requirement. Using Patch Now with scan and install (B) applies those patches to all instances immediately and stores operation logs in S3, providing centralized evidence of successful patching. Options C and D introduce delays or do not guarantee immediate install, and E does not provide centralized patching evidence."
1.5,Incident Response,"An application has been built with Amazon EC2 instances that retrieve messages from Amazon SQS. Recently, IAM changes were made and the instances can no longer retrieve messages. What actions should be taken to troubleshoot the issue while maintaining least privilege? (Choose two.)",Conﬁgure and assign an MFA device to the role used by the instances.,Verify that the SQS resource policy does not explicitly deny access to the role used by the instances.,Verify that the access key attached to the role used by the instances is active.,Attach the AmazonSQSFullAccess managed policy to the role used by the instances.,Verify that the role attached to the instances contains policies that allow access to the queue.,,"B, E",1,0,1,0,0,0,1,,,,,4.1,Authentication & Authorization,,204,SCS-C02,AWS Certified Security - Specialty,,"To maintain least privilege, first confirm the instance role’s inline/attached policies allow the necessary SQS actions on the specific queue (E), and ensure the queue’s resource policy does not explicitly deny that role (B), which would override allows. MFA devices and access keys are not used by EC2 instance roles, and granting AmazonSQSFullAccess would violate least-privilege principles."
1.5,Incident Response,"A company uses AWS Organizations to manage a small number of AWS accounts. However, the company plans to add 1,000 more accounts soon. The company allows only a centralized security team to create IAM roles for all AWS accounts and teams. Application teams submit requests for IAM roles to the security team. The security team has a backlog of IAM role requests and cannot review and provision the IAM roles quickly. The security team must create a process that will allow application teams to provision their own IAM roles. The process must also limit the scope of IAM roles and prevent privilege escalation. Which solution will meet these requirements with the LEAST operational overhead?",Create an IAM group for each application team. Associate policies with each IAM group. Provision IAM users for each application team member. Add the new IAM users to the appropriate IAM group by using role-based access control (RBAC).,Delegate application team leads to provision IAM roles for each team. Conduct a quarterly review of the IAM roles the team leads have provisioned. Ensure that the application team leads have the appropriate training to review IAM roles.,Put each AWS account in its own OU. Add an SCP to each OU to grant access to only the AWS services that the teams plan to use. Include conditions in the AWS account of each team.,Create an SCP and a permissions boundary for IAM roles. Add the SCP to the root OU so that only roles that have the permissions boundary attached can create any new IAM roles.,,,D,0,0,1,0,0,0,1,,,,,4.3,Permission Boundaries,,236,SCS-C02,AWS Certified Security - Specialty,,"Using an SCP to require that any newly created role must have a specific permissions boundary enforces a global guardrail while allowing teams to self-serve role creation. The permissions boundary caps the maximum permissions of roles, preventing privilege escalation, and the single SCP at the root OU minimizes operational overhead across all accounts. Other options either require heavy manual oversight or do not reliably prevent privilege escalation."
1.5,Incident Response,A company has a strict policy against using root credentials. The company’s security team wants to be alerted as soon as possible when root credentials are used to sign in to the AWS Management Console. How should the security team achieve this goal?,Use AWS Lambda to periodically query AWS CloudTrail for console login events and send alerts using Amazon Simple Notiﬁcation Service (Amazon SNS).,Use Amazon EventBridge to monitor console logins and direct them to Amazon Simple Notiﬁcation Service (Amazon SNS).,Use Amazon Athena to query AWS IAM Identity Center logs and send alerts using Amazon Simple Notiﬁcation Service (Amazon SNS) for root login events.,Conﬁgure AWS Resource Access Manager to review the access logs and send alerts using Amazon Simple Notiﬁcation Service (Amazon SNS).,,,B,0,0,1,0,0,0,1,,,,,2.4,Alerting & Remediation,,254,SCS-C02,AWS Certified Security - Specialty,,"Amazon EventBridge can match AWS CloudTrail ConsoleLogin events with userIdentity.type = ""Root"" and route them to an SNS topic for immediate, near–real-time alerts. This provides an event-driven, managed solution without polling. The other options either introduce latency (periodic queries) or involve unrelated services (IAM Identity Center, RAM)."
1.5,Incident Response,"A company runs workloads that are spread across hundreds of Amazon EC2 instances. During a recent security incident, an EC2 instance was compromised and ran malware code until the company manually terminated the instance. The company is now using Amazon GuardDuty to detect malware on EC2 instances. A security engineer needs to implement a solution that automates a response when GuardDuty determines that an instance is infected. The solution must mitigate the incident and must comply with the AWS Well-Architected Framework guidance for incident response. Which solution will meet these requirements?",Conﬁgure AWS Systems Manager Run Command to run when a GuardDuty scan determines that an instance is infected. Use Run Command to remove all network adapters from the operating system of the infected instance. Use Run Command to also add a tag of “Infected” to the instance.,Create an AWS Lambda function that runs when a GuardDuty scan determines that an instance is infected. Program the Lambda function to delete all elastic network interfaces that are associated with the instance. Program the Lambda function to also add a tag of “Infected” to the instance.,Create an AWS Lambda function that runs when a GuardDuty scan determines that an instance is infected. Program the Lambda function to detach all Amazon Elastic Block Store (Amazon EBS) volumes from the instance. Program the Lambda function to also add a tag of “Infected” to the EBS volumes and to terminate the instance afterward.,Deﬁne a separate VPC to isolate EC2 instances. Deﬁne a security group that does not allow any network traﬃc. Create an AWS Lambda function that runs when a GuardDuty scan determines that an instance is infected. Program the Lambda function to move the instance into the separate VPC and to assign the security group to the instance.,,,C,0,0,1,0,0,0,1,,,,,1.3,Containment & Eradication,,284,SCS-C02,AWS Certified Security - Specialty,,"Option C follows Well-Architected incident response by automating containment and recovery: it terminates the compromised instance to stop malware execution and detaches/tag EBS volumes to preserve evidence for forensics. Triggering a Lambda from GuardDuty findings enables a rapid, repeatable response. Other options are unreliable or unsupported (e.g., modifying NICs from a compromised host, moving instances across VPCs) and may leave malware running."
1.5,Incident Response,A company runs a web application on a ﬂeet of Amazon EC2 instances that are in an Auto Scaling group. The EC2 instances are in the same VPC subnet as other workloads. A security engineer deploys an Amazon GuardDuty detector in the same AWS Region as the EC2 instances. The security engineer also sets up an AWS Security Hub integration with GuardDuty. The security engineer needs to implement an automated solution to detect and appropriately respond to anomalous traﬃc patterns for the web application. The solution must comply with AWS best practices for initial response to security incidents and must minimize disruption to the web application. Which solution will meet these requirements?,Create an Amazon EventBridge rule that detects the Behavior:EC2/TraﬃcVolumeUnusual GuardDuty ﬁnding. Conﬁgure the rule to invoke an AWS Lambda function to disable the EC2 instance proﬁle access keys.,Create an Amazon EventBridge rule that invokes an AWS Lambda function when GuardDuty detects anomalous traﬃc. Program the Lambda function to disassociate the identiﬁed instance from the Auto Scaling group and to isolate the instance by using a new restricted security group.,Create a Security Hub automated response that updates the network ACL that is associated with the subnet of the EC2 instances. Conﬁgure the response to update the network ACL to deny traﬃc from the source of detected anomalous traﬃc.,Create an Amazon Simple Notiﬁcation Service (Amazon SNS) topic. Subscribe the security engineer’s email address to the SNS topic. Conﬁgure GuardDuty to send all ﬁndings to the SNS topic.,,,B,0,0,1,0,0,0,1,,,,,1.2,Detection & Investigation,,304,SCS-C02,AWS Certified Security - Specialty,,Option B follows AWS incident response best practices by automatically isolating only the suspected EC2 instance—removing it from the Auto Scaling group and attaching a restrictive security group—so the rest of the application remains unaffected. This preserves the instance for forensics and stops potential malicious traffic with minimal disruption. The other options either don’t take automated containment action or risk broader impact to the subnet or application.
2.1,Config & Security Hub,A security engineer is conﬁguring a new website that is named example.com. The security engineer wants to secure communications with the website by requiring users to connect to example.com through HTTPS. Which of the following is a valid option for storing SSL/TLS certiﬁcates?,Custom SSL certiﬁcate that is stored in AWS Key Management Service (AWS KMS),Default SSL certiﬁcate that is stored in Amazon CloudFront,Custom SSL certiﬁcate that is stored in AWS Certiﬁcate Manager (ACM),Default SSL certiﬁcate that is stored in Amazon S3,,, ,0,0,1,0,0,0,1,,,,,5.1,Encryption at Rest,,2,SCS-C02,AWS Certified Security - Specialty,,"AWS Certificate Manager (ACM) is the correct place to import or provision and manage custom SSL/TLS certificates for use with services like CloudFront and ALB for domains such as example.com. CloudFront’s default certificate only covers *.cloudfront.net, and KMS or S3 are not used to store/deploy website TLS certificates."
2.1,Config & Security Hub,"Company A has an AWS account that is named Account A. Company A recently acquired Company B, which has an AWS account that is named Account B. Company B stores its ﬁles in an Amazon S3 bucket. The administrators need to give a user from Account A full access to the S3 bucket in Account B. After the administrators adjust the IAM permissions for the user in Account A to access the S3 bucket in Account B, the user still cannot access any ﬁles in the S3 bucket. Which solution will resolve this issue?","In Account B, create a bucket ACL to allow the user from Account A to access the S3 bucket in Account B.","In Account B, create an object ACL to allow the user from Account A to access all the objects in the S3 bucket in Account B.","In Account B, create a bucket policy to allow the user from Account A to access the S3 bucket in Account B.","In Account B, create a user policy to allow the user from Account A to access the S3 bucket in Account B.",,,C,0,0,1,0,0,0,1,,,,,4.1,Authentication & Authorization,,10,SCS-C02,AWS Certified Security - Specialty,,"For cross-account S3 access, you need both permissions on the principal and a resource-based policy on the bucket granting that principal access. A bucket policy in Account B explicitly allows the Account A user to perform actions like ListBucket and GetObject; ACLs are legacy/limited and a user policy in Account B cannot authorize a principal from Account A."
2.1,Config & Security Hub,A company wants to receive an email notiﬁcation about critical ﬁndings in AWS Security Hub. The company does not have an existing architecture that supports this functionality. Which solution will meet the requirement?,Create an AWS Lambda function to identify critical Security Hub ﬁndings. Create an Amazon Simple Notiﬁcation Service (Amazon SNS) topic as the target of the Lambda function. Subscribe an email endpoint to the SNS topic to receive published messages.,Create an Amazon Kinesis Data Firehose delivery stream. Integrate the delivery stream with Amazon EventBridge. Create an EventBridge rule that has a ﬁlter to detect critical Security Hub ﬁndings. Conﬁgure the delivery stream to send the ﬁndings to an email address.,Create an Amazon EventBridge rule to detect critical Security Hub ﬁndings. Create an Amazon Simple Notiﬁcation Service (Amazon SNS) topic as the target of the EventBridge rule. Subscribe an email endpoint to the SNS topic to receive published messages.,Create an Amazon EventBridge rule to detect critical Security Hub ﬁndings. Create an Amazon Simple Email Service (Amazon SES) topic as the target of the EventBridge rule. Use the Amazon SES API to format the message. Choose an email address to be the recipient of the message.,,,C,0,0,1,0,0,0,1,,,,,2.4,Alerting & Remediation,,11,SCS-C02,AWS Certified Security - Specialty,,"Security Hub natively publishes findings to Amazon EventBridge, allowing you to create a rule that filters for critical findings and forwards them to an SNS topic for email notifications. This is the simplest, event-driven pattern without extra components. Options A and D add unnecessary services (Lambda/SES), and B is incorrect because Firehose doesn’t send emails."
2.1,Config & Security Hub,"A company uses AWS Organizations and has production workloads across multiple AWS accounts. A security engineer needs to design a solution that will proactively monitor for suspicious behavior across all the accounts that contain production workloads. The solution must automate remediation of incidents across the production accounts. The solution also must publish a notiﬁcation to an Amazon Simple Notiﬁcation Service (Amazon SNS) topic when a critical security ﬁnding is detected. In addition, the solution must send all security incident logs to a dedicated account. Which solution will meet these requirements?","Activate Amazon GuardDuty in each production account. In a dedicated logging account, aggregate all GuardDuty logs from each production account. Remediate incidents by conﬁguring GuardDuty to directly invoke an AWS Lambda function. Conﬁgure the Lambda function to also publish notiﬁcations to the SNS topic.","Activate AWS Security Hub in each production account. In a dedicated logging account, aggregate all Security Hub ﬁndings from each production account. Remediate incidents by using AWS Conﬁg and AWS Systems Manager. Conﬁgure Systems Manager to also publish notiﬁcations to the SNS topic.","Activate Amazon GuardDuty in each production account. In a dedicated logging account, aggregate all GuardDuty logs from each production account. Remediate incidents by using Amazon EventBridge to invoke a custom AWS Lambda function from the GuardDuty ﬁndings. Conﬁgure the Lambda function to also publish notiﬁcations to the SNS topic.","Activate AWS Security Hub in each production account. In a dedicated logging account, aggregate all Security Hub ﬁndings from each production account. Remediate incidents by using Amazon EventBridge to invoke a custom AWS Lambda function from the Security Hub ﬁndings. Conﬁgure the Lambda function to also publish notiﬁcations to the SNS topic.",,,C,0,0,1,0,0,0,1,,,,,2.3,Monitoring & Detection,,14,SCS-C02,AWS Certified Security - Specialty,,"GuardDuty is the native AWS threat detection service for proactively monitoring suspicious activity and supports multi-account aggregation in a dedicated logging/administrator account. GuardDuty findings are emitted to EventBridge, which can trigger a Lambda function to automate remediation and publish critical alerts to SNS—meeting both automated response and centralized incident logging requirements."
2.1,Config & Security Hub,A security engineer is checking an AWS CloudFormation template for vulnerabilities. The security engineer ﬁnds a parameter that has a default value that exposes an application's API key in plaintext. The parameter is referenced several times throughout the template. The security engineer must replace the parameter while maintaining the ability to reference the value in the template. Which solution will meet these requirements in the MOST secure way?,"Store the API key value as a SecureString parameter in AWS Systems Manager Parameter Store. In the template, replace all references to the value with {{resolve:ssm:MySSMParameterName:1}}.","Store the API key value in AWS Secrets Manager. In the template, replace all references to the value with {{resolve:secretsmanager:MySecretId:SecretString}}.","Store the API key value in Amazon DynamoDB. In the template, replace all references to the value with {{resolve:dynamodb:MyTableName:MyPrimaryKey}}.","Store the API key value in a new Amazon S3 bucket. In the template, replace all references to the value with {{resolve:s3:MyBucketName:MyObjectName}}.",,,B,0,0,1,0,0,0,1,,,,,5.1,Encryption at Rest,,50,SCS-C02,AWS Certified Security - Specialty,,"Using Secrets Manager with a CloudFormation dynamic reference keeps the API key out of the template and parameters, resolving it securely at deployment and supporting encryption, rotation, and auditing. This is the most appropriate service for secrets. Option A is less secure as written (it should use ssm-secure), and S3/DynamoDB are not valid dynamic reference sources for secrets."
2.1,Config & Security Hub,A company has several petabytes of data. The company must preserve this data for 7 years to comply with regulatory requirements. The company's compliance team asks a security oﬃcer to develop a strategy that will prevent anyone from changing or deleting the data. Which solution will meet this requirement MOST cost-effectively?,Create an Amazon S3 bucket. Conﬁgure the bucket to use S3 Object Lock in compliance mode. Upload the data to the bucket. Create a resource-based bucket policy that meets all the regulatory requirements.,Create an Amazon S3 bucket. Conﬁgure the bucket to use S3 Object Lock in governance mode. Upload the data to the bucket. Create a user-based IAM policy that meets all the regulatory requirements.,Create a vault in Amazon S3 Glacier. Create a Vault Lock policy in S3 Glacier that meets all the regulatory requirements. Upload the data to the vault.,Create an Amazon S3 bucket. Upload the data to the bucket. Use a lifecycle rule to transition the data to a vault in S3 Glacier. Create a Vault Lock policy that meets all the regulatory requirements.,,,C,0,0,0,1,0,1,0,,,,,5.4,Data Classification & Access Control,,52,SCS-C02,AWS Certified Security - Specialty,,"S3 Glacier Vault Lock enforces WORM at the vault level with an immutable retention policy, preventing anyone from deleting or modifying data for the required 7 years. Glacier is the most cost-effective storage for multi-year archival of petabytes. S3 Object Lock in compliance mode (A) works but is typically more expensive, governance mode (B) isn’t tamper-proof, and lifecycle to Glacier then applying Vault Lock (D) doesn’t lock S3-managed objects."
2.1,Config & Security Hub,A company has a new partnership with a vendor. The vendor will process data from the company's customers. The company will upload data ﬁles as objects into an Amazon S3 bucket. The vendor will download the objects to perform data processing. The objects will contain sensitive data. A security engineer must implement a solution that prevents objects from residing in the S3 bucket for longer than 72 hours. Which solution will meet these requirements?,Use Amazon Macie to scan the S3 bucket for sensitive data every 72 hours. Conﬁgure Macie to delete the objects that contain sensitive data when they are discovered.,Conﬁgure an S3 Lifecycle rule on the S3 bucket to expire objects that have been in the S3 bucket for 72 hours.,Create an Amazon EventBridge scheduled rule that invokes an AWS Lambda function every day. Program the Lambda function to remove any objects that have been in the S3 bucket for 72 hours.,Use the S3 Intelligent-Tiering storage class for all objects that are uploaded to the S3 bucket. Use S3 Intelligent-Tiering to expire objects that have been in the $3 bucket for 72 hours.,,,B,0,0,1,0,0,0,1,,,,,5.4,Data Classification & Access Control,,60,SCS-C02,AWS Certified Security - Specialty,,"S3 Lifecycle rules natively support object expiration and can automatically delete objects after a specified number of days (e.g., 3 days = 72 hours), meeting the requirement with a managed, low-ops solution. The other options either don’t control deletion based on time (Macie, Intelligent-Tiering) or add unnecessary operational overhead and complexity (Lambda/EventBridge)."
2.1,Config & Security Hub,A company accidentally deleted the private key for an Amazon Elastic Block Store (Amazon EBS)-backed Amazon EC2 instance. A security engineer needs to regain access to the instance. Which combination of steps will meet this requirement? (Choose two.),Stop the instance. Detach the root volume. Generate a new key pair.,Keep the instance running. Detach the root volume. Generate a new key pair.,"When the volume is detached from the original instance, attach the volume to another instance as a data volume. Modify the authorized_keys ﬁle with a new public key. Move the volume back to the original instance. Start the instance.","When the volume is detached from the original instance, attach the volume to another instance as a data volume. Modify the authorized_keys ﬁle with a new private key. Move the volume back to the original instance. Start the instance.","When the volume is detached from the original instance, attach the volume to another instance as a data volume. Modify the authorized_keys ﬁle with a new public key. Move the volume back to the original instance that is running.",,"A, C",1,0,1,0,0,0,2,,,,,1.1,Preparation & Runbooks,,61,SCS-C02,AWS Certified Security - Specialty,,"You must stop the instance to detach its root EBS volume and generate a new key pair (A). Then attach the volume to a helper instance, add the new key pair’s public key to the appropriate user’s ~/.ssh/authorized_keys, reattach it to the original instance, and start it (C). Detaching from a running instance (B/E) isn’t supported, and you should never place a private key in authorized_keys (D)."
2.1,Config & Security Hub,"A company is building an application on AWS that will store sensitive information. The company has a support team with access to the IT infrastructure, including databases. The company’s security engineer must introduce measures to protect the sensitive data against any data breach while minimizing management overhead. The credentials must be regularly rotated. What should the security engineer recommend?",Enable Amazon RDS encryption to encrypt the database and snapshots. Enable Amazon Elastic Block Store (Amazon EBS) encryption on Amazon EC2 instances. Include the database credential in the EC2 user data ﬁeld. Use an AWS Lambda function to rotate database credentials. Set up TLS for the connection to the database.,Install a database on an Amazon EC2 instance. Enable third-party disk encryption to encrypt the Amazon Elastic Block Store (Amazon EBS) volume. Store the database credentials in AWS CloudHSM with automatic rotation. Set up TLS for the connection to the database.,Enable Amazon RDS encryption to encrypt the database and snapshots. Enable Amazon Elastic Black Store (Amazon EBS) encryption on Amazon EC2 instances. Store the database credentials in AWS Secrets Manager with automatic rotation. Set up TLS for the connection to the RDS hosted database.,Set up an AWS CloudHSM cluster with AWS Key Management Service (AWS KMS) to store KMS keys. Set up Amazon RDS encryption using AWS KMS to encrypt the database. Store database credentials in the AWS Systems Manager Parameter Store with automatic rotation. Set up TLS for the connection to the RDS hosted database.,,,C,0,0,1,0,0,0,1,,,,,5.1,Encryption at Rest,,64,SCS-C02,AWS Certified Security - Specialty,,"Option C uses fully managed native services to minimize overhead: RDS encryption (KMS) protects data at rest, EBS encryption secures EC2-hosted data, and AWS Secrets Manager provides built-in, automatic credential rotation. Coupled with TLS for in-transit protection, this addresses breach risk without risky practices (like storing creds in user data) or heavy, complex setups like CloudHSM."
2.1,Config & Security Hub,"A security engineer recently rotated all IAM access keys in an AWS account. The security engineer then conﬁgured AWS Conﬁg and enabled the following AWS Conﬁg managed rules: mfa-enabled-for-iam-console-access, iam-user-mfa-enabled, access-keys-rotated, and iam-user-unused-credentials-check. The security engineer notices that all resources are displaying as noncompliant after the IAM GenerateCredentialReport API operation is invoked. What could be the reason for the noncompliant status?",The IAM credential report was generated within the past 4 hours.,The security engineer does not have the GenerateCredentialReport permission.,The security engineer does not have the GetCredenlialReport permission.,The AWS Conﬁg rules have a MaximumExecutionFrequency value of 24 hours.,,,A,0,0,1,0,0,0,1,,,,,4.1,Authentication & Authorization,,109,SCS-C02,AWS Certified Security - Specialty,,"These AWS Config managed rules evaluate IAM users using data from the IAM credential report, which is refreshed approximately every 4 hours. If the report was generated within the last 4 hours, it may not yet include the recent key rotations, so Config evaluations use stale data and mark resources as noncompliant temporarily."
2.1,Config & Security Hub,A company uses an external identity provider to allow federation into different AWS accounts. A security engineer for the company needs to identify the federated user that terminated a production Amazon EC2 instance a week ago. What is the FASTEST way for the security engineer to identify the federated user?,Review the AWS CloudTrail event history logs in an Amazon S3 bucket and look for the TerminateInstances event to identify the federated user from the role session name.,Filter the AWS CloudTrail event history for the TerminateInstances event and identify the assumed IAM role. Review the AssumeRoleWithSAML event call in CloudTrail to identify the corresponding username.,Search the AWS CloudTrail logs for the TerminateInstances event and note the event time. Review the IAM Access Advisor tab for all federated roles. The last accessed time should match the time when the instance was terminated.,Use Amazon Athena to run a SQL query on the AWS CloudTrail logs stored in an Amazon S3 bucket and ﬁlter on the TerminateInstances event. Identify the corresponding role and run another query to ﬁlter the AssumeRoleWithWebIdentity event for the user name.,,,B,0,0,1,0,0,0,1,,,,,2.3,Monitoring & Detection,,134,SCS-C02,AWS Certified Security - Specialty,,"Filtering CloudTrail event history for TerminateInstances quickly reveals the assumed role used, then correlating it with the AssumeRoleWithSAML event shows the federated user (via role session name/attributes). This uses the console’s event history (fast) and the correct federation mechanism, unlike options that require S3/Athena or rely on less precise Access Advisor data."
2.1,Config & Security Hub,"Two Amazon EC2 instances in different subnets should be able to connect to each other but cannot. It has been conﬁrmed that other hosts in the same subnets are able to communicate successfully, and that security groups have valid ALLOW rules in place to permit this traﬃc. Which of the following troubleshooting steps should be performed?","Check inbound and outbound security groups, looking for DENY rules","Check inbound and outbound Network ACL rules, looking for DENY rules",Review the rejected packet reason codes in the VPC Flow Logs,Use AWS X-Ray to trace the end-to-end application ﬂow,,,B,0,0,0,1,0,1,0,,,,,3.1,Network Architecture Security,,135,SCS-C02,AWS Certified Security - Specialty,,"Network ACLs are stateless and can contain explicit DENY rules that block traffic even if security groups allow it, so checking inbound and outbound NACL rules across both subnets is the right next step. Security groups do not support DENY rules (making A irrelevant), and X-Ray is for application tracing, not network-level connectivity issues."
2.1,Config & Security Hub,A company is worried about potential DDoS attacks. The company has a web application that runs on Amazon EC2 instances. The application uses Amazon S3 to serve static content such as images and videos. A security engineer must create a resilient architecture that can withstand DDoS attacks. Which solution will meet these requirements MOST cost-effectively?,Create an Amazon CloudWatch alarm that invokes an AWS Lambda function when an EC2 instance’s CPU utilization reaches 90%. Program the Lambda function to update security groups that are attached to the EC2 instance to deny inbound ports 80 and 443.,Put the EC2 instances into an Auto Scaling group behind an Elastic Load Balancing (ELB) load balancer. Use Amazon CioudFront with Amazon S3 as an origin.,Set up a warm standby disaster recovery (DR) environment. Fail over to the warm standby DR environment if a DDoS attack is detected on the application.,Subscribe to AWS Shield Advanced. Conﬁgure permissions to allow the Shield Response Team to manage resources on the company's behalf during a DDoS event.,,,B,0,0,1,0,0,0,1,,,,,3.2,Edge & Web Protection,,156,SCS-C02,AWS Certified Security - Specialty,,"Placing EC2 instances in an Auto Scaling group behind an ELB allows the application to scale and absorb traffic spikes, while CloudFront serves cached static content from edge locations, reducing load on the origin. This setup automatically benefits from AWS Shield Standard at no extra cost, providing DDoS mitigation and making it the most cost-effective resilient architecture."
2.1,Config & Security Hub,A company deployed an Amazon EC2 instance to a VPC on AWS. A recent alert indicates that the EC2 instance is receiving a suspicious number of requests over an open TCP port from an external source. The TCP port remains open for long periods of time. The company's security team needs to stop all activity to this port from the external source to ensure that the EC2 instance is not being compromised. The application must remain available to other users. Which solution will meet these requirements?,Update the network ACL that is attached to the subnet that is associated with the EC2 instance. Add a Deny statement for the port and the source IP addresses.,Update the elastic network interface security group that is attached to the EC2 instance to remove the port from the inbound rule list.,Update the elastic network interface security group that is attached to the EC2 instance by adding a Deny entry in the inbound list for the port and the source IP addresses.,Create a new network ACL for the subnet. Deny all traﬃc from the EC2 instance to prevent data from being removed.,,,A,0,0,0,1,0,1,0,,,,,3.1,Network Architecture Security,,181,SCS-C02,AWS Certified Security - Specialty,,"Network ACLs are stateless and support explicit allow/deny rules, letting you block traffic from specific source IPs and ports while allowing other traffic to continue, keeping the application available. Security groups cannot create deny rules (C) and removing the port entirely (B) would block all users. Option D would disrupt all traffic and is not targeted to the malicious source."
2.1,Config & Security Hub,"A company has secured the AWS account root user for its AWS account by following AWS best practices. The company also has enabled AWS CloudTrail, which is sending its logs to Amazon S3. A security engineer wants to receive notiﬁcation in near-real time if a user uses the AWS account root user credentials to sign in to the AWS Management Console Which solutions will provide this notiﬁcation? (Choose two.)",Use AWS Trusted Advisor and its security evaluations for the root account. Conﬁgure an Amazon EventBridge event rule that is invoked by the Trusted Advisor API. Conﬁgure the rule to target an Amazon Simple Notiﬁcation Service (Amazon SNS) topic. Subscribe any required endpoints to the SNS topic so that these endpoints can receive notiﬁcation.,Use AWS IAM Access Analyzer. Create an Amazon Cloud Watch Logs metric ﬁlter to evaluate log entries from Access Analyzer that detect a successful root account login. Create an Amazon CloudWatch alarm that monitors whether a root login has occurred. Conﬁgure the CloudWatch alarm to notify an Amazon Simple Notiﬁcation Service (Amazon SNS) topic when the alarm enters the ALARM state. Subscribe any required endpoints to this SNS topic so that these endpoints can receive notiﬁcation.,Conﬁgure AWS CloudTrail to send its logs to Amazon CloudWatch Logs. Conﬁgure a metric ﬁlter on the CloudWatch Logs log group used by CloudTrail to evaluate log entries for successful root account logins. Create an Amazon CloudWatch alarm that monitors whether a root login has occurred. Conﬁgure the CloudWatch alarm to notify an Amazon Simple Notiﬁcation Service (Amazon SNS) topic when the alarm enters the ALARM state. Subscribe any required endpoints to this SNS topic so that these endpoints can receive notiﬁcation.,Conﬁgure AWS CloudTrail to send log notiﬁcations to an Amazon Simple Notiﬁcation Service (Amazon SNS) topic. Create an AWS Lambda function that parses the CloudTrail notiﬁcation for root login activity and notiﬁes a separate SNS topic that contains the endpoints that should receive notiﬁcation. Subscribe the Lambda function to the SNS topic that is receiving log notiﬁcations from CloudTrail.,Conﬁgure an Amazon EventBridge event rule that runs when Amazon CloudWatch API calls are recorded for a successful root login. Conﬁgure the rule to target an Amazon Simple Notiﬁcation Service (Amazon SNS) topic. Subscribe any required endpoints to the SNS topic so that these endpoints can receive notiﬁcation.,,"C, E",1,0,1,0,0,0,1,,,,,2.3,Monitoring & Detection,,182,SCS-C02,AWS Certified Security - Specialty,,"Option C uses the standard approach: send CloudTrail to CloudWatch Logs, create a metric filter for ConsoleLogin by the root user, and alarm to SNS—delivering near‑real time alerts. Option E leverages EventBridge’s native aws.signin ConsoleLogin events to directly trigger SNS, providing immediate notification. Other options either don’t detect sign-ins (Access Analyzer), aren’t real-time (Trusted Advisor), or misuse CloudTrail/SNS integration."
2.1,Config & Security Hub,A company is storing data in Amazon S3 Glacier. A security engineer implemented a new vault lock policy for 10 TB of data and called the initiate-vault-lock operation 12 hours ago. The audit team identiﬁed a typo in the policy that is allowing unintended access to the vault. What is the MOST cost-effective way to correct this error?,Call the abort-vault-lock operation. Update the policy. Call the initiate-vault-lock operation again.,Copy the vault data to a new S3 bucket. Delete the vault Create a new vault with the data.,Update the policy to keep the vault lock in place.,Update the policy. Call the initiate-vault-lock operation again to apply the new policy.,,,A,0,0,1,0,0,0,1,,,,,5.4,Data Classification & Access Control,,188,SCS-C02,AWS Certified Security - Specialty,,"Glacier Vault Lock has a 24-hour in-progress phase where you can abort the lock, update the policy, and reinitiate it. Since it was initiated 12 hours ago, calling abort-vault-lock, fixing the policy, and re-initiating is allowed and avoids data movement. Other options either aren’t permitted at this stage or would incur unnecessary transfer/storage costs."
2.1,Config & Security Hub,A security engineer is investigating a malware infection that has spread across a set of Amazon EC2 instances. A key indicator of the compromise is outbound traﬃc on TCP port 2905 to a set of command and control hosts on the internet. The security engineer creates a network ACL rule that denies the identiﬁed outbound traﬃc. The security engineer applies the network ACL rule to the subnet of the EC2 instances. The security engineer must identify any EC2 instances that are trying to communicate on TCP port 2905. Which solution will identify the affected EC2 instances with the LEAST operational effort?,Create a Network Access Scope in Amazon VPC Network Access Analyzer. Use the Network Access Scope to identify EC2 instances that try to send traﬃc to TCP port 2905.,"Enable VPC ﬂow logs for the VPC where the affected EC2 instances are located. Conﬁgure the ﬂow logs to capture rejected traﬃc. In the ﬂow logs, search for REJECT records that have a destination TCP port of 2905.",Enable Amazon GuardDuty. Create a custom GuardDuty IP list to create a ﬁnding when an EC2 instance tries to communicate with one of the command and control hosts. Use Amazon Detective to identify the EC2 instances that initiate the communication.,Create a ﬁrewall in AWS Network Firewall. Attach the ﬁrewall to the subnet of the EC2 instances. Create a custom rule to identify and log traﬃc from the ﬁrewall on TCP port 2905. Create an Amazon CloudWatch Logs metric ﬁlter to identify ﬁrewall logs that reference traﬃc on TCP port 2905.,,,B,0,0,1,0,0,0,1,,,,,1.2,Detection & Investigation,,208,SCS-C02,AWS Certified Security - Specialty,,"Enabling VPC flow logs with REJECT traffic captures exactly which instances are attempting outbound connections blocked by the NACL, and you can quickly filter for destination port 2905. This requires minimal setup and no additional services. The other options involve more complex deployments or do not provide straightforward, instance-level detection of actual connection attempts."
2.1,Config & Security Hub,"A company uses AWS Key Management Service (AWS KMS). During an attempt to attach an encrypted Amazon Elastic Block Store (Amazon EBS) volume to an Amazon EC2 instance, the attachment fails. The company discovers that a customer managed key has become unusable because the key material for the key was deleted. The company needs the data that is on the EBS volume. A security engineer must recommend a solution to decrypt the EBS volume’s encrypted data key. The solution must also attach the volume to the EC2 instance. Which solution will meet these requirements?",Import new key material into the key. Attach the EBS volume.,Restore the EBS volume from a snapshot that was taken before the deletion of the key material.,Reimport the same key material that originally was imported into the key. Attach the EBS volume.,Create a new key. Import new key material. Attach the EBS volume.,,,C,0,0,1,0,0,0,1,,,,,5.3,Key Management,,214,SCS-C02,AWS Certified Security - Specialty,,"EBS volumes are encrypted with a data key that is itself encrypted under the original KMS CMK; when imported key material for that CMK is deleted, the CMK becomes unusable. Reimporting the exact same key material into the same CMK restores its ability to decrypt the volume’s encrypted data key so the volume can be attached; new/different key material, a new key, or snapshots (which use the same CMK) will not work."
2.1,Config & Security Hub,A company has used AWS Lambda functions to build an application on AWS. The company’s security engineer implemented Amazon Inspector and activated Lambda standard scanning and Lambda code scanning. The security engineer reviews the Amazon Inspector console and learns that Amazon Inspector is not scanning some of the Lambda functions. The provided reason is that the scan eligibility expired. What should the security engineer do to investigate the reason that the scans are failing?,Validate that the AmazonInspector2ServiceRolePolicy AWS managed policy grants permissions to access Lambda.,Increase the timeout value of the Lambda functions to complete the scans successfully while the code is running.,Build a custom runtime for the unscanned Lambda functions. Include the Amazon Inspector agent in the runtime.,Determine whether the unscanned Lambda functions have been invoked in the last 90 days.,,,D,0,0,1,0,0,0,1,,,,,1.2,Detection & Investigation,,244,SCS-C02,AWS Certified Security - Specialty,,"Amazon Inspector only scans Lambda functions that have been invoked within the last 90 days; if a function hasn’t been invoked, its scan eligibility expires. Therefore, the engineer should check whether the unscanned functions were invoked recently and invoke them to restore scan eligibility. Other options (permissions, timeout, custom agent) do not affect Lambda scan eligibility."
2.1,Config & Security Hub,A security engineer received an Amazon GuardDuty alert indicating a ﬁnding involving the Amazon EC2 instance that hosts the company’s primary website. The GuardDuty ﬁnding received read: UnauthorizedAccess:IAMUser/InstanceCredentialExﬁltration. The security engineer conﬁrmed that a malicious actor used API access keys intended for the EC2 instance from a country where the company does not operate. The security engineer needs to deny access to the malicious actor. What is the ﬁrst step the security engineer should take?,Open the EC2 console and remove any security groups that allow inbound traﬃc from 0.0.0.0/0.,Install the AWS Systems Manager Agent on the EC2 instance and run an inventory report.,Install the Amazon Inspector agent on the host and run an assessment with the CVE rules package.,Open the IAM console and revoke all IAM sessions that are associated with the instance proﬁle.,,,D,0,0,1,0,0,0,1,,,,,4.1,Authentication & Authorization,,245,SCS-C02,AWS Certified Security - Specialty,,"The finding indicates the EC2 instance’s role credentials were exfiltrated and are being used from an unauthorized location. The fastest containment action is to revoke all IAM sessions tied to the instance profile, immediately invalidating the stolen temporary credentials and blocking further API calls. The other options do not stop the active misuse of credentials."
2.1,Config & Security Hub,A company needs to retain data that is stored in Amazon CloudWatch Logs log groups. The company must retain this data for 90 days. The company must receive notiﬁcation in AWS Security Hub when log group retention is not compliant with this requirement. Which solution will provide the appropriate notiﬁcation?,Create a Security Hub custom action to assess the log group retention period.,Create a data protection policy in CloudWatch Logs to assess the log group retention period.,Create a Security Hub automation rule. Conﬁgure the automation rule to assess the log group retention period.,Use the AWS Conﬁg managed rule that assesses the log group retention period. Ensure that AWS Conﬁg integration is enabled in Security Hub.,,,D,0,0,1,0,0,0,1,,,,,2.4,Alerting & Remediation,,262,SCS-C02,AWS Certified Security - Specialty,,"AWS Config provides a managed rule to evaluate CloudWatch Logs log group retention against a required period (e.g., 90 days). Enabling AWS Config’s integration with Security Hub automatically surfaces noncompliant findings in Security Hub for notification and remediation. The other options do not perform compliance evaluation of retention or are not designed for this use case."
2.1,Config & Security Hub,A company has a multi-account strategy that uses an organization in AWS Organizations with all features enabled. The company has enabled trusted access for AWS Account Management. New accounts are provisioned through AWS Control Tower Account Factory. The company must ensure that all new accounts in the organization become AWS Security Hub member accounts. Which solution will meet these requirements with the LEAST development effort?,Enable Security Hub in the organization’s management account. Create an AWS Step Functions workﬂow. Create an Amazon EventBridge rule to invoke the workﬂow when a CreateAccount event occurs.,Enable Security Hub in the organization’s management account. Wait for all new accounts to complete automatic onboarding.,Enable Security Hub in the organization’s management account. Create an AWS Lambda function to enable Security Hub for new accounts. Invoke the Lambda function by using an AWS Control Tower lifecycle event that occurs when a new account is provisioned.,"Use the organization’s management account to designate a Security Hub delegated administrator account. In the delegated administrator account, create a conﬁguration policy to enable Security Hub. Associate the conﬁguration policy with the organization root.",,,D,0,0,1,0,0,0,1,,,,,4.2,Cross-Account & Federation,,271,SCS-C02,AWS Certified Security - Specialty,,"Designating a Security Hub delegated administrator and applying a Security Hub configuration policy at the organization root uses built-in, organization-wide onboarding so all current and future accounts are automatically enabled as members with no custom code. Enabling only in the management account (B) doesn’t auto-enroll new accounts, while A and C require building and maintaining custom automation."
2.1,Config & Security Hub,"A company is implementing new compliance requirements to meet customer needs. According to the new requirements, the company must not use any Amazon RDS DB instances or DB clusters that lack encryption of the underlying storage. The company needs a solution that will generate an email alert when an unencrypted DB instance or DB cluster is created. The solution also must terminate the unencrypted DB instance or DB cluster. Which solution will meet these requirements in the MOST operationally eﬃcient manner?",Create an AWS Conﬁg managed rule to detect unencrypted RDS storage. Conﬁgure an automatic remediation action to publish messages to an Amazon Simple Notiﬁcation Service (Amazon SNS) topic that includes an AWS Lambda function and an email delivery target as subscribers. Conﬁgure the Lambda function to delete the unencrypted resource.,Create an AWS Conﬁg managed rule to detect unencrypted RDS storage. Conﬁgure a manual remediation action to invoke an AWS Lambda function. Conﬁgure the Lambda function to publish messages to an Amazon Simple Notiﬁcation Service (Amazon SNS) topic and to delete the unencrypted resource.,Create an Amazon EventBridge rule that evaluates RDS event patterns and is initiated by the creation of DB instances or DB clusters. Conﬁgure the rule to publish messages to an Amazon Simple Notiﬁcation Service (Amazon SNS) topic that includes an AWS Lambda function and an email delivery target as subscribers. Conﬁgure the Lambda function to delete the unencrypted resource.,Create an Amazon EventBridge rule that evaluates RDS event patterns and is initiated by the creation of DB instances or DB clusters. Conﬁgure the rule to invoke an AWS Lambda function. Conﬁgure the Lambda function to publish messages to an Amazon Simple Notiﬁcation Service (Amazon SNS) topic and to delete the unencrypted resource.,,,A,0,0,1,0,0,0,1,,,,,6.2,Compliance Validation,,294,SCS-C02,AWS Certified Security - Specialty,,"AWS Config has managed rules (e.g., rds-storage-encrypted) that continuously detect unencrypted RDS DB instances/clusters and can trigger automatic remediation, which is the most operationally efficient approach. By sending a notification to SNS (email) and invoking a Lambda subscriber to delete the noncompliant resource, the solution both alerts and enforces compliance. EventBridge options don’t natively evaluate encryption state, and manual remediation is less efficient."
2.1,Config & Security Hub,"A security engineer wants to evaluate conﬁguration changes to a speciﬁc AWS resource to ensure that the resource meets compliance standards. However, the security engineer is concerned about a situation in which several conﬁguration changes are made to the resource in quick succession. The security engineer wants to record only the latest conﬁguration of that resource to indicate the cumulative impact of the set of changes. Which solution will meet this requirement in the MOST operationally eﬃcient way?",Use AWS CloudTrail to detect the conﬁguration changes by ﬁltering API calls to monitor the changes. Use the most recent API call to indicate the cumulative impact of multiple calls.,Use AWS Conﬁg to detect the conﬁguration changes and to record the latest conﬁguration in case of multiple conﬁguration changes.,Use Amazon CloudWatch to detect the conﬁguration changes by ﬁltering API calls to monitor the changes. Use the most recent API call to indicate the cumulative impact of multiple calls.,Use AWS Cloud Map to detect the conﬁguration changes. Generate a report of conﬁguration changes from AWS Cloud Map to track the latest state by using a sliding time window.,,,B,0,0,1,0,0,0,1,,,,,6.2,Compliance Validation,,295,SCS-C02,AWS Certified Security - Specialty,,"AWS Config continuously records resource configuration changes and evaluates them against compliance rules, allowing you to reference the latest configuration item that reflects the cumulative effect of rapid successive changes. This is operationally efficient and purpose-built for compliance state tracking. CloudTrail/CloudWatch focus on API calls (not resulting resource state), and Cloud Map is unrelated."
2.2,Systems Manager & Patch Manager,"A company has a legacy application that runs on a single Amazon EC2 instance. A security audit shows that the application has been using an IAM access key within its code to access an Amazon S3 bucket that is named DOC-EXAMPLE-BUCKET1 in the same AWS account. This access key pair has the s3:GetObject permission to all objects in only this S3 bucket. The company takes the application oﬄine because the application is not compliant with the company’s security policies for accessing other AWS resources from Amazon EC2. A security engineer validates that AWS CloudTrail is turned on in all AWS Regions. CloudTrail is sending logs to an S3 bucket that is named DOC-EXAMPLE-BUCKET2. This S3 bucket is in the same AWS account as DOC-EXAMPLE-BUCKET1. However, CloudTrail has not been conﬁgured to send logs to Amazon CloudWatch Logs. The company wants to know if any objects in DOC-EXAMPLE-BUCKET1 were accessed with the IAM access key in the past 60 days. If any objects were accessed, the company wants to know if any of the objects that are text ﬁles (.txt extension) contained personally identiﬁable information (PII). Which combination of steps should the security engineer take to gather this information? (Choose two.)",Use Amazon CloudWatch Logs Insights to identify any objects in DOC-EXAMPLE-BUCKET1 that contain PII and that were available to the access key.,Use Amazon OpenSearch Service to query the CloudTrail logs in DOC-EXAMPLE-BUCKET2 for API calls that used the access key to access an object that contained PII.,Use Amazon Athena to query the CloudTrail logs in DOC-EXAMPLE-BUCKET2 for any API calls that used the access key to access an object that contained PII.,Use AWS Identity and Access Management Access Analyzer to identify any API calls that used the access key to access objects that contained PII in DOC-EXAMPLE-BUCKET1.,Conﬁgure Amazon Macie to identify any objects in DOC-EXAMPLE-BUCKET1 that contain PII and that were available to the access key.,,"C, E",1,1,,,,0,0,,,,,2.1,Logging Configuration,,21,SCS-C02,AWS Certified Security - Specialty,,"CloudTrail logs are stored in S3 (not CloudWatch Logs), so Amazon Athena is the appropriate service to query those logs and find any GetObject API calls that used the specific access key within the last 60 days. To determine which accessed objects contain PII, Amazon Macie can scan the S3 bucket for sensitive data (including .txt files), identifying PII in the relevant objects."
2.2,Systems Manager & Patch Manager,A company uses AWS Signer with all of the company's AWS Lambda functions. A developer recently stopped working for the company. The company wants to ensure that all the code that the developer wrote can no longer be deployed to the Lambda functions. Which solution will meet this requirement?,Revoke all versions of the signing proﬁle assigned to the developer.,Examine the developer's IAM roles. Remove all permissions that grant access to Signer.,Re-encrypt all source code with a new AWS Key Management Service (AWS KMS) key.,Use Amazon CodeGuru to proﬁle all the code that the Lambda functions use.,,,A,0,1,,,,0,0,,,,,4.1,Authentication & Authorization,,71,SCS-C02,AWS Certified Security - Specialty,,"With Lambda code signing, only code signed by allowed AWS Signer profiles can be deployed. Revoking the developer’s signing profile versions invalidates their signatures, preventing any of their existing or future packages from being accepted by Lambda. Removing IAM access doesn’t invalidate already signed artifacts, and KMS or CodeGuru are unrelated to deployment trust."
2.2,Systems Manager & Patch Manager,A security engineer needs to build a solution to turn AWS CloudTrail back on in multiple AWS Regions in case it is ever turned off. What is the MOST eﬃcient way to implement this solution?,Use AWS Conﬁg with a managed rule to initiate the AWS-EnableCloudTrail remediation.,Create an Amazon EventBridge event with a cloudtrail.amazonaws.com event source and a StartLogging event name to invoke an AWS Lambda function to call the StartLogging API.,Create an Amazon CloudWatch alarm with a cloudtrail.amazonaws.com event source and a StopLoggmg event name to invoke an AWS Lambda function to call the StartLogging API.,Monitor AWS Trusted Advisor to ensure CloudTrail logging is enabled.,,,A,0,0,0,1,0,1,0,,,,,1.1,Preparation & Runbooks,,147,SCS-C02,AWS Certified Security - Specialty,,"AWS Config’s managed rule (e.g., multi-region-cloudtrail-enabled) with the AWS-EnableCloudTrail remediation automatically detects when CloudTrail is disabled and auto-remediates by turning it back on across Regions. Alternatives either only monitor without remediation (Trusted Advisor) or rely on incorrect/inefficient event setups (e.g., StartLogging won’t fire; CloudWatch alarms aren’t for EventBridge pattern events)."
2.2,Systems Manager & Patch Manager,An application team wants to use AWS Certiﬁcate Manager (ACM) to request public certiﬁcates to ensure that data is secured in transit. The domains that are being used are not currently hosted on Amazon Route 53. The application team wants to use an AWS managed distribution and caching solution to optimize requests to its systems and provide better points of presence to customers. The distribution solution will use a primary domain name that is customized. The distribution solution also will use several alternative domain names. The certiﬁcates must renew automatically over an indeﬁnite period of time. Which combination of steps should the application team take to deploy this architecture? (Choose three.),Request a certiﬁcate from ACM in the us-west-2 Region. Add the domain names that the certiﬁcate will secure.,Send an email message to the domain administrators to request validation of the domains for ACM.,Request validation of the domains for ACM through DNS. Insert CNAME records into each domain's DNS zone.,Create an Application Load Balancer for the caching solution. Select the newly requested certiﬁcate from ACM to be used for secure connections.,Create an Amazon CloudFront distribution for the caching solution. Enter the main CNAME record as the Origin Name. Enter the subdomain names or alternate names in the Alternate Domain Names Distribution Settings. Select the newly requested certiﬁcate from ACM to be used for secure connections.,Request a certiﬁcate from ACM in the us-east-1 Region. Add the domain names that the certiﬁcate will secure.,"C, E, F",1,1,,,,0,0,,,,,5.2,Encryption in Transit,,198,SCS-C02,AWS Certified Security - Specialty,,"CloudFront is the AWS-managed distribution and caching service, and it requires ACM certificates to be issued in us-east-1, so you must request the certificate in us-east-1 (F) and attach it to the CloudFront distribution with the custom primary and alternate domain names (E). Since the domains are not in Route 53 and the team needs automatic, ongoing renewal, DNS validation via CNAME records (C) is the correct validation method for ACM to auto-renew."
2.2,Systems Manager & Patch Manager,A company stores sensitive data in AWS Secrets Manager. A security engineer needs to design a solution to generate a notiﬁcation email when anomalous GetSecretValue API calls occur. The security engineer has conﬁgured an Amazon EventBridge rule for all Secrets Manager events that AWS CloudTrail delivers. Which solution will meet these requirements?,Conﬁgure CloudTrail as the target of the EventBridge rule. Set up an attribute ﬁlter on the IncomingBytes attribute and enable anomaly detection. Create an Amazon Simple Notiﬁcation Service (Amazon SNS) topic. Conﬁgure a CloudTrail alarm that uses the SNS topic to send the notiﬁcation.,Conﬁgure CloudTrail as the target of the EventBridge rule. Set up an attribute ﬁlter on the IncomingBytes attribute and enable anomaly detection. Create an Amazon Simple Queue Service (Amazon SQS) queue. Conﬁgure a CloudTrail alarm that uses the SQS queue to send the notiﬁcation.,Conﬁgure Amazon CloudWatch Logs as the target of the EventBridge rule. Set up a metric ﬁlter on the IncomingBytes metric and enable anomaly detection. Create an Amazon Simple Notiﬁcation Service (Amazon SNS) topic. Conﬁgure a CloudWatch alarm that uses the SNS topic to send the notiﬁcation.,Conﬁgure Amazon CloudWatch Logs as the target of the EventBridge rule. Use CloudWatch Logs Insights query syntax to search for anomalous GetSecretValue API calls. Create an Amazon Simple Queue Service (Amazon SQS) queue. Conﬁgure a CloudWatch alarm that uses the SQS queue to send the notiﬁcation.,,,C,0,1,,,,0,0,,,,,2.3,Monitoring & Detection,,257,SCS-C02,AWS Certified Security - Specialty,,"Sending the EventBridge events to a CloudWatch Logs log group allows you to create a log metric filter (e.g., matching GetSecretValue) and apply CloudWatch Anomaly Detection, then alarm to an SNS topic for email notifications. The other options misuse CloudTrail as a target and refer to non-existent “CloudTrail alarms,” or use SQS instead of SNS for email notifications."
2.2,Systems Manager & Patch Manager,A company has a large ﬂeet of Amazon Linux 2 Amazon EC2 instances that run an application. The application processes sensitive data and has the following compliance requirements: • No remote access management ports to the EC2 instances can be exposed internally or externally. • All remote session activity must be recorded in an audit log. • All remote access to the EC2 instances must be authenticated and authorized by AWS IAM Identity Center. The company’s DevOps team occasionally needs to connect to one of the EC2 instances to troubleshoot issues. Which solution will provide remote access to the EC2 instances while meeting the compliance requirements?,Grant access to the EC2 serial console at the account level. Create an IAM policy that allows an IAM role of the DevOps team to access the EC2 serial console.,Enable EC2 instance Connect on the AMI of the EC2 instances. Conﬁgure the appropriate security group rules. Grant EC2 console access to the DevOps team for access to EC2 instance Connect.,Assign an EC2 instance role that allows access to AWS Systems Manager. Create an IAM policy that grants access to Systems Manager Session Manager. Assign the policy to an IAM role of the DevOps team.,Use AWS Systems Manager Automation runbooks to open remote access ports to the EC2 instances. Attach a role to the EC2 instances to allow the runbooks to run.,,,C,0,1,,,,0,0,,,,,4.1,Authentication & Authorization,,307,SCS-C02,AWS Certified Security - Specialty,,"AWS Systems Manager Session Manager enables remote shell access without opening inbound ports, satisfying the requirement to expose no management ports. It integrates with IAM/IAM Identity Center for authentication/authorization and can log all session activity to CloudWatch Logs or S3 for auditing, unlike the other options."
2.3,CloudWatch & EventBridge,A company needs a security engineer to implement a scalable solution for multi-account authentication and authorization. The solution should not introduce additional user-managed architectural components. Native AWS features should be used as much as possible. The security engineer has set up AWS Organizations with all features activated and AWS IAM Identity Center (AWS Single Sign-On) enabled. Which additional steps should the security engineer take to complete the task?,Use AD Connector to create users and groups for all employees that require access to AWS accounts. Assign AD Connector groups to AWS accounts and link to the IAM roles in accordance with the employees’ job functions and access requirements. Instruct employees to access AWS accounts by using the AWS Directory Service user portal.,Use an IAM Identity Center default directory to create users and groups for all employees that require access to AWS accounts. Assign groups to AWS accounts and link to permission sets in accordance with the employees’ job functions and access requirements. Instruct employees to access AWS accounts by using the IAM Identity Center user portal.,Use an IAM Identity Center default directory to create users and groups for all employees that require access to AWS accounts. Link IAM Identity Center groups to the IAM users present in all accounts to inherit existing permissions. Instruct employees to access AWS accounts by using the IAM Identity Center user portal.,Use AWS Directory Service for Microsoft Active Directory to create users and groups for all employees that require access to AWS accounts. Enable AWS Management Console access in the created directory and specify IAM Identity Center as a source of information for integrated accounts and permission sets. Instruct employees to access AWS accounts by using the AWS Directory Service user portal.,,,B,0,1,,,,0,0,,,,,4.1,Authentication & Authorization,,7,SCS-C02,AWS Certified Security - Specialty,,"Option B uses IAM Identity Center’s default directory, permission sets, and the SSO user portal to centrally manage access across AWS Organizations without adding any extra infrastructure. A and D introduce additional directory components (AD Connector/Managed AD), and C misunderstands Identity Center by trying to map groups to IAM users instead of using permission sets and roles."
2.3,CloudWatch & EventBridge,A company has deployed Amazon GuardDuty and now wants to implement automation for potential threats. The company has decided to start with RDP brute force attacks that come from Amazon EC2 instances in the company's AWS environment. A security engineer needs to implement a solution that blocks the detected communication from a suspicious instance until investigation and potential remediation can occur. Which solution will meet these requirements?,Conﬁgure GuardDuty to send the event to an Amazon Kinesis data stream. Process the event with an Amazon Kinesis Data Analytics for Apache Flink application that sends a notiﬁcation to the company through Amazon Simple Notiﬁcation Service (Amazon SNS). Add rules to the network ACL to block traﬃc to and from the suspicious instance.,Conﬁgure GuardDuty to send the event to Amazon EventBridge. Deploy an AWS WAF web ACL. Process the event with an AWS Lambda function that sends a notiﬁcation to the company through Amazon Simple Notiﬁcation Service (Amazon SNS) and adds a web ACL rule to block traﬃc to and from the suspicious instance.,Enable AWS Security Hub to ingest GuardDuty ﬁndings and send the event to Amazon EventBridge. Deploy AWS Network Firewall. Process the event with an AWS Lambda function that adds a rule to a Network Firewall ﬁrewall policy to block traﬃc to and from the suspicious instance.,Enable AWS Security Hub to ingest GuardDuty ﬁndings. Conﬁgure an Amazon Kinesis data stream as an event destination for Security Hub. Process the event with an AWS Lambda function that replaces the security group of the suspicious instance with a security group that does not allow any connections.,,,C,0,1,,,,0,0,,,,,1.3,Containment & Eradication,,8,SCS-C02,AWS Certified Security - Specialty,,"Option C uses GuardDuty findings ingested by Security Hub and routed via EventBridge to a Lambda function that dynamically updates AWS Network Firewall to block traffic to and from the suspicious EC2 instance—providing centralized, VPC-wide isolation until remediation. Other options misuse services: AWS WAF is for HTTP/S, not RDP; NACLs are subnet-level and cumbersome for per-instance blocking; and sending Security Hub events to Kinesis is not a typical or necessary path for this automation."
2.3,CloudWatch & EventBridge,"A company used a lift-and-shift approach to migrate from its on-premises data centers to the AWS Cloud. The company migrated on- premises VMs to Amazon EC2 instances. Now the company wants to replace some of components that are running on the EC2 instances with managed AWS services that provide similar functionality. Initially, the company will transition from load balancer software that runs on EC2 instances to AWS Elastic Load Balancers. A security engineer must ensure that after this transition, all the load balancer logs are centralized and searchable for auditing. The security engineer must also ensure that metrics are generated to show which ciphers are in use. Which solution will meet these requirements?",Create an Amazon CloudWatch Logs log group. Conﬁgure the load balancers to send logs to the log group. Use the CloudWatch Logs console to search the logs. Create CloudWatch Logs ﬁlters on the logs for the required metrics.,Create an Amazon S3 bucket. Conﬁgure the load balancers to send logs to the S3 bucket. Use Amazon Athena to search the logs that are in the S3 bucket. Create Amazon CloudWatch ﬁlters on the S3 log ﬁles for the required metrics.,Create an Amazon S3 bucket. Conﬁgure the load balancers to send logs to the S3 bucket. Use Amazon Athena to search the logs that are in the S3 bucket. Create Athena queries for the required metrics. Publish the metrics to Amazon CloudWatch.,Create an Amazon CloudWatch Logs log group. Conﬁgure the load balancers to send logs to the log group. Use the AWS Management Console to search the logs. Create Amazon Athena queries for the required metrics. Publish the metrics to Amazon CloudWatch.,,,C,0,1,,,,0,0,,,,,2.2,Centralization & Retention,,19,SCS-C02,AWS Certified Security - Specialty,,"Elastic Load Balancer access logs are delivered only to Amazon S3, not CloudWatch Logs, so options A and D are not feasible. From S3, Amazon Athena can query the logs (including TLS cipher fields) to make them searchable and to compute metrics. Those metrics can then be published to Amazon CloudWatch, satisfying the centralization, searchability, and cipher-usage metric requirements."
2.3,CloudWatch & EventBridge,"A company is using Amazon Macie, AWS Firewall Manager, Amazon Inspector, and AWS Shield Advanced in its AWS account. The company wants to receive alerts if a DDoS attack occurs against the account. Which solution will meet this requirement?",Use Macie to detect an active DDoS event. Create Amazon CloudWatch alarms that respond to Macie ﬁndings.,Use Amazon inspector to review resources and to invoke Amazon CloudWatch alarms for any resources that are vulnerable to DDoS attacks.,Create an Amazon CloudWatch alarm that monitors Firewall Manager metrics for an active DDoS event.,Create an Amazon CloudWatch alarm that monitors Shield Advanced metrics for an active DDoS event.,,,D,0,1,,,,0,0,,,,,2.4,Alerting & Remediation,,23,SCS-C02,AWS Certified Security - Specialty,,"AWS Shield Advanced is purpose-built to detect and mitigate DDoS attacks and publishes attack metrics (e.g., DDoSDetected) to CloudWatch, which you can alarm on. Macie and Inspector do not detect DDoS events, and Firewall Manager manages policies rather than emitting attack detection metrics. Therefore, monitoring Shield Advanced metrics with CloudWatch alarms meets the requirement."
2.3,CloudWatch & EventBridge,"A company is hosting a web application on Amazon EC2 instances behind an Application Load Balancer (ALB). The application has become the target of a DoS attack. Application logging shows that requests are coming from a small number of client IP addresses, but the addresses change regularly. The company needs to block the malicious traﬃc with a solution that requires the least amount of ongoing effort. Which solution meets these requirements?","Create an AWS WAF rate-based rule, and attach it to the ALB.",Update the security group that is attached to the ALB to block the attacking IP addresses.,Update the ALB subnet's network ACL to block the attacking client IP addresses.,"Create an AWS WAF rate-based rule, and attach it to the security group of the EC2 instances.",,,A,0,1,,,,0,0,,,,,3.2,Edge & Web Protection,,30,SCS-C02,AWS Certified Security - Specialty,,"An AWS WAF rate-based rule on the ALB automatically detects and blocks IPs that exceed a defined request rate, adapting as the attacking IPs change and minimizing manual maintenance. Security groups and NACLs require continual manual updates to block shifting IPs, and WAF cannot be attached to a security group."
2.3,CloudWatch & EventBridge,A security engineer is using AWS Organizations and wants to optimize SCPs. The security engineer needs to ensure that the SCPs conform to best practices. Which approach should the security engineer take to meet this requirement?,Use AWS IAM Access Analyzer to analyze the polices. View the ﬁndings from policy validation checks.,Review AWS Trusted Advisor checks for all accounts in the organization.,Set up AWS Audit Manager. Run an assessment for all AWS Regions for all accounts.,Ensure that Amazon Inspector agents are installed on all Amazon EC2 instances in all accounts.,,,A,0,1,,,,0,0,,,,,4.2,Cross-Account & Federation,,43,SCS-C02,AWS Certified Security - Specialty,,"IAM Access Analyzer includes policy validation that analyzes IAM policies, resource policies, and SCPs to detect errors and deviations from AWS policy best practices and provides actionable findings to optimize them. Trusted Advisor, Audit Manager, and Amazon Inspector do not validate SCPs, so they won’t ensure SCPs conform to best practices."
2.3,CloudWatch & EventBridge,"A company's security engineer is designing an isolation procedure for Amazon EC2 instances as part of an incident response plan. The security engineer needs to isolate a target instance to block any traﬃc to and from the target instance, except for traﬃc from the company's forensics team. Each of the company's EC2 instances has its own dedicated security group. The EC2 instances are deployed in subnets of a VPC. A subnet can contain multiple instances. The security engineer is testing the procedure for EC2 isolation and opens an SSH session to the target instance. The procedure starts to simulate access to the target instance by an attacker. The security engineer removes the existing security group rules and adds security group rules to give the forensics team access to the target instance on port 22. After these changes, the security engineer notices that the SSH connection is still active and usable. When the security engineer runs a ping command to the public IP address of the target instance, the ping command is blocked. What should the security engineer do to isolate the target instance?",Add an inbound rule to the security group to allow traﬃc from 0.0.0.0/0 for all ports. Add an outbound rule to the security group to allow traﬃc to 0.0.0.0/0 for all ports. Then immediately delete these rules.,Remove the port 22 security group rule. Attach an instance role policy that allows AWS Systems Manager Session Manager connections so that the forensics team can access the target instance.,Create a network ACL that is associated with the target instance's subnet. Add a rule at the top of the inbound rule set to deny all traﬃc from 0.0.0.0/0. Add a rule at the top of the outbound rule set to deny all traﬃc to 0.0.0.0/0.,Create an AWS Systems Manager document that adds a host-level ﬁrewall rule to block all inbound traﬃc and outbound traﬃc. Run the document on the target instance.,,,B,0,1,,,,0,0,,,,,1.3,Containment & Eradication,,46,SCS-C02,AWS Certified Security - Specialty,,"Security groups are stateful, so removing the SSH rule doesn’t tear down the already-established SSH session, which is why the connection persisted while new traffic like ping was blocked. Switching to AWS Systems Manager Session Manager lets the forensics team access the instance over outbound HTTPS with an IAM role (no inbound ports required), allowing you to remove port 22 entirely and effectively isolate the instance."
2.3,CloudWatch & EventBridge,A company has a web server in the AWS Cloud. The company will store the content for the web server in an Amazon S3 bucket. A security engineer must use an Amazon CloudFront distribution to speed up delivery of the content. None of the ﬁles can be publicly accessible from the S3 bucket directly. Which solution will meet these requirements?,Conﬁgure the permissions on the individual ﬁles in the S3 bucket so that only the CloudFront distribution has access to them.,Create an origin access control (OAC). Associate the OAC with the CloudFront distribution. Conﬁgure the S3 bucket permissions so that only the OAC can access the ﬁles in the S3 bucket.,Create an S3 role in AWS Identity and Access Management (IAM). Allow only the CloudFront distribution to assume the role to access the ﬁles in the S3 bucket.,Create an S3 bucket policy that uses only the CloudFront distribution ID as the principal and the Amazon Resource Name (ARN) as the target.,,,B,0,1,,,,0,0,,,,,3.2,Edge & Web Protection,,58,SCS-C02,AWS Certified Security - Specialty,,"CloudFront Origin Access Control (OAC) is the modern, recommended way to restrict S3 bucket access so that only CloudFront can retrieve objects, keeping the bucket private. By associating an OAC with the distribution and updating the S3 bucket policy to allow only the OAC, content is served only through CloudFront. Other options are invalid because CloudFront cannot assume IAM roles or be used directly as a principal in S3 bucket policies."
2.3,CloudWatch & EventBridge,"A security team is working on a solution that will use Amazon EventBridge to monitor new Amazon S3 objects. The solution will monitor for public access and for changes to any S3 bucket policy or setting that result in public access. The security team conﬁgures EventBridge to watch for speciﬁc API calls that are logged from AWS CloudTrail. EventBridge has an action to send an email notiﬁcation through Amazon Simple Notiﬁcation Service (Amazon SNS) to the security team immediately with details of the API call. Speciﬁcally, the security team wants EventBridge to watch for the s3:PutObjectAcl, s3:DeleteBucketPolicy, and s3:PutBucketPolicy API invocation logs from CloudTrail. While developing the solution in a single account, the security team discovers that the s3:PutObjectAcl API call does not invoke an EventBridge event However, the s3:DeleteBucketPolicy API call and the s3:PutBucketPolicy API call do invoke an event. The security team has enabled CloudTrail for AWS management events with a basic conﬁguration in the AWS Region in which EventBridge is being tested. Veriﬁcation of the EventBridge event pattern indicates that the pattern is set up correctly. The security team must implement a solution so that the s3:PutObjectAcl API call will invoke an EventBridge event. The solution must not generate false notiﬁcations. Which solution will meet these requirements?",Modify the EventBridge event pattern by selecting Amazon S3. Select All Events as the event type.,Modify the EventBridge event pattern by selecting Amazon S3. Select Bucket Level Operations as the event type.,Enable CloudTrail Insights to identify unusual API activity.,Enable CloudTrail to monitor data events for read and write operations to S3 buckets.,,,D,0,1,,,,0,0,,,,,2.3,Monitoring & Detection,,73,SCS-C02,AWS Certified Security - Specialty,,"PutBucketPolicy and DeleteBucketPolicy are management events, which are captured by the existing CloudTrail configuration and trigger EventBridge, but PutObjectAcl is an S3 data event (object-level) and is not logged unless S3 data events are enabled. Enabling CloudTrail data events for S3 read/write will log PutObjectAcl to CloudTrail so EventBridge can match and notify, without broadening the event pattern and causing false positives."
2.3,CloudWatch & EventBridge,"A security engineer is asked to update an AWS CloudTrail log ﬁle preﬁx for an existing trail. When attempting to save the change in the CloudTrail console, the security engineer receives the following error message: ""There is a problem with the bucket policy."" What will enable the security engineer to save the change?","Create a new trail with the updated log ﬁle preﬁx, and then delete the original trail. Update the existing bucket policy in the Amazon S3 console with the new log ﬁle preﬁx, and then update the log ﬁle preﬁx in the CloudTrail console.","Update the existing bucket policy in the Amazon S3 console to allow the security engineer's principal to perform PutBucketPolicy, and then update the log ﬁle preﬁx in the CloudTrail console.","Update the existing bucket policy in the Amazon S3 console with the new log ﬁle preﬁx, and then update the log ﬁle preﬁx in the CloudTrail console.","Update the existing bucket policy in the Amazon S3 console to allow the security engineer's principal to perform GetBucketPolicy, and then update the log ﬁle preﬁx in the CloudTrail console.",,,C,0,1,,,,0,0,,,,,2.1,Logging Configuration,,80,SCS-C02,AWS Certified Security - Specialty,,"CloudTrail must have permission in the S3 bucket policy to write to the specific log file prefix. When changing the prefix, the bucket policy must be updated to allow the CloudTrail service to PutObject to the new prefix; otherwise the console reports a bucket policy error. After updating the policy, the prefix change can be saved in the CloudTrail console."
2.3,CloudWatch & EventBridge,"A company uses infrastructure as code (IaC) to create AWS infrastructure. The company writes the code as AWS CloudFormation templates to deploy the infrastructure. The company has an existing CI/CD pipeline that the company can use to deploy these templates. After a recent security audit, the company decides to adopt a policy-as-code approach to improve the company's security posture on AWS. The company must prevent the deployment of any infrastructure that would violate a security policy, such as an unencrypted Amazon Elastic Block Store (Amazon EBS) volume. Which solution will meet these requirements?",Turn on AWS Trusted Advisor. Conﬁgure security notiﬁcations as webhooks in the preferences section of the CI/CD pipeline.,Turn on AWS Conﬁg. Use the prebuilt rules or customized rules. Subscribe tile CI/CD pipeline to an Amazon Simple Notiﬁcation Service (Amazon SNS) topic that receives notiﬁcations from AWS Conﬁg.,Create rule sets in AWS CloudFormation Guard. Run validation checks for CloudFormation templates as a phase of the CI/CD process.,Create rule sets as SCPs. Integrate the SCPs as a part of validation control in a phase of the CI/CD process.,,,C,0,0,0,1,0,1,0,,,,,6.4,Policy Automation,,93,SCS-C02,AWS Certified Security - Specialty,,"CloudFormation Guard enables policy-as-code by defining rule sets and validating CloudFormation templates during the CI/CD pipeline, preventing deployment of noncompliant resources (e.g., unencrypted EBS volumes). Trusted Advisor and AWS Config are reactive and detect issues after deployment. SCPs control allowed API actions at the org level but cannot validate template resource configurations."
2.3,CloudWatch & EventBridge,A company is using Amazon Elastic Container Service (Amazon ECS) to run its container-based application on AWS. The company needs to ensure that the container images contain no severe vulnerabilities. The company also must ensure that only speciﬁc IAM roles and speciﬁc AWS accounts can access the container images. Which solution will meet these requirements with the LEAST management overhead?,Pull images from the public container registry. Publish the images to Amazon Elastic Container Registry (Amazon ECR) repositories with scan on push conﬁgured in a centralized AWS account. Use a CI/CD pipeline to deploy the images to different AWS accounts. Use identity- based policies to restrict access to which IAM principals can access the images.,Pull images from the public container registry. Publish the images to a private container registry that is hosted on Amazon EC2 instances in a centralized AWS account. Deploy host-based container scanning tools to EC2 instances that run Amazon ECS. Restrict access to the container images by using basic authentication over HTTPS.,Pull images from the public container registry. Publish the images to Amazon Elastic Container Registry (Amazon ECR) repositories with scan on push conﬁgured in a centralized AWS account. Use a CI/CD pipeline to deploy the images to different AWS accounts. Use repository policies and identity-based policies to restrict access to which IAM principals and accounts can access the images.,Pull images from the public container registry. Publish the images to AWS CodeArtifact repositories in a centralized AWS account. Use a CI/CD pipeline to deploy the images to different AWS accounts. Use repository policies and identity-based policies to restrict access to which IAM principals and accounts can access the images.,,,C,0,1,,,,0,0,,,,,3.4,Container & Serverless Security,,96,SCS-C02,AWS Certified Security - Specialty,,"Amazon ECR supports built-in scan-on-push for vulnerability detection and allows both repository (resource-based) policies and IAM identity policies, enabling precise restrictions by IAM roles and by specific AWS accounts with minimal admin overhead. Option A lacks repository policies for cross-account control, B adds significant management by hosting a private registry and scanners, and D uses CodeArtifact, which isn’t intended for container images."
2.3,CloudWatch & EventBridge,A company hosts a public website on an Amazon EC2 instance. HTTPS traﬃc must be able to access the website. The company uses SSH for management of the web server. The website is on the subnet 10.0.1.0/24. The management subnet is 192.168.100.0/24. A security engineer must create a security group for the EC2 instance. Which combination of steps should the security engineer take to meet these requirements in the MOST secure manner? (Choose two.),Allow port 22 from source 0.0.0.0/0.,Allow port 443 from source 0.0 0 0/0.,Allow port 22 from 192.168.100.0/24.,Allow port 22 from 10.0.1.0/24.,Allow port 443 from 10.0.1.0/24.,,"B, C",1,1,,,,0,0,,,,,3.1,Network Architecture Security,,105,SCS-C02,AWS Certified Security - Specialty,,Port 443 must be open to 0.0.0.0/0 so the public can reach the site. SSH should be limited to the management network (192.168.100.0/24) for least privilege. Opening SSH to 0.0.0.0/0 or limiting either port to the web subnet (10.0.1.0/24) would be insecure or block required access.
2.3,CloudWatch & EventBridge,A company needs to follow security best practices to deploy resources from an AWS CloudFormation template. The CloudFormation template must be able to conﬁgure sensitive database credentials. The company already uses AWS Key Management Service (AWS KMS) and AWS Secrets Manager. Which solution will meet the requirements?,Use a dynamic reference in the CloudFormation template to reference the database credentials in Secrets Manager.,Use a parameter in the CloudFormation template to reference the database credentials. Encrypt the CloudFormation template by using AWS KMS.,Use a SecureString parameter in the CloudFormation template to reference the database credentials in Secrets Manager.,Use a SecureString parameter in the CloudFormation template to reference an encrypted value in AWS KMS.,,,A,0,1,,,,0,0,,,,,5.3,Key Management,,115,SCS-C02,AWS Certified Security - Specialty,,"Dynamic references let CloudFormation securely retrieve secrets from AWS Secrets Manager at deployment time without embedding or exposing them in the template, change sets, or logs. This leverages KMS-backed encryption in Secrets Manager and adheres to best practices for handling sensitive credentials. Other options either misuse SecureString/parameters or risk exposing secrets."
2.3,CloudWatch & EventBridge,"A company is implementing a new application in a new AWS account. A VPC and subnets have been created for the application. The application has been peered to an existing VPC in another account in the same AWS Region for database access Amazon EC2 instances will regularly be created and terminated in the application VPC, but only some of them will need access to the databases in the peered VPC over TCP port 1521. A security engineer must ensure that only the EC2 instances that need access to the databases can access them through the network. How can the security engineer implement this solution?",Create a new security group in the database VPC and create an inbound rule that allows all traﬃc from the IP address range of the application VPC. Add a new network ACL rule on the database subnets. Conﬁgure the rule to TCP port 1521 from the IP address range of the application VPC. Attach the new security group to the database instances that the application instances need to access.,Create a new security group in the application VPC with an inbound rule that allows the IP address range of the database VPC over TCP port 1521. Create a new security group in the database VPC with an inbound rule that allows the IP address range of the application VPC over port 1521. Attach the new security group to the database instances and the application instances that need database access.,Create a new security group in the application VPC with no inbound rules. Create a new security group in the database VPC with an inbound rule that allows TCP port 1521 from the new application security group in the application VPAttach the application security group to the application instances that need database access and attach the database security group to the database instances.,Create a new security group in the application VPC with an inbound rule that allows the IP address range of the database VPC over TCP port 1521. Add a new network ACL rule on the database subnets. Conﬁgure the rule to allow all traﬃc from the IP address range of the application VPC. Attach the new security group to the application instances that need database access.,,,C,0,1,,,,0,0,,,,,3.1,Network Architecture Security,,122,SCS-C02,AWS Certified Security - Specialty,,"Security groups can reference a security group in a peered VPC within the same Region, enabling instance-level, stateful control instead of broad CIDR/NACL rules. By allowing TCP 1521 from the application SG and attaching that SG only to EC2 instances that need access, only those instances can reach the databases even as instances churn. The other options over-permit access with CIDR ranges or unnecessary NACL changes."
2.3,CloudWatch & EventBridge,A company uses Amazon EC2 Linux instances in the AWS Cloud. A member of the company’s security team recently received a report about common vulnerability identiﬁers on the instances. A security engineer needs to verify patching and perform remediation if the instances do not have the correct patches installed. The security engineer must determine which EC2 instances are at risk and must implement a solution to automatically update those instances with the applicable patches. What should the security engineer do to meet these requirements?,Use AWS Systems Manager Patch Manager to view vulnerability identiﬁers for missing patches on the instances. Use Patch Manager also to automate the patching process.,Use AWS Shield Advanced to view vulnerability identiﬁers for missing patches on the instances. Use AWS Systems Manager Patch Manager to automate the patching process.,Use Amazon GuardDuty to view vulnerability identiﬁers for missing patches on the instances. Use Amazon inspector to automate the patching process.,Use Amazon inspector to view vulnerability identiﬁers for missing patches on the instances. Use Amazon Inspector also to automate the patching process.,,,A,0,1,,,,0,0,,,,,3.3,Host & Instance Hardening,,127,SCS-C02,AWS Certified Security - Specialty,,"AWS Systems Manager Patch Manager can scan EC2 instances for missing patches, map them to CVEs, report compliance, and automatically apply approved patches via patch baselines and maintenance windows. Shield Advanced and GuardDuty don’t assess patch compliance, and while Amazon Inspector identifies vulnerabilities, it does not automate patching."
2.3,CloudWatch & EventBridge,A company has two AWS accounts: Account A and Account B. Each account has a VPC. An application that runs in the VPC in Account A needs to write to an Amazon S3 bucket in Account B. The application in Account A already has permission to write to the S3 bucket in Account B. The application and the S3 bucket are in the same AWS Region. The company cannot send network traﬃc over the public internet. Which solution will meet these requirements?,"In both accounts, create a transit gateway and VPC attachments in a subnet in each Availability Zone. Update the VPC route tables.",Deploy a software VPN appliance in Account A. Create a VPN connection between the software VPN appliance and a virtual private gateway in Account B.,"Create a VPC peering connection between the VPC in Account A and the VPC in Account B. Update the VPC route tables, network ACLs, and security groups to allow network traﬃc between the peered IP ranges","In Account A, create a gateway VPC endpoint for Amazon S3. Update the VPC route table in Account A.",,,D,0,1,,,,0,0,,,,,4.2,Cross-Account & Federation,,142,SCS-C02,AWS Certified Security - Specialty,,"A gateway VPC endpoint for Amazon S3 in Account A provides private connectivity to S3 over the AWS network, satisfying the no-internet requirement while leveraging the existing cross-account bucket permissions. S3 is accessed as a service, so VPC peering or transit gateways are unnecessary, and a VPN would still traverse the internet. The endpoint keeps traffic within AWS and can be restricted via endpoint and bucket policies if needed."
2.3,CloudWatch & EventBridge,A company is undergoing a layer 3 and layer 4 DDoS attack on its web servers running on AWS. Which combination of AWS services and features will provide protection in this scenario? (Choose three.),Amazon Route 53,AWS Certiﬁcate Manager (ACM),Amazon S3,AWS Shield,Network Load Balancer,Amazon GuardDuty,"A, D, E",1,1,,,,0,0,,,,,3.2,Edge & Web Protection,,152,SCS-C02,AWS Certified Security - Specialty,,"Route 53 uses a global anycast network with built-in DDoS resilience to absorb and route DNS traffic during volumetric attacks. AWS Shield provides managed L3/L4 DDoS mitigation for Route 53 and load balancers, and a Network Load Balancer operates at layer 4 to scale and distribute large TCP/UDP floods to healthy targets. ACM, S3, and GuardDuty do not directly mitigate network/transport-layer DDoS attacks."
2.3,CloudWatch & EventBridge,A company has a web-based application that runs behind an Application Load Balancer (ALB). The application is experiencing a credential stuﬃng attack that is producing many failed login attempts. The attack is coming from many IP addresses. The login attempts are using a user agent string of a known mobile device emulator. A security engineer needs to implement a solution to mitigate the credential stuﬃng attack. The solution must still allow legitimate logins to the application. Which solution will meet these requirements?,Create an Amazon CloudWatch alarm that reacts to login attempts that contain the speciﬁed user agent string Add an Amazon Simple Notiﬁcation Service (Amazon SNS) topic to the alarm.,Modify the inbound security group on the ALB to deny traﬃc from the IP addresses that are involved in the attack.,Create an AWS WAF web ACL for the ALB Create a custom rule that blocks requests that contain the user agent string of the device emulator.,Create an AWS WAF web ACL for the ALB. Create a custom rule that allows requests from legitimate user agent strings.,,,C,0,1,,,,0,0,,,,,2.3,Monitoring & Detection,,162,SCS-C02,AWS Certified Security - Specialty,,"AWS WAF on the ALB can inspect the User-Agent header and block requests that match the known malicious emulator string, mitigating the attack while allowing other traffic. This avoids the impractical task of blocking many changing IPs and provides active protection, unlike CloudWatch/SNS alerts. An allowlist of user agents could block legitimate clients and is harder to maintain."
2.3,CloudWatch & EventBridge,A company deploys its application as a service on an Amazon Elastic Container Service (Amazon ECS) cluster with theAWS Fargate launch type. A security engineer suspects that some incoming requests are malicious. The security engineer needs to inspect the running container by retrieving log ﬁles and memory dump ﬂies. Which solution will meet these requirements with the LEAST operational effort?,Migrate the application to an ECS cluster with the Amazon EC2 launch type. Conﬁgure the EC2 instances with proper remote access. Log in and inspect the container.,Update the application to dump the required data to STDOUT. Use the awslogs log driver to pass the logs to Amazon CloudWatch Logs. Examine the log ﬁles in CloudWatch Logs.,Turn on Amazon CloudWatch Container Insights for the ECS cluster. Send the log data to Amazon CloudWatch Logs by using AWS Distro for OpenTelemetry. Examine the log data in CloudWatch Logs.,Update the ECS task role with AWS Systems Manager permissions. Enable the ECS Exec feature for the ECS service. Use ECS Exec to inspect the container.,,,D,0,1,,,,0,0,,,,,3.4,Container & Serverless Security,,169,SCS-C02,AWS Certified Security - Specialty,,"ECS Exec allows secure, on-demand interactive access to running containers on Fargate via AWS Systems Manager without managing servers or opening inbound ports, enabling retrieval of files and running commands to generate memory dumps. Options A and B require migration or code changes and still wouldn’t provide direct container access, and C focuses on metrics/telemetry rather than enabling in-container inspection or file retrieval."
2.3,CloudWatch & EventBridge,"A security engineer needs to run an AWS CloudFormation script. The CloudFormation script builds AWS infrastructure to support a stack that includes web servers and a MySQL database. The stack has been deployed in pre-production environments and is ready for production. The production script must comply with the principle of least privilege. Additionally, separation of duties must exist between the security engineer’s IAM account and CloudFormation. Which solution will meet these requirements?",Use IAM Access Analyzer policy generation to generate a policy that allows the CloudFormation script to run and manage the stack. Attach the policy to a new IAM role. Modify the security engineer's IAM permissions to be able to pass the new role to CloudFormation.,Create an IAM policy that allows ec2:* and rds:* permissions. Attach the policy to a new IAM role. Modify the security engineer's IAM permissions to be able to assume the new role.,Use IAM Access Analyzer policy generation to generate a policy that allows the CloudFormation script to run and manage the stack. Modify the security engineer's IAM permissions to be able to run the CloudFormation script.,Create an IAM policy that allows ec2:* and rds:* permissions. Attach the policy to a new IAM role. Use the IAM policy simulator to conﬁrm that the policy allows the AWS API calls that are necessary to build the stack. Modify the security engineer's IAM permissions to be able to pass the new role to CloudFormation.,,,A,0,1,,,,0,0,,,,,4.1,Authentication & Authorization,,174,SCS-C02,AWS Certified Security - Specialty,,"Using IAM Access Analyzer to auto-generate a policy for the specific stack actions enforces least privilege, and attaching it to a dedicated IAM role for CloudFormation provides a proper service role. Granting the security engineer only the iam:PassRole permission to that role ensures separation of duties between the engineer’s account and CloudFormation. Other options either grant overly broad permissions (ec2:* and rds:*) or let the engineer run the stack directly, violating least privilege and separation."
2.3,CloudWatch & EventBridge,A company is using an Amazon CloudFront distribution to deliver content from two origins. One origin is a dynamic application that is hosted on Amazon EC2 instances. The other origin is an Amazon S3 bucket for static assets. A security analysis shows that HTTPS responses from the application do not comply with a security requirement to provide an X-Frame- Options HTTP header to prevent frame-related cross-site scripting attacks. A security engineer must make the full stack compliant by adding the missing HTTP header to the responses. Which solution will meet these requirements?,Create a Lambda@Edge function. Include code to add the X-Frame-Options header to the response. Conﬁgure the function to run in response to the CloudFront origin response event.,Create a Lambda@Edge function. Include code to add the X-Frame-Options header to the response. Conﬁgure the function to run in response to the CloudFront viewer request event.,Update the CloudFront distribution by adding X-Frame-Options to custom headers in the origin settings.,Customize the EC2 hosted application to add the X-Frame-Options header to the responses that are returned to CloudFront.,,,A,0,1,,,,0,0,,,,,2.3,Monitoring & Detection,,203,SCS-C02,AWS Certified Security - Specialty,,"A Lambda@Edge function on the origin response event can inject the X-Frame-Options header into HTTP responses from CloudFront origins before they are sent to viewers, covering both the EC2 app and the S3 origin without changing the origins. Viewer request runs before a response exists, and CloudFront custom origin headers only affect requests to the origin, not responses. Modifying the EC2 app (D) would not centrally enforce the header at the CDN layer and would not address other origins."
2.3,CloudWatch & EventBridge,A company has a requirement that no Amazon EC2 security group can allow SSH access from the CIDR block 0.0.0.0/0. The company wants to monitor compliance with this requirement at all times and wants to receive a near-real-time notiﬁcation if any security group is noncompliant. A security engineer has conﬁgured AWS Conﬁg and will use the restricted-ssh managed rule to monitor the security groups. What should the security engineer do next to meet these requirements?,Conﬁgure AWS Conﬁg to send its conﬁguration snapshots to an Amazon S3 bucket. Create an AWS Lambda function to run on a PutEvent to the S3 bucket. Conﬁgure the Lambda function to parse the snapshot for a compliance change to the restricted-ssh managed rule. Conﬁgure the Lambda function to send a notiﬁcation to an Amazon Simple Notiﬁcation Service (Amazon SNS) topic if a change is discovered.,Conﬁgure an Amazon EventBridge event rule that is invoked by a compliance change event from AWS Conﬁg for the restricted-ssh managed rule. Conﬁgure the event rule to target an Amazon Simple Notiﬁcation Service (Amazon SNS) topic that will provide a notiﬁcation.,Conﬁgure AWS Conﬁg to push all its compliance notiﬁcations to Amazon CloudWatch Logs. Conﬁgure a CloudWatch Logs metric ﬁlter on the AWS Conﬁg log group to look for a compliance notiﬁcation change on the restricted-ssh managed rule. Create an Amazon CloudWatch alarm on the metric ﬁlter to send a notiﬁcation to an Amazon Simple Notiﬁcation Service (Amazon SNS) topic if the alarm is in the ALARM state.,Conﬁgure an Amazon CloudWatch alarm on the CloudWatch metric for the restricted-ssh managed rule. Conﬁgure the CloudWatch alarm to send a notiﬁcation to an Amazon Simple Notiﬁcation Service (Amazon SNS) topic if the alarm is in the ALARM state.,,,B,0,1,,,,0,0,,,,,2.3,Monitoring & Detection,,212,SCS-C02,AWS Certified Security - Specialty,,AWS Config emits compliance change events for managed rules to Amazon EventBridge in near real time. Creating an EventBridge rule for the restricted-ssh rule and targeting an SNS topic provides immediate notifications without extra processing. The other options add unnecessary complexity or rely on less direct/real-time mechanisms.
2.3,CloudWatch & EventBridge,A company uses an organization in AWS Organizations to help separate its Amazon EC2 instances and VPCs. The company has separate OUs for development workloads and production workloads. A security engineer must ensure that only AWS accounts in the production OU can write VPC ﬂow logs to an Amazon S3 bucket. The security engineer is conﬁguring the S3 bucket policy with a Condition element to allow the s3:PutObject action for VPC ﬂow logs. How should the security engineer conﬁgure the Condition element to meet these requirements?,Set the value of the aws:SourceOrgID condition key to be the organization ID.,Set the value of the aws:SourceOrgPaths condition key to be the Organizations entity path of the production OU.,Set the value of the aws:ResourceOrgID condition key to be the organization ID.,Set the value of the aws:ResourceOrgPaths condition key to be the Organizations entity path of the production OU.,,,B,0,1,,,,0,0,,,,,4.1,Authentication & Authorization,,232,SCS-C02,AWS Certified Security - Specialty,,"Using aws:SourceOrgPaths lets you scope the bucket policy to only allow requests from principals whose AWS Organizations path matches the production OU, ensuring only those accounts can write VPC flow logs. Using aws:SourceOrgID would allow any account in the organization, and the aws:ResourceOrg* keys apply to the resource owner, not the calling principal."
2.3,CloudWatch & EventBridge,A developer is receiving AccessDenied errors when the developer invokes API calls to AWS services from a workstation. The developer previously conﬁgured environment variables and conﬁguration ﬁles on the workstation to use multiple roles with other AWS accounts. A security engineer needs to help the developer conﬁgure authentication. The current credentials must be evaluated without conﬂicting with other credentials that were previously conﬁgured on the workstation. Where these credentials should be conﬁgured to meet this requirement?,In the local AWS CLI conﬁguration ﬁle,As environment variables on the local workstation,As variables in the AWS CLI command line options,In the AWS shared conﬁguration ﬁle,,,C,0,1,,,,0,0,,,,,4.1,Authentication & Authorization,,237,SCS-C02,AWS Certified Security - Specialty,,"AWS CLI command line options (e.g., --profile) have the highest precedence, overriding environment variables and configuration files. This lets the developer explicitly select or supply credentials for a single invocation without altering or conflicting with existing workstation configurations. It enables immediate evaluation of the current credentials safely and temporarily."
2.3,CloudWatch & EventBridge,A company runs its microservices architecture in Kubernetes containers on AWS by using Amazon Elastic Kubernetes Service (Amazon EKS) and Amazon Aurora The company has an organization in AWS Organizations to manage hundreds of AWS accounts that host different microservices. The company needs to implement a monitoring solution for logs from all AWS resources across all accounts. The solution must include automatic detection of security-related issues. Which solution will meet these requirements with the LEAST operational effort?,Designate an Amazon GuardDuty administrator account in the organization’s management account. Enable GuardDuty for all accounts. Enable EKS Protection and RDS Protection in the GuardDuty administrator account.,Designate a monitoring account. Share Amazon CloudWatch logs from all accounts with the monitoring account. Conﬁgure Aurora to publish all logs to CloudWatch. Use Amazon Inspector in the monitoring account to evaluate the CloudWatch logs.,Create a central Amazon S3 bucket in the organization’s management account. Conﬁgure AWS CloudTrail in all AWS accounts to deliver CloudTrail logs to the S3 bucket. Conﬁgure Aurora to publish all logs to CloudTrail. Use Amazon Athena to query the CloudTrail logs in the S3 bucket for security issues.,Designate a monitoring account. Share Amazon CloudWatch logs from all accounts with the monitoring account. Subscribe an Amazon Kinesis data stream to the CloudWatch logs. Create AWS Lambda functions to process log records in the data stream to detect security issues.,,,A,0,1,,,,0,0,,,,,2.3,Monitoring & Detection,,252,SCS-C02,AWS Certified Security - Specialty,,"GuardDuty supports organization-wide deployment with a delegated administrator and provides managed, automatic threat detection across accounts, including EKS Protection and RDS (Aurora) Protection—meeting the requirement with minimal operational overhead. The other options require building custom log pipelines or manual analysis, and Amazon Inspector does not analyze CloudWatch logs, leading to higher effort and gaps in coverage."
2.3,CloudWatch & EventBridge,"A company uses a collaboration application. A security engineer needs to conﬁgure automated alerts from AWS Security Hub in the us- west-2 Region for the application. The security engineer wants to receive an alert in a channel in the application every time Security Hub receives a new ﬁnding. The security engineer creates an AWS Lambda function to convert the message to the format that the application requires. The Lambda function also sends the message to the application’s API. The security engineer conﬁgures a corresponding Amazon EventBridge rule that speciﬁes the Lambda function as the target. After the EventBridge rule is implemented, the channel begins to constantly receive alerts from Security Hub. Many of the alerts are Amazon Inspector alerts that do not require any action. The security engineer wants to stop the Amazon Inspector alerts. Which solution will meet this requirement with the LEAST operational effort?",Update the Lambda function code to ﬁnd pattern matches of events from Amazon Inspector and to suppress the ﬁndings.,Create a Security Hub custom action that automatically sends ﬁndings from all services except Amazon Inspector to the EventBridge event bus.,Modify the value of the ProductArn attribute in the event pattern of the EventBridge rule to “anything-but”: [“arn:aws:securityhub:us- west-2::product/aws/inspector”].,Create an Amazon Simple Notiﬁcation Service (Amazon SNS) topic to send messages to the application. Set a ﬁlter policy on the topic subscriptions to reject any messages that contain the product/aws/inspector string.,,,C,0,1,,,,0,0,,,,,2.3,Monitoring & Detection,,267,SCS-C02,AWS Certified Security - Specialty,,"Using an EventBridge event pattern with an anything-but match on ProductArn cleanly filters out Amazon Inspector findings before they trigger the Lambda function, minimizing noise with a simple config change. This is the least operational effort compared to code changes (A), custom actions that don’t auto-filter all findings (B), or adding SNS with filter policies and extra wiring (D)."
2.3,CloudWatch & EventBridge,A company runs an application that sends logs to a log group in Amazon CloudWatch Logs. The email addresses of the application users are in the logs. The company’s developers need to view the logs in CloudWatch Logs. A security engineer must ensure that the developers who access the log group cannot see the user email addresses. Which solution will meet this requirement?,Use Amazon Macie to scan the log group. Conﬁgure Macie to use a custom data identiﬁer that uses a regular expression to identify an email address pattern. Activate automated data discovery in Macie.,Create an AWS Key Management Service (AWS KMS) key. Conﬁgure the log group to use the key to encrypt the logs. Conﬁgure the key policy to deny access to the IAM role that the developers assume to use CloudWatch Logs.,Create a subscription ﬁlter for the log group. Conﬁgure the log subscription to send the log data to an AWS Lambda function. Program the Lambda function to parse the log entries and to mask values that are email addresses.,Conﬁgure a data protection policy for the log group. Specify the AWS managed data identiﬁer of EmailAddress for the type of data to mask. Activate data protection for the log group.,,,D,0,1,,,,0,0,,,,,4.3,Permission Boundaries,,277,SCS-C02,AWS Certified Security - Specialty,,"CloudWatch Logs data protection policies can automatically detect and mask sensitive data in logs at query and view time using managed data identifiers like EmailAddress, letting developers see logs without exposing emails. KMS encryption (B) doesn’t redact content, Macie (A) doesn’t mask CloudWatch Logs, and a subscription with Lambda (C) only masks copies, not the original logs developers view."
2.3,CloudWatch & EventBridge,A company hosts its microservices application on Amazon Elastic Kubernetes Service (Amazon EKS). The company has set up continuous deployments to update the application on demand. A security engineer must implement a solution to provide automatic detection of anomalies in application logs in near real time. The solution also must send notiﬁcations about these anomalies to the security team. Which solution will meet these requirements?,Conﬁgure Amazon CloudWatch Container Insights to collect and aggregate EKS application logs. Create a CloudWatch alarm to monitor for anomalies. Conﬁgure the alarm to launch an AWS Lambda function to alert the security team when anomalies are detected.,Conﬁgure Amazon EKS to send application logs to Amazon CloudWatch. Create a CloudWatch alarm based on a log group metric ﬁlter. Specify anomaly detection as the threshold type. Conﬁgure the alarm to use Amazon Simple Notiﬁcation Service (Amazon SNS) to alert the security team.,Conﬁgure Amazon EKS to export logs to Amazon S3. Use Amazon Athena queries to analyze the logs for anomalies. Use Amazon QuickSight to visualize and monitor user access requests for anomalies. Conﬁgure Amazon Simple Notiﬁcation Service (Amazon SNS) notiﬁcations to alert the security team.,Conﬁgure AWS App Mesh to monitor the traﬃc to the microservices in Amazon EKS. Integrate App Mesh with AWS CloudTrail for logging. Use Amazon Detective to analyze the logs for anomalies and to alert the security team when anomalies are detected.,,,B,0,1,,,,0,0,,,,,2.3,Monitoring & Detection,,287,SCS-C02,AWS Certified Security - Specialty,,"Sending EKS application logs to CloudWatch Logs and creating a metric filter with CloudWatch anomaly detection provides near real-time detection of unusual log patterns. The alarm can natively notify the security team via SNS, meeting the automation and alerting requirements. Other options are either not focused on log anomalies or are not near real time."
2.3,CloudWatch & EventBridge,A company is investigating an increase in its AWS monthly bill. The company discovers that bad actors compromised some Amazon EC2 instances and served webpages for a large email phishing campaign. A security engineer must implement a solution to monitor for cost increases in the future to help detect malicious activity. Which solution will offer the company the EARLIEST detection of cost increases?,Create an Amazon EventBridge rule that invokes an AWS Lambda function hourly. Program the Lambda function to download an AWS usage report from AWS Data Exports about usage of all services. Program the Lambda function to analyze the report and to send a notiﬁcation when anomalies are detected.,Create a cost monitor in AWS Cost Anomaly Detection. Conﬁgure an individual alert to notify an Amazon Simple Notiﬁcation Service (Amazon SNS) topic when the percentage above the expected cost exceeds a threshold.,Review AWS Cost Explorer daily to detect anomalies in cost from prior months. Review the usage of any services that experience a signiﬁcant cost increase from prior months.,Capture VPC ﬂow logs from the VPC where the EC2 instances run. Use a third-party network analysis tool to analyze the ﬂow logs and to detect anomalies in network traﬃc that might increase cost.,,,B,0,1,,,,0,0,,,,,2.3,Monitoring & Detection,,302,SCS-C02,AWS Certified Security - Specialty,,"AWS Cost Anomaly Detection uses machine learning to continuously model normal spend and can send SNS alerts as soon as it detects an anomalous cost spike, providing the earliest automated notification. The alternatives are delayed or indirect: Data Exports are generated on a delay, Cost Explorer requires manual daily review, and VPC flow logs focus on traffic patterns rather than immediate cost anomalies."
2.4,Secrets Manager & Parameter Store,A company has public certiﬁcates that are managed by AWS Certiﬁcate Manager (ACM). The certiﬁcates are either imported certiﬁcates or managed certiﬁcates from ACM with mixed validation methods. A security engineer needs to design a monitoring solution to provide alerts by email when a certiﬁcate is approaching its expiration date. What is the MOST operationally eﬃcient way to meet this requirement?,Create an AWS Lambda function to list all certiﬁcates and to go through each certiﬁcate to describe the certiﬁcate by using the AWS SDK. Filter on the NotAfter attribute and send an email notiﬁcation. Use an Amazon EventBridge rate expression to schedule the Lambda function to run daily.,Create an Amazon CloudWatch alarm. Add all the certiﬁcate ARNs in the AWS/CertiﬁcateManager namespace to the DaysToExpiry metric. Conﬁgure the alarm to publish a notiﬁcation to an Amazon Simple Notiﬁcation Service (Amazon SNS) topic when the value for the DaysToExpiry metric is less than or equal to 31.,Set up AWS Security Hub. Turn on the AWS Foundational Security Best Practices standard with integrated ACM to send ﬁndings. Conﬁgure and use a custom action by creating a rule to match the pattern from the ACM ﬁndings on the NotBefore attribute as the event source. Create an Amazon Simple Notiﬁcation Service (Amazon SNS) topic as the target.,Create an Amazon EventBridge rule by using a predeﬁned pattern for ACM Choose the metric in the ACM Certiﬁcate Approaching Expiration event as the event pattern. Create an Amazon Simple Notiﬁcation Service (Amazon SNS) topic as the target.,,,D,0,1,,,,0,0,,,,,2.3,Monitoring & Detection,,193,SCS-C02,AWS Certified Security - Specialty,,"ACM emits a built-in EventBridge event (ACM Certificate Approaching Expiration) for both imported and ACM-managed certificates, which can be directly routed to SNS for email alerts—no custom code or polling required. This is the most operationally efficient approach. The other options either rely on custom Lambda logic or misuse metrics/attributes and add unnecessary complexity."
2.4,Secrets Manager & Parameter Store,"A security engineer is setting up an AWS CloudTrail trail for all regions in an AWS account. For added security, the logs are stored using server-side encryption with AWS KMS-managed keys (SSE-KMS) and have log integrity validation enabled. While testing the solution, the security engineer discovers that the digest ﬁles are readable, but the log ﬁles are not. What is the MOST likely cause?",The log ﬁles fail integrity validation and automatically are marked as unavailable.,The KMS key policy does not grant the security engineer’s IAM user or role permissions to decrypt with it.,The bucket is set up to use server-side encryption with Amazon S3-managed keys (SSE-S3) as the default and does not allow SSE-KMS- encrypted ﬁles.,An IAM policy applicable to the security engineer’s IAM user or role denies access to the “CloudTrail/” preﬁx in the Amazon S3 bucket.,,,B,0,1,,,,0,0,,,,,2.1,Logging Configuration,,219,SCS-C02,AWS Certified Security - Specialty,,"With SSE-KMS enabled, CloudTrail log files are encrypted using the specified KMS key and require kms:Decrypt permission to read, while digest files are not KMS-encrypted and remain readable with only S3 permissions. Since the digest files are readable but the log files are not, the most likely cause is missing KMS decrypt permissions for the security engineer’s IAM principal."
3.1,IAM Policies & SCPs,A security engineer needs to develop a process to investigate and respond to potential security events on a company's Amazon EC2 instances. All the EC2 instances are backed by Amazon Elastic Block Store (Amazon EBS). The company uses AWS Systems Manager to manage all the EC2 instances and has installed Systems Manager Agent (SSM Agent) on all the EC2 instances. The process that the security engineer is developing must comply with AWS security best practices and must meet the following requirements: A compromised EC2 instance's volatile memory and non-volatile memory must be preserved for forensic purposes. A compromised EC2 instance's metadata must be updated with corresponding incident ticket information. A compromised EC2 instance must remain online during the investigation but must be isolated to prevent the spread of malware. Any investigative activity during the collection of volatile data must be captured as part of the process. Which combination of steps should the security engineer take to meet these requirements with the LEAST operational overhead? (Choose three.),Gather any relevant metadata for the compromised EC2 instance. Enable termination protection. Isolate the instance by updating the instance's security groups to restrict access. Detach the instance from any Auto Scaling groups that the instance is a member of. Deregister the instance from any Elastic Load Balancing (ELB) resources.,Gather any relevant metadata for the compromised EC2 instance. Enable termination protection. Move the instance to an isolation subnet that denies all source and destination traﬃc. Associate the instance with the subnet to restrict access. Detach the instance from any Auto Scaling groups that the instance is a member of. Deregister the instance from any Elastic Load Balancing (ELB) resources.,Use Systems Manager Run Command to invoke scripts that collect volatile data.,Establish a Linux SSH or Windows Remote Desktop Protocol (RDP) session to the compromised EC2 instance to invoke scripts that collect volatile data.,Create a snapshot of the compromised EC2 instance's EBS volume for follow-up investigations. Tag the instance with any relevant metadata and incident ticket information.,Create a Systems Manager State Manager association to generate an EBS volume snapshot of the compromised EC2 instance. Tag the instance with any relevant metadata and incident ticket information.,"A, C, E",1,1,,,,0,0,,,,,1.2,Detection & Investigation,,3,SCS-C02,AWS Certified Security - Specialty,,Updating security groups and detaching from ELB/ASG (A) isolates the instance while keeping it online and protected from termination. Using Systems Manager Run Command (C) collects volatile data and automatically logs investigator actions without needing SSH/RDP. Creating an EBS snapshot and tagging the instance (E) preserves non-volatile data and records incident ticket metadata.
3.1,IAM Policies & SCPs,"A company uses several AWS CloudFormation stacks to handle the deployment of a suite of applications. The leader of the company's application development team notices that the stack deployments fail with permission errors when some team members try to deploy the stacks. However, other team members can deploy the stacks successfully. The team members access the account by assuming a role that has a speciﬁc set of permissions that are necessary for the job responsibilities of the team members. All team members have permissions to perform operations on the stacks. Which combination of steps will ensure consistent deployment of the stacks MOST securely? (Choose three.)",Create a service role that has a composite principal that contains each service that needs the necessary permissions. Conﬁgure the role to allow the sts:AssumeRole action.,Create a service role that has cloudformation.amazonaws.com as the service principal. Conﬁgure the role to allow the sts:AssumeRole action.,"For each required set of permissions, add a separate policy to the role to allow those permissions. Add the ARN of each CloudFormation stack in the resource ﬁeld of each policy.","For each required set of permissions, add a separate policy to the role to allow those permissions. Add the ARN of each service that needs the permissions in the resource ﬁeld of the corresponding policy.",Update each stack to use the service role. F Add a policy to each member role to allow the iam:PassRole action. Set the policy's resource ﬁeld to the ARN of the service role.,,"B, D, E",1,1,,,,0,0,,,,,4.1,Authentication & Authorization,,18,SCS-C02,AWS Certified Security - Specialty,,"Use a dedicated CloudFormation service role trusted by cloudformation.amazonaws.com (B) and attach least‑privilege policies that grant the required actions on the target service resources (D). Update each stack to use this service role (E) so CloudFormation, not the individual users, performs the operations, ensuring consistent, secure deployments. Composite principals and scoping permissions to stack ARNs do not provide the necessary underlying service access."
3.1,IAM Policies & SCPs,"A company developed an application by using AWS Lambda, Amazon S3, Amazon Simple Notiﬁcation Service (Amazon SNS), and Amazon DynamoDB. An external application puts objects into the company's S3 bucket and tags the objects with date and time. A Lambda function periodically pulls data from the company's S3 bucket based on date and time tags and inserts speciﬁc values into a DynamoDB table for further processing. The data includes personally identiﬁable information (PII). The company must remove data that is older than 30 days from the S3 bucket and the DynamoDB table. Which solution will meet this requirement with the MOST operational eﬃciency?",Update the Lambda function to add a TTL S3 ﬂag to S3 objects. Create an S3 Lifecycle policy to expire objects that are older than 30 days by using the TTL S3 ﬂag.,Create an S3 Lifecycle policy to expire objects that are older than 30 days. Update the Lambda function to add the TTL attribute in the DynamoDB table. Enable TTL on the DynamoDB table to expire entries that are older than 30 days based on the TTL attribute.,Create an S3 Lifecycle policy to expire objects that are older than 30 days and to add all preﬁxes to the S3 bucket. Update the Lambda function to delete entries that are older than 30 days.,Create an S3 Lifecycle policy to expire objects that are older than 30 days by using object tags. Update the Lambda function to delete entries that are older than 30 days.,,,B,0,1,,,,0,0,,,,,5.4,Data Classification & Access Control,,26,SCS-C02,AWS Certified Security - Specialty,,"Option B leverages managed, automated deletion in both services: S3 Lifecycle policies to expire objects after 30 days and DynamoDB TTL to remove items based on a TTL attribute, minimizing custom code and operations. This meets the PII removal requirement with built-in features rather than Lambda-driven deletions. Other options either rely on custom deletion logic or reference non-existent features (e.g., an S3 TTL flag)."
3.1,IAM Policies & SCPs,What are the MOST secure ways to protect the AWS account root user of a recently opened AWS account? (Choose two.),Use the AWS account root user access keys instead of the AWS Management Console.,Enable multi-factor authentication for the AWS IAM users with the AdministratorAccess managed policy attached to them.,Use AWS KMS to encrypt all AWS account root user and AWS IAM access keys and set automatic rotation to 30 days.,"Do not create access keys for the AWS account root user; instead, create AWS IAM users.",Enable multi-factor authentication for the AWS account root user.,,"D, E",1,1,,,,0,0,,,,,4.1,Authentication & Authorization,,27,SCS-C02,AWS Certified Security - Specialty,,"Best practice is to never create or use root access keys; instead, create IAM users/roles for daily administration (D). You should also enable MFA on the root user to add a strong second factor to the account’s most privileged credentials (E). Using root credentials (A) is discouraged, MFA for IAM admins (B) doesn’t directly protect the root user, and KMS can’t encrypt or auto-rotate IAM/root access keys (C)."
3.1,IAM Policies & SCPs,A company manages multiple AWS accounts using AWS Organizations. The company’s security team notices that some member accounts are not sending AWS CloudTrail logs to a centralized Amazon S3 logging bucket. The security team wants to ensure there is at least one trail conﬁgured for all existing accounts and for any account that is created in the future. Which set of actions should the security team implement to accomplish this?,Create a new trail and conﬁgure it to send CloudTrail logs to Amazon S3. Use Amazon EventBridge to send notiﬁcation if a trail is deleted or stopped.,"Deploy an AWS Lambda function in every account to check if there is an existing trail and create a new trail, if needed.",Edit the existing trail in the Organizations management account and apply it to the organization.,Create an SCP to deny the cloudtrail:Delete* and cloudtrail:Stop* actions. Apply the SCP to all accounts.,,,C,0,1,,,,0,0,,,,,2.1,Logging Configuration,,39,SCS-C02,AWS Certified Security - Specialty,,"Creating an organization trail from the Organizations management account automatically applies the trail to all existing and future member accounts and can centralize logs to a single S3 bucket. Other options only alert or restrict deletion/stop actions or require per-account management, and do not guarantee a trail exists across all accounts."
3.1,IAM Policies & SCPs,"A-company uses a third-party identity provider and SAML-based SSO for its AWS accounts. After the third-party identity provider renewed an expired signing certiﬁcate, users saw the following message when trying to log in: Error: Response Signature Invalid (Service: AWSSecurityTokenService; Status Code: 400; Error Code: InvalidIdentityToken) A security engineer needs to provide a solution that corrects the error and minimizes operational overhead. Which solution meets these requirements?",Upload the third-party signing certiﬁcate’s new private key to the AWS identity provider entity deﬁned in AWS Identity and Access Management (IAM) by using the AWS Management Console.,Sign the identity provider's metadata ﬁle with the new public key. Upload the signature to the AWS identity provider entity deﬁned in AWS Identity and Access Management (IAM) by using the AWS CLI.,Download the updated SAML metadata ﬁle from the identity service provider. Update the ﬁle in the AWS identity provider entity deﬁned in AWS Identity and Access Management (IAM) by using the AWS CLI.,Conﬁgure the AWS identity provider entity deﬁned in AWS Identity and Access Management (IAM) to synchronously fetch the new public key by using the AWS Management Console.,,,C,0,1,,,,0,0,,,,,4.1,Authentication & Authorization,,53,SCS-C02,AWS Certified Security - Specialty,,"The error occurs because the IdP’s signing certificate changed, but the IAM SAML provider still has the old certificate, causing signature validation to fail. Updating the IAM SAML provider with the new SAML metadata file (which includes the new public key) restores trust with minimal effort. Options A, B, and D are invalid because you never upload private keys, you don’t upload a separate signature, and IAM cannot auto-fetch new keys."
3.1,IAM Policies & SCPs,"A company has contracted with a third party to audit several AWS accounts. To enable the audit, cross-account IAM roles have been created in each account targeted for audit. The auditor is having trouble accessing some of the accounts. Which of the following may be causing this problem? (Choose three.)",The external ID used by the auditor is missing or incorrect.,The auditor is using the incorrect password.,The auditor has not been granted sts:AssumeRole for the role in the destination account.,The Amazon EC2 role used by the auditor must be set to the destination account role.,The secret key used by the auditor is missing or incorrect.,The role ARN used by the auditor is missing or incorrect.,"A, C, F",1,1,,,,0,0,,,,,4.1,Authentication & Authorization,,68,SCS-C02,AWS Certified Security - Specialty,,"For cross-account access via STS, the auditor must call sts:AssumeRole on the exact role ARN and must be allowed to do so by their identity policy; if an ExternalId condition is configured in the role’s trust policy, it must be supplied correctly. If any of these are wrong, the assume-role call fails, preventing access. Passwords, long-term access keys, or EC2 role settings are not directly involved in assuming a cross-account role in this scenario."
3.1,IAM Policies & SCPs,A company's security team needs to receive a notiﬁcation whenever an AWS access key has not been rotated in 90 or more days. A security engineer must develop a solution that provides these notiﬁcations automatically. Which solution will meet these requirements with the LEAST amount of effort?,"Deploy an AWS Conﬁg managed rule to run on a periodic basis of 24 hours. Select the access-keys-rotated managed rule, and set the maxAccessKeyAge parameter to 90 days. Create an Amazon EventBridge rule with an event pattern that matches the compliance type of NON_ COMPLIANT from AWS Conﬁg for the managed rule. Conﬁgure EventBridge to send an Amazon Simple Notiﬁcation Service (Amazon SNS) notiﬁcation to the security team.",Create a script to export a .csv ﬁle from the AWS Trusted Advisor check for IAM access key rotation. Load the script into an AWS Lambda function that will upload the .csv ﬁle to an Amazon S3 bucket. Create an Amazon Athena table query that runs when the .csv ﬁle is uploaded to the S3 bucket. Publish the results for any keys older than 90 days by using an invocation of an Amazon Simple Notiﬁcation Service (Amazon SNS) notiﬁcation to the security team.,"Create a script to download the IAM credentials report on a periodic basis. Load the script into an AWS Lambda function that will run on a schedule through Amazon EventBridge. Conﬁgure the Lambda script to load the report into memory and to ﬁlter the report for records in which the key was last rotated at least 90 days ago. If any records are detected, send an Amazon Simple Notiﬁcation Service (Amazon SNS) notiﬁcation to the security team.",Create an AWS Lambda function that queries the IAM API to list all the users. Iterate through the users by using the ListAccessKeys operation. Verify that the value in the CreateDate ﬁeld is not at least 90 days old. Send an Amazon Simple Notiﬁcation Service (Amazon SNS) notiﬁcation to the security team if the value is at least 90 days old. Create an Amazon EventBridge rule to schedule the Lambda function to run each day.,,,A,0,1,,,,0,0,,,,,4.1,Authentication & Authorization,,76,SCS-C02,AWS Certified Security - Specialty,,"Option A uses the AWS Config managed rule access-keys-rotated with a 90-day parameter and EventBridge to send SNS notifications on NON_COMPLIANT findings, providing a fully managed, low-maintenance solution. The other options require custom scripts, Lambda code, or data processing, which increases effort and operational overhead."
3.1,IAM Policies & SCPs,"A company maintains an open-source application that is hosted on a public GitHub repository. While creating a new commit to the repository, an engineer uploaded their AWS access key and secret access key. The engineer reported the mistake to a manager, and the manager immediately disabled the access key. The company needs to assess the impact of the exposed access key. A security engineer must recommend a solution that requires the least possible managerial overhead. Which solution meets these requirements?",Analyze an AWS Identity and Access Management (IAM) use report from AWS Trusted Advisor to see when the access key was last used.,Analyze Amazon CloudWatch Logs for activity by searching for the access key.,Analyze VPC ﬂow logs for activity by searching for the access key.,Analyze a credential report in AWS Identity and Access Management (IAM) to see when the access key was last used.,,,D,0,1,,,,0,0,,,,,1.2,Detection & Investigation,,77,SCS-C02,AWS Certified Security - Specialty,,"An IAM credential report directly shows each access key’s last-used time and service, allowing a quick impact assessment with minimal setup or overhead. Trusted Advisor’s IAM checks are limited, and searching CloudWatch Logs or VPC flow logs for the key is unreliable (API usage is evidenced in CloudTrail, not flow logs)."
3.1,IAM Policies & SCPs,A security engineer is conﬁguring a mechanism to send an alert when three or more failed sign-in attempts to the AWS Management Console occur during a 5-minute period. The security engineer creates a trail in AWS CloudTrail to assist in this work. Which solution will meet these requirements?,"In CloudTrail, turn on Insights events on the trail. Conﬁgure an alarm on the insight with eventName matching ConsoleLogin and errorMessage matching ""Failed authentication''. Conﬁgure a threshold of 3 and a period of 5 minutes.","Conﬁgure CloudTrail to send events to Amazon CloudWatch Logs. Create a metric ﬁlter for the relevant log group. Create a ﬁlter pattern with eventName matching ConsoleLogin and errorMessage matching ""Failed authentication"". Create a CloudWatch alarm with a threshold of 3 and a period of 5 minutes.","Create an Amazon Athena table from the CloudTrail events. Run a query for eventName matching ConsoleLogin and for errorMessage matching ""Failed authentication"". Create a notiﬁcation action from the query to send an Amazon Simple Notiﬁcation Service (Amazon SNS) notiﬁcation when the count equals 3 within a period of 5 minutes.","In AWS Identity and Access Management Access Analyzer, create a new analyzer. Conﬁgure the analyzer to send an Amazon Simple Notiﬁcation Service (Amazon SNS) notiﬁcation when a failed sign-in event occurs 3 times for any IAM user within a period of 5 minutes.",,,B,0,1,,,,0,0,,,,,2.3,Monitoring & Detection,,89,SCS-C02,AWS Certified Security - Specialty,,"CloudTrail can deliver events to CloudWatch Logs, where a metric filter can detect ConsoleLogin events with ""Failed authentication"" and a CloudWatch alarm can alert when the count reaches 3 within 5 minutes—matching the requirement. CloudTrail Insights doesn’t filter specific events, Athena isn’t a real-time alerting mechanism, and IAM Access Analyzer doesn’t monitor sign-in failures."
3.1,IAM Policies & SCPs,A security engineer wants to use Amazon Simple Notiﬁcation Service (Amazon SNS) to send email alerts to a company's security team for Amazon GuardDuty ﬁndings that have a High severity level. The security engineer also wants to deliver these ﬁndings to a visualization tool for further examination. Which solution will meet these requirements?,"Set up GuardDuty to send notiﬁcations to an Amazon CloudWatch alarm with two targets in CloudWatch. From CloudWatch, stream the ﬁndings through Amazon Kinesis Data Streams into an Amazon Open Search Service domain as the ﬁrst target for delivery. Use Amazon QuickSight to visualize the ﬁndings. Use OpenSearch queries for further analysis. Deliver email alerts to the security team by conﬁguring an SNS topic as a second target for the CloudWatch alarm. Use event pattern matching with an Amazon EventBridge event rule to send only High severity ﬁndings in the alerts.","Set up GuardDuty to send notiﬁcations to AWS CloudTrail with two targets in CloudTrail. From CloudTrail, stream the ﬁndings through Amazon Kinesis Data Firehose into an Amazon OpenSearch Service domain as the ﬁrst target for delivery. Use OpenSearch Dashboards to visualize the ﬁndings. Use OpenSearch queries for further analysis. Deliver email alerts to the security team by conﬁguring an SNS topic as a second target for CloudTrail. Use event pattern matching with a CloudTrail event rule to send only High severity ﬁndings in the alerts.","Set up GuardDuty to send notiﬁcations to Amazon EventBridge with two targets. From EventBridge, stream the ﬁndings through Amazon Kinesis Data Firehose into an Amazon OpenSearch Service domain as the ﬁrst target for delivery. Use OpenSearch Dashboards to visualize the ﬁndings. Use OpenSearch queries for further analysis. Deliver email alerts to the security team by conﬁguring an SNS topic as a second target for EventBridge. Use event pattern matching with an EventBridge event rule to send only High severity ﬁndings in the alerts.","Set up GuardDuty to send notiﬁcations to Amazon EventBridge with two targets. From EventBridge, stream the ﬁndings through Amazon Kinesis Data Streams into an Amazon OpenSearch Service domain as the ﬁrst target for delivery. Use Amazon QuickSight to visualize the ﬁndings. Use OpenSearch queries for further analysis. Deliver email alerts to the security team by conﬁguring an SNS topic as a second target for EventBridge. Use event pattern matching with an EventBridge event rule to send only High severity ﬁndings in the alerts.",,,C,0,1,,,,0,0,,,,,2.3,Monitoring & Detection,,101,SCS-C02,AWS Certified Security - Specialty,,"GuardDuty natively integrates with Amazon EventBridge, allowing an EventBridge rule to filter only High severity findings and route them to multiple targets. One target can be an SNS topic for email alerts, and another can be a Kinesis Data Firehose stream delivering to Amazon OpenSearch Service for analysis in OpenSearch Dashboards. Other options rely on unsupported/less appropriate integrations (e.g., CloudTrail/CloudWatch or Kinesis Data Streams and QuickSight) for this use case."
3.1,IAM Policies & SCPs,A company is designing a new application stack. The design includes web servers and backend servers that are hosted on Amazon EC2 instances. The design also includes an Amazon Aurora MySQL DB cluster. The EC2 instances are in an Auto Scaling group that uses launch templates. The EC2 instances for the web layer and the backend layer are backed by Amazon Elastic Block Store (Amazon EBS) volumes. No layers are encrypted at rest A security engineer needs to implement encryption at rest. Which combination of steps will meet these requirements? (Choose two.),Modify EBS default encryption settings in the target AWS Region to enable encryption. Use an Auto Scaling group instance refresh.,Modify the launch templates for the web layer and the backend layer to add AWS Certiﬁcate Manager (ACM) encryption for the attached EBS volumes. Use an Auto Scaling group instance refresh.,Create a new AWS Key Management Service (AWS KMS) encrypted DB cluster from a snapshot of the existing DB cluster.,Apply AWS Key Management Service (AWS KMS) encryption to the existing DB cluster.,Apply AWS Certiﬁcate Manager (ACM) encryption to the existing DB cluster.,,"A, C",1,1,,,,0,0,,,,,5.1,Encryption at Rest,,120,SCS-C02,AWS Certified Security - Specialty,,"Enabling EBS default encryption in the Region and performing an Auto Scaling instance refresh ensures all newly launched web and backend EC2 instances get encrypted EBS volumes. For Aurora, an unencrypted cluster cannot be encrypted in place, so you must create a new KMS-encrypted cluster from a snapshot and cut over. Options referencing ACM or in-place DB encryption are invalid because ACM is for TLS certificates and Aurora at-rest encryption cannot be retrofitted."
3.1,IAM Policies & SCPs,A company has a large ﬂeet of Linux Amazon EC2 instances and Windows EC2 instances that run in private subnets. The company wants all remote administration to be performed as securely as possible in the AWS Cloud. Which solution will meet these requirements?,Do not use SSH-RSA private keys during the launch of new instances Implement AWS Systems Manager Session Manager,Generate new SSH-RSA private keys for existing instances Implement AWS Systems Manager Session Manager,Do not use SSH-RSA private keys during the launch of new instances Conﬁgure EC2 Instance Connect,Generate new SSH-RSA private keys for existing instances Conﬁgure EC2 Instance Connect,,,A,0,1,,,,0,0,,,,,3.1,Network Architecture Security,,132,SCS-C02,AWS Certified Security - Specialty,,"AWS Systems Manager Session Manager provides secure, auditable CLI/console access to both Linux and Windows instances in private subnets without opening inbound ports or managing SSH/RDP keys. Since Session Manager uses the SSM Agent and IAM, there’s no need to use SSH-RSA keys at launch; EC2 Instance Connect is Linux-only and still relies on SSH access. Thus, option A best delivers the most secure, unified remote administration approach."
3.1,IAM Policies & SCPs,"A company has decided to move its ﬂeet of Linux-based web server instances to an Amazon EC2 Auto Scaling group. Currently, the instances are static and are launched manually. When an administrator needs to view log ﬁles, the administrator uses SSH to establish a connection to the instances and retrieves the logs manually. The company often needs to query the logs to produce results about application sessions and user issues. The company does not want its new automatically scaling architecture to result in the loss of any log ﬁles when instances are scaled in. Which combination of steps should a security engineer take to meet these requirements MOST cost-effectively? (Choose two.)",Conﬁgure a cron job on the instances to forward the log ﬁles to Amazon S3 periodically.,Conﬁgure AWS Glue and Amazon Athena to query the log ﬁles.,Conﬁgure the Amazon CloudWatch agent on the instances to forward the logs to Amazon CloudWatch Logs.,Conﬁgure Amazon CloudWatch Logs Insights to query the log ﬁles.,Conﬁgure the instances to write the logs to an Amazon Elastic File System (Amazon EFS) volume.,,"C, D",1,1,,,,0,0,,,,,2.2,Centralization & Retention,,133,SCS-C02,AWS Certified Security - Specialty,,"Using the CloudWatch agent (C) centralizes logs in CloudWatch Logs, preserving them when instances scale in and eliminating the need for SSH access. CloudWatch Logs Insights (D) provides on-demand, cost-effective querying without building ETL pipelines or maintaining additional storage. Alternatives like S3/Athena/Glue or EFS add complexity, cost, or risk missing logs during instance termination."
3.1,IAM Policies & SCPs,A company has an application that needs to get objects from an Amazon S3 bucket. The application runs on Amazon EC2 instances. All the objects in the S3 bucket are encrypted with an AWS Key Management Service (AWS KMS) customer managed key. The resources in the VPC do not have access to the internet and use a gateway VPC endpoint to access Amazon S3. The company discovers that the application is unable to get objects from the S3 bucket. Which factors could cause this issue? (Choose three.),The IAM instance proﬁle that is attached to the EC2 instances does not allow the s3:ListBucket action for the S3 bucket.,The IAM instance proﬁle that is attached to the EC2 instances does not allow the s3:ListParts action for the S3 bucket.,The KMS key policy that encrypts the objects in the S3 bucket does not allow the kms:ListKeys action to the EC2 instance proﬁle ARN.,The KMS key policy that encrypts the objects in the S3 bucket does not allow the kms:Decrypt action to the EC2 instance proﬁle ARN.,The S3 bucket policy does not allow access from the gateway VPC endpoint.,The security group that is attached to the EC2 instances is missing an inbound rule from the S3 managed preﬁx list over port 443.,"A, D, E",1,1,,,,0,0,,,,,5.3,Key Management,,136,SCS-C02,AWS Certified Security - Specialty,,"The CMK’s key policy must allow kms:Decrypt to the EC2 instance role; without it, S3 cannot decrypt and return the objects (D). With a gateway VPC endpoint, the bucket policy must explicitly allow access via that endpoint ID, or the requests will be denied (E). If the application lists the bucket to find object keys, missing s3:ListBucket on the instance profile would also cause failures (A)."
3.1,IAM Policies & SCPs,An ecommerce company is developing new architecture for an application release. The company needs to implement TLS for incoming traﬃc to the application. Traﬃc for the application will originate from the internet. TLS does not have to be implemented in an end-to-end conﬁguration because the company is concerned about impacts on performance The incoming traﬃc types will be HTTP and HTTPS The application uses ports 80 and 443. What should a security engineer do to meet these requirements?,Create a public Application Load Balancer. Create two listeners: one listener on port 80 and one listener on port 443. Create one target group. Create a rule to forward traﬃc from port 80 to the listener on port 443. Provision a public TLS certiﬁcate in AWS Certiﬁcate Manager (ACM). Attach the certiﬁcate to the listener on port 443.,Create a public Application Load Balancer. Create two listeners one listener on port 80 and one listener on port 443. Create one target group. Create a rule to forward traﬃc from port 80 to the listener on port 443. Provision a public TLS certiﬁcate in AWS Certiﬁcate Manager (ACM). Attach the certiﬁcate to the listener on port 80.,Create a public Network Load Balancer. Create two listeners one listener on port 80 and one listener on port 443. Create one target group. Create a rule to forward traﬃc from port 80 to the listener on port 443. Set the protocol for the listener on port 443 to TLS.,Create a public Network Load Balancer. Create a listener on port 443. Create one target group. Create a rule to forward traﬃc from port 443 to the target group. Set the protocol for the listener on port 443 to TLS.,,,A,0,1,,,,0,0,,,,,3.2,Edge & Web Protection,,148,SCS-C02,AWS Certified Security - Specialty,,"An Application Load Balancer supports Layer 7 features, allowing both HTTP and HTTPS listeners and an HTTP-to-HTTPS redirect, while terminating TLS at the load balancer with an ACM certificate to avoid end-to-end encryption. Attaching the certificate to the 443 listener is correct; NLB cannot perform HTTP redirects and option D omits the required HTTP listener."
3.1,IAM Policies & SCPs,"A security engineer is conﬁguring AWS Conﬁg for an AWS account that uses a new IAM entity. When the security engineer tries to conﬁgure AWS Conﬁg rules and automatic remediation options, errors occur. In the AWS CloudTrail logs, the security engineer sees the following error message: “Insuﬃcient delivery policy to s3 bucket: DOC-EXAMPLE-BUCKET, unable to write to bucket, provided s3 key preﬁx is ‘null’.” Which combination of steps should the security engineer take to remediate this issue? (Choose two.)","Check the Amazon S3 bucket policy. Verify that the policy allows the conﬁg amazonaws,com service to write to the target bucket.",Verify that the IAM entity has the permissions necessary to perform the s3:GetBucketAcl and s3:PutObject* operations to write to the target bucket.,Verify that the Amazon S3 bucket policy has the permissions necessary to perform the s3:GetBucketAcl and s3:PutObject* operations to write to the target bucket.,Check the policy that is associated with the IAM entity. Verify that the policy allows the conﬁg.amazonaws.com service to write to the target bucket.,Verify that the AWS Conﬁg service role has permissions to invoke the BatchGetResourceConﬁg action instead of the GetResourceConﬁgHistory action and s3:PutObject* operation.,,"A, B",1,1,,,,0,0,,,,,4.1,Authentication & Authorization,,151,SCS-C02,AWS Certified Security - Specialty,,"The error indicates AWS Config cannot deliver to the S3 bucket because the bucket policy does not allow the config.amazonaws.com service principal to put objects, so the bucket’s delivery policy must be fixed (A). During setup, the IAM entity also needs S3 permissions (s3:GetBucketAcl to validate bucket ownership and s3:PutObject* for test writes), so its permissions must be verified (B). Options C and D misapply permissions: the bucket policy should authorize the service principal, and an IAM user policy cannot grant permissions to the service principal."
3.1,IAM Policies & SCPs,A security engineer must Implement monitoring of a company's Amazon Aurora MySQL DB instances. The company wants to receive email notiﬁcations when unknown users try to log in to the database endpoint. Which solution will meet these requirements with the LEAST operational overhead?,Enable Amazon GuardDuty. Enable the Amazon RDS Protection feature in GuardDuty to detect login attempts by unknown users. Create an Amazon EventBridge rule to ﬁlter GuardDuty ﬁndings. Send email notiﬁcations by using Amazon Simple Notiﬁcation Service (Amazon SNS).,Enable the server_audit_logglng parameter on the Aurora MySQL DB instances. Use AWS Lambda to periodically scan the delivered log ﬁles for login attempts by unknown users. Send email notiﬁcations by using Amazon Simple Notiﬁcation Service (Amazon SNS).,Create an Amazon RDS Custom AMI. Include a third-party security agent in the AMI to detect login attempts by unknown users. Deploy RDS Custom DB instances. Migrate data from the existing installation to the RDS Custom DB instances. Conﬁgure email notiﬁcations from the third-party agent.,Write a stored procedure to detect login attempts by unknown users. Schedule a recurring job inside the database engine. Conﬁgure Aurora MySQL to use Amazon Simple Notiﬁcation Service (Amazon SNS) to send email notiﬁcations.,,,A,0,1,,,,0,0,,,,,2.3,Monitoring & Detection,,167,SCS-C02,AWS Certified Security - Specialty,,"GuardDuty RDS Protection natively monitors Aurora MySQL authentication activity and produces findings for suspicious or unauthorized login attempts without agents or custom code, minimizing operational overhead. An EventBridge rule can then route those findings to SNS for email alerts. The other options require custom logging, code, or replatforming and are more complex to manage."
3.1,IAM Policies & SCPs,A company has two AWS accounts: Account A and Account B. Account A has an IAM role that IAM users in Account B assume when they need to upload sensitive documents to Amazon S3 buckets in Account A. A new requirement mandates that users can assume the role only if they are authenticated with multi-factor authentication (MFA). A security engineer must recommend a solution that meets this requirement with minimum risk and effort. Which solution should the security engineer recommend?,Add an aws:MultiFactorAuthPresent condition to the role's permissions policy.,Add an aws MultiFactorAuthPresent condition to the role’s trust policy.,Add an aws:MultiFactorAuthPresent condition to the session policy.,Add an aws:MultiFactorAuthPresent condition to the S3 bucket policies.,,,B,0,1,,,,0,0,,,,,4.1,Authentication & Authorization,,179,SCS-C02,AWS Certified Security - Specialty,,"The requirement must be enforced at role assumption time, which is controlled by the role’s trust policy. Adding the aws:MultiFactorAuthPresent condition to the trust policy ensures sts:AssumeRole is allowed only when the user authenticated with MFA. Permissions or session policies apply after the role is assumed, and bucket policies don’t control role assumption."
3.1,IAM Policies & SCPs,A company wants to receive automated email notiﬁcations when AWS access keys from developer AWS accounts are detected on code repository sites. Which solution will provide the required email notiﬁcations?,Create an Amazon EventBridge rule to send Amazon Simple Notiﬁcation Service (Amazon SNS) email notiﬁcations for Amazon GuardDuty UnauthorizedAccess:IAMUser/lnstanceCredentialExﬁltration.OutsideAWS ﬁndings.,Change the AWS account contact information for the Operations type to a separate email address. Periodically poll this email address for notiﬁcations.,Create an Amazon EventBridge rule that reacts to AWS Health events that have a value of Risk for the service category. Conﬁgure email notiﬁcations by using Amazon Simple Notiﬁcation Service (Amazon SNS).,Implement new anomaly detection software. Ingest AWS CloudTrail logs. Conﬁgure monitoring for ConsoleLogin events in the AWS Management Console. Conﬁgure email notiﬁcations from the anomaly detection software.,,,A,0,1,,,,0,0,,,,,2.3,Monitoring & Detection,,180,SCS-C02,AWS Certified Security - Specialty,,"Amazon GuardDuty detects exposed or exfiltrated credentials being used from outside AWS and generates findings such as UnauthorizedAccess:IAMUser/InstanceCredentialExfiltration.OutsideAWS. By routing these findings through Amazon EventBridge to Amazon SNS, the company can receive automated email alerts. The other options do not specifically detect leaked keys from code repositories or require manual processes."
3.1,IAM Policies & SCPs,A company has AWS accounts that are in an organization in AWS Organizations. A security engineer needs to set up AWS Security Hub in a dedicated account for security monitoring. The security engineer must ensure that Security Hub automatically manages all existing accounts and all new accounts that are added to the organization. Security Hub also must receive ﬁndings from all AWS Regions. Which combination of actions will meet these requirements with the LEAST operational overhead? (Choose two.),Conﬁgure a ﬁnding aggregation Region for Security Hub. Link the other Regions to the aggregation Region.,Create an AWS Lambda function that routes events from other Regions to the dedicated Security Hub account. Create an Amazon EventBridge rule to invoke the Lambda function.,Turn on the option to automatically enable accounts for Security Hub.,Create an SCP that denies the securityhub:DisableSecurityHub permission. Attach the SCP to the organization’s root account.,Conﬁgure services in other Regions to write events to an AWS CloudTrail organization trail. Conﬁgure Security Hub to read events from the trail.,,"A, C",1,1,,,,0,0,,,,,4.1,Authentication & Authorization,,183,SCS-C02,AWS Certified Security - Specialty,,"Security Hub can centrally collect findings from all Regions by configuring a finding aggregation Region and linking other Regions to it (A), avoiding custom EventBridge/Lambda or CloudTrail routing. Enabling the auto-enable option for AWS Organizations (C) automatically onboards all existing and new accounts, minimizing operational overhead. Options B, D, and E add unnecessary complexity or are not the native mechanism for Security Hub ingestion and management."
3.1,IAM Policies & SCPs,"A company uses Amazon EC2 instances to host frontend services behind an Application Load Balancer. Amazon Elastic Block Store (Amazon EBS) volumes are attached to the EC2 instances. The company uses Amazon S3 buckets to store large ﬁles for images and music. The company has implemented a security architecture on AWS to prevent, identify, and isolate potential ransomware attacks. The company now wants to further reduce risk. A security engineer must develop a disaster recovery solution that can recover to normal operations if an attacker bypasses preventive and detective controls. The solution must meet an RPO of 1 hour. Which solution will meet these requirements?",Use AWS Backup to create backups of the EC2 instances and S3 buckets every hour. Create AWS CloudFormation templates that replicate existing architecture components. Use AWS CodeCommit to store the CloudFormation templates alongside application conﬁguration code.,Use AWS Backup to create backups of the EBS volumes and S3 objects every day. Use Amazon Security Lake to create a centralized data lake for AWS CloudTrail logs and VPC ﬂow logs. Use the logs for automated response.,Use Amazon Security Lake to create a centralized data lake for AWS CloudTrail logs and VPC ﬂow logs. Use the logs for automated response. Enable AWS Security Hub to establish a single location for recovery procedures. Create AWS CloudFormation templates that replicate existing architecture components. Use AWS CodeCommit to store the CloudFormation templates alongside application conﬁguration code.,Create EBS snapshots every 4 hours. Enable Amazon GuardDuty Malware Protection. Create automation to immediately restore the most recent snapshot for any EC2 instances that produce an Execution:EC2/MaliciousFile ﬁnding in GuardDuty.,,,A,0,0,0,1,0,1,0,,,,,1.1,Preparation & Runbooks,,196,SCS-C02,AWS Certified Security - Specialty,,"Option A meets the 1-hour RPO by using AWS Backup to perform hourly backups of both EC2 (via EBS) and S3, ensuring data can be restored to within an hour. CloudFormation templates and CodeCommit provide versioned, repeatable infrastructure and configuration to quickly rebuild the environment after a ransomware incident. The other options either lack sufficient backup frequency, do not cover S3, or focus on detection rather than recovery."
3.1,IAM Policies & SCPs,A company runs a cron job on an Amazon EC2 instance on a predeﬁned schedule. The cron job calls a bash script that encrypts a 2 KB ﬁle. A security engineer creates an AWS Key Management Service (AWS KMS) customer managed key with a key policy. The key policy and the EC2 instance role have the necessary conﬁguration for this job. Which process should the bash script use to encrypt the ﬁle?,Use the aws kms encrypt command to encrypt the ﬁle by using the existing KMS key.,Use the aws kms create-grant command to generate a grant for the existing KMS key.,Use the aws kms encrypt command to generate a data key. Use the plaintext data key to encrypt the ﬁle.,Use the aws kms generate-data-key command to generate a data key. Use the encrypted data key to encrypt the ﬁle.,,,A,0,1,,,,0,0,,,,,5.3,Key Management,,216,SCS-C02,AWS Certified Security - Specialty,,"Because the file is only 2 KB, it is within AWS KMS’s 4 KB limit for direct encryption, so the script can call aws kms encrypt with the CMK to encrypt it. A grant is unnecessary since the instance role and key policy already permit access, and data keys are primarily used for envelope encryption of larger data."
3.1,IAM Policies & SCPs,A company uses AWS Conﬁg rules to identify Amazon S3 buckets that are not compliant with the company’s data protection policy. The S3 buckets are hosted in several AWS Regions and several AWS accounts. The accounts are in an organization in AWS Organizations. The company needs a solution to remediate the organization’s existing noncompliant S3 buckets and any noncompliant S3 buckets that are created in the future. Which solution will meet these requirements?,Deploy an AWS Conﬁg aggregator with organization-wide resource data aggregation. Create an AWS Lambda function that responds to AWS Conﬁg ﬁndings of noncompliant S3 buckets by deleting or reconﬁguring the S3 buckets.,Deploy an AWS Conﬁg aggregator with organization-wide resource data aggregation. Create an SCP that contains a Deny statement that prevents the creation of new noncompliant S3 buckets. Apply the SCP to all OUs in the organization.,Deploy an AWS Conﬁg aggregator that scopes only the accounts and Regions that the company currently uses. Create an AWS Lambda function that responds to AWS Conﬁg ﬁndings of noncompliant S3 buckets by deleting or reconﬁguring the S3 buckets.,Deploy an AWS Conﬁg aggregator that scopes only the accounts and Regions that the company currently uses. Create an SCP that contains a Deny statement that prevents the creation of new noncompliant S3 buckets. Apply the SCP to all OUs in the organization.,,,A,0,1,,,,0,0,,,,,5.4,Data Classification & Access Control,,224,SCS-C02,AWS Certified Security - Specialty,,"Option A uses an organization-wide AWS Config aggregator to detect noncompliant S3 buckets across all accounts and Regions, and an AWS Lambda remediation to automatically fix or delete them—covering both existing and future buckets. SCPs (B, D) can block creation but cannot remediate existing resources, and scoping the aggregator to only current accounts/Regions (C, D) would miss future accounts or Regions."
3.1,IAM Policies & SCPs,"A company’s engineering team is developing a new application that creates AWS Key Management Service (AWS KMS) customer managed key grants for users. Immediately after a grant is created, users must be able to use the KMS key to encrypt a 512-byte payload. During load testing, AccessDeniedException errors occur occasionally when a user ﬁrst attempts to use the key to encrypt. Which solution should the company’s security specialist recommend to eliminate these AccessDeniedException errors?",Instruct users to implement a retry mechanism every 2 minutes until the call succeeds.,Instruct the engineering team to consume a random grant token from users and to call the CreateGrant operation by passing the grant token to the operation. Instruct users to use that grant token in their call to encrypt.,Instruct the engineering team to create a random name for the grant when calling the CreateGrant operation. Return the name to the users and instruct them to provide the name as the grant token in the call to encrypt.,Instruct the engineering team to pass the grant token returned in the CreateGrant response to users. Instruct users to use that grant token in their call to encrypt.,,,D,0,1,,,,0,0,,,,,4.1,Authentication & Authorization,,225,SCS-C02,AWS Certified Security - Specialty,,"KMS grants are eventually consistent, so right after CreateGrant, permissions may not have propagated, causing intermittent AccessDeniedException. Passing the grant token from the CreateGrant response in subsequent Encrypt calls forces KMS to recognize the new grant immediately. This eliminates timing-related access errors without arbitrary delays."
3.1,IAM Policies & SCPs,A security engineer needs to implement a solution to identify any sensitive data that is stored in an Amazon S3 bucket. The solution must report on sensitive data in the S3 bucket by using an existing Amazon Simple Notiﬁcation Service (Amazon SNS) topic. Which solution will meet these requirements with the LEAST implementation effort?,Enable AWS Conﬁg. Conﬁgure AWS Conﬁg to monitor for sensitive data in the S3 bucket and to send notiﬁcations to the SNS topic.,Create an AWS Lambda function to scan the S3 bucket for sensitive data that matches a pattern. Program the Lambda function to send notiﬁcations to the SNS topic.,Conﬁgure Amazon Macie to use managed data identiﬁers to identify and categorize sensitive data. Create an Amazon EventBridge rule to send notiﬁcations to the SNS topic.,Enable Amazon GuardDuty. Conﬁgure AWS CloudTrail S3 data events. Create an Amazon CloudWatch alarm that reacts to GuardDuty ﬁndings and sends notiﬁcations to the SNS topic.,,,C,0,1,,,,0,0,,,,,5.4,Data Classification & Access Control,,230,SCS-C02,AWS Certified Security - Specialty,,"Amazon Macie is purpose-built to discover and classify sensitive data in S3 using managed data identifiers with minimal setup. Macie publishes findings to EventBridge, which can route notifications to the existing SNS topic. Other options either don't detect sensitive data (AWS Config/GuardDuty) or require custom code and maintenance (Lambda)."
3.1,IAM Policies & SCPs,"A company plans to create Amazon S3 buckets to store log data. All the S3 buckets will have versioning enabled and will use the S3 Standard storage class. A security engineer needs to implement a solution that protects objects in the S3 buckets from deletion for 90 days. The solution must ensure that no object can be deleted during this time period, even by an administrator or the AWS account root user. Which solution will meet these requirements?",Enable S3 Object Lock in governance mode. Set a legal hold of 90 days.,Enable S3 Object Lock in governance mode. Set a retention period of 90 days.,Enable S3 Object Lock in compliance mode. Set a retention period of 90 days.,Create an S3 Glacier Vault Lock policy that prevents deletion for 90 days.,,,C,0,1,,,,0,0,,,,,5.1,Encryption at Rest,,243,SCS-C02,AWS Certified Security - Specialty,,"S3 Object Lock in compliance mode enforces WORM so that no user, including administrators and the root account, can delete or overwrite objects until the retention period expires. Governance mode can be bypassed by users with the appropriate permission, and Glacier Vault Lock applies to Glacier vaults, not S3 Standard objects."
3.1,IAM Policies & SCPs,A company is testing incident response procedures for destination containment. The company needs to contain a critical Amazon EC2 instance as quickly as possible while keeping the EC2 instance running. The EC2 instance is the only resource in a public subnet and has active connections to other resources. Which solution will contain the EC2 instance IMMEDIATELY?,Create a new security group that has no inbound rules or outbound rules. Attach the new security group to the EC2 instance.,Conﬁgure the existing security group for the EC2 instance. Remove all existing inbound rules and outbound rules from the security group.,Create a new network ACL that has a single Deny rule for inbound traﬃc and outbound traﬃc. Associate the new network ACL with the subnet that contains the EC2 instance.,Create a new VPC for isolation. Stop the EC2 instance. Create a new AMI from the EC2 instance. Use the new AMI to launch a new EC2 instance in the new VPC.,,,C,0,1,,,,0,0,,,,,1.3,Containment & Eradication,,246,SCS-C02,AWS Certified Security - Specialty,,"A deny-all network ACL applied to the subnet immediately blocks both inbound and outbound traffic at the subnet boundary, severing existing connections while keeping the instance running. Because the instance is the only resource in that subnet, this isolates only that instance. Security groups are stateful and would not immediately terminate existing connections, and recreating in a new VPC is not immediate."
3.1,IAM Policies & SCPs,A company needs to prevent Amazon S3 objects from being shared with IAM identities outside of the company’s organization in AWS Organizations. A security engineer is creating and deploying an SCP to accomplish this goal. The company has enabled the S3 Block Public Access feature on all of its S3 buckets. What should the SCP do to meet these requirements?,"Deny the S3:* action with a Condition element that comprises an operator of StringNotEquals, a key of aws:ResourceOrgID, and a value of S{aws PrincipalOrgID}.","Deny the S3:PutAccountPublicAccessBlock action with a Condition element that comprises an operator of StringLike, a key of aws:PrincipalArn, and the values of the external IAM principals.","Allow the S3:* action with a Condition element that comprises an operator of StringNotEquals, a key of aws:PrincipalOrgID, and a value of S{aws:PrincipalOrgID}.","Deny the S3:* action with a Condition element that comprises an operator of StringLike, a key of aws:PrincipalArn, and the values of the external IAM principals",,,A,0,1,,,,0,0,,,,,4.2,Cross-Account & Federation,,263,SCS-C02,AWS Certified Security - Specialty,,"Use an SCP to deny S3 actions when the resource’s organization ID doesn’t match the caller’s organization (aws:ResourceOrgID != aws:PrincipalOrgID). This ensures only principals from the same AWS Organizations org can access S3 objects, preventing cross-org sharing. S3 Block Public Access stops public access but doesn’t prevent cross-account access from other organizations, which the SCP addresses."
3.1,IAM Policies & SCPs,A company is developing a new serverless application that uses AWS Lambda functions. The company uses AWS CloudFormation to deploy the Lambda functions. The company’s developers are trying to debug a Lambda function that is deployed. The developers cannot debug the Lambda function because the Lambda function is not logging its output to Amazon CloudWatch Logs. Which combination of steps should a security engineer take to resolve this issue? (Choose two.),Check the role that is deﬁned in the CloudFormation template and is passed to the Lambda function. Ensure that the role has a trust policy that allows the sts:AssumeRole action by the service principal lambda amazonaws.com.,Check the execution role that is conﬁgured in the CloudFormation template for the Lambda function. Ensure that the execution role has the necessary permissions to write to CloudWatch Logs.,Check the Lambda function conﬁguration in the CloudFormation template. Ensure that the Lambda function has an AWS X-Ray tracing conﬁguration that is set to Active mode or PassThrough mode.,Check the resource policy that is conﬁgured in the CloudFormation template for the Lambda function. Ensure that the resource policy has the necessary permissions to write to CloudWatch Logs.,Check the role that the developers use to debug the Lambda function. Ensure that the role has a trust policy that allows the sts:AssumeRole action by the service principal lambda.amazonaws.com.,,"A, B",1,1,,,,0,0,,,,,2.1,Logging Configuration,,266,SCS-C02,AWS Certified Security - Specialty,,"Lambda must assume its execution role and that role must have permissions to create and write to CloudWatch Logs, so you need a trust policy allowing sts:AssumeRole by the Lambda service principal and CloudWatch Logs permissions (e.g., CreateLogGroup, CreateLogStream, PutLogEvents). X-Ray tracing is unrelated, a Lambda resource policy doesn’t grant write access to CloudWatch Logs, and the developers’ role trust policy doesn’t affect the function’s logging."
3.1,IAM Policies & SCPs,A company must retain backup copies of Amazon RDS DB instances and Amazon Elastic Block Store (Amazon EBS) volumes. The company must retain the backup copies in data centers that are several hundred miles apart. Which solution will meet these requirements with the LEAST operational overhead?,"Conﬁgure AWS Backup to create the backups according to the needed schedule. In the backup plan, specify multiple Availability Zones as backup destinations.",Conﬁgure Amazon Data Lifecycle Manager to create the backups. Conﬁgure the Amazon Data Lifecycle Manager policy to copy the backups to an Amazon S3 bucket. Enable replication on the S3 bucket.,Conﬁgure AWS Backup to create the backups according to the needed schedule. Create a destination backup vault in a different AWS Region. Conﬁgure AWS Backup to copy the backups to the destination backup vault.,Conﬁgure Amazon Data Lifecycle Manager to create the backups. Create an AWS Lambda function to copy the backups to a different AWS Region. Use Amazon EventBridge to invoke the Lambda function on a schedule.,,,C,0,1,,,,0,0,,,,,5.4,Data Classification & Access Control,,282,SCS-C02,AWS Certified Security - Specialty,,"AWS Backup natively creates and copies backups for multiple services (including RDS and EBS) across Regions to a destination backup vault, providing geographic separation with minimal management. Alternatives either don’t support RDS (Data Lifecycle Manager) or require custom replication/automation (Lambda/EventBridge), increasing operational overhead."
3.1,IAM Policies & SCPs,"A company has hundreds of AWS accounts and uses AWS Organizations. The company plans to create many different IAM roles and policies for its product team, security team, and platform team. Some IAM policies will be shared across teams. A security engineer needs to implement a solution to logically group together the IAM roles of each team. The solution must allow only the platform team to delegate IAM permissions to AWS services. Which solution will meet these requirements?",Set up an IAM path with the IAM roles for each team. Deploy an SCP that denies the iam:PassRole permission to all entities except the IAM path of the platform team.,Apply different tags for each team to the IAM roles. Deploy an SCP that denies the sts:AssumeRole permission to all entities except the roles of the platform team.,Apply different tags for each team to the IAM policies. Deploy an SCP that denies the iam:PassRole permission to all entities except the policies of the platform team.,Set up an IAM path with the IAM roles for each team. Use IAM permissions boundaries to deny the sts:AssumeRole permission to the IAM roles for the product team and the security team.,,,A,0,1,,,,0,0,,,,,4.1,Authentication & Authorization,,292,SCS-C02,AWS Certified Security - Specialty,,"Using IAM paths groups each team’s roles logically, and an SCP can reference the platform team’s role path to allow iam:PassRole only for that team while denying it for everyone else. This directly controls delegation of IAM permissions to AWS services. The other options either target the wrong action (sts:AssumeRole) or tag policies/roles in ways that don’t effectively enforce PassRole restrictions for the correct principals."
3.1,IAM Policies & SCPs,A company’s developers are using AWS Lambda function URLs to invoke functions directly. The company must ensure that developers cannot conﬁgure or deploy unauthenticated functions in production accounts. The company wants to meet this requirement by using AWS Organizations. The solution must not require additional work for the developers. Which solution will meet these requirements?,Require the developers to conﬁgure all function URL to support cross-origin resource sharing (CORS) when the functions are called from a different domain.,"Use an AWS WAF delegated administrator account to view and block unauthenticated access to function URLs in production accounts, based on the OU of accounts that are using the functions.",Use SCPs to allow all lambda:CreateFunctionUrlConﬁg and lambda:UpdateFunctionUrlConﬁg actions that have a lambda:FunctionUrlAuthType condition key value of AWS_IAM.,Use SCPs to deny all lambda:CreateFunctionUrlConﬁg and lambda:UpdateFunctionUrlConﬁg actions that have a lambda:FunctionUrlAuthType condition key value of NONE.,,,D,0,1,,,,0,0,,,,,4.1,Authentication & Authorization,,293,SCS-C02,AWS Certified Security - Specialty,,"An SCP that denies lambda:CreateFunctionUrlConfig and lambda:UpdateFunctionUrlConfig when lambda:FunctionUrlAuthType is NONE prevents deploying unauthenticated function URLs in production at the organization level. This enforces authenticated access without any developer changes. Other choices either don’t enforce auth at configuration time (CORS/WAF) or rely on an SCP Allow, which doesn’t grant or restrict permissions by itself."
3.1,IAM Policies & SCPs,"A company uses AWS Organizations to manage an organization that consists of three workload OUs. Production, Development, and Testing. The company uses AWS CloudFormation templates to deﬁne and deploy workload infrastructure in AWS accounts that are associated with the OUs. Different SCPs are attached to each workload OU. The company successfully deployed a CloudFormation stack update to workloads in the Development OU and the Testing OU. When the company uses the same CloudFormation template to deploy the stack update in.an account in the Production OU, the update fails. The error message reports insuﬃcient IAM permissions. What is the FIRST step that a security engineer should take to troubleshoot this issue?",Review the AWS CloudTrail logs in the account in the Production OU. Search for any failed API calls from CloudFormation during the deployment attempt.,Remove all the SCPs that are attached to the Production OU. Rerun the CloudFormation stack update to determine if the SCPs were preventing the CloudFormation API calls.,"Conﬁrm that the role used by CloudFormation has suﬃcient permissions to create, update, and delete the resources that are referenced in the CloudFormation template.",Make all the SCPs that are attached to the Production OU the same as the SCPs that are attached to the Testing OU.,,,A,0,1,,,,0,0,,,,,4.1,Authentication & Authorization,,296,SCS-C02,AWS Certified Security - Specialty,,"CloudTrail shows exactly which API calls CloudFormation attempted and whether they failed due to an explicit deny from an SCP or insufficient IAM role permissions, making it the least disruptive first diagnostic step. This pinpoints the permission source before changing roles or SCPs. Removing or modifying SCPs without evidence is risky and unnecessary as an initial action."
3.2,IAM Identity Center (SSO),An international company has established a new business entity in South Korea. The company also has established a new AWS account to contain the workload for the South Korean region. The company has set up the workload in the new account in the ap-northeast-2 Region. The workload consists of three Auto Scaling groups of Amazon EC2 instances. All workloads that operate in this Region must keep system logs and application logs for 7 years. A security engineer must implement a solution to ensure that no logging data is lost for each instance during scaling activities. The solution also must keep the logs for only the required period of 7 years. Which combination of steps should the security engineer take to meet these requirements? (Choose three.),Ensure that the Amazon CloudWatch agent is installed on all the EC2 instances that the Auto Scaling groups launch. Generate a CloudWatch agent conﬁguration ﬁle to forward the required logs to Amazon CloudWatch Logs.,Set the log retention for desired log groups to 7 years.,Attach an IAM role to the launch conﬁguration or launch template that the Auto Scaling groups use. Conﬁgure the role to provide the necessary permissions to forward logs to Amazon CloudWatch Logs.,Attach an IAM role to the launch conﬁguration or launch template that the Auto Scaling groups use. Conﬁgure the role to provide the necessary permissions to forward logs to Amazon S3.,Ensure that a log forwarding application is installed on all the EC2 instances that the Auto Scaling groups launch. Conﬁgure the log forwarding application to periodically bundle the logs and forward the logs to Amazon S3.,Conﬁgure an Amazon S3 Lifecycle policy on the target S3 bucket to expire objects after 7 years.,"A, B, C",1,1,,,,0,0,,,,,2.2,Centralization & Retention,,12,SCS-C02,AWS Certified Security - Specialty,,"Installing the CloudWatch agent (A) on all EC2 instances ensures system and application logs are streamed off-instance to a durable, centralized destination during scaling or termination, preventing data loss. The instances need an IAM role with permissions to put logs to CloudWatch Logs (C). Setting CloudWatch Logs retention to 7 years (B) enforces the required retention period without additional S3 lifecycle management."
3.2,IAM Identity Center (SSO),A company purchased a subscription to a third-party cloud security scanning solution that integrates with AWS Security Hub. A security engineer needs to implement a solution that will remediate the ﬁndings from the third-party scanning solution automatically. Which solution will meet this requirement?,Set up an Amazon EventBridge rule that reacts to new Security Hub ﬁndings. Conﬁgure an AWS Lambda function as the target for the rule to remediate the ﬁndings.,Set up a custom action in Security Hub. Conﬁgure the custom action to call AWS Systems Manager Automation runbooks to remediate the ﬁndings.,Set up a custom action in Security Hub. Conﬁgure an AWS Lambda function as the target for the custom action to remediate the ﬁndings.,Set up AWS Conﬁg rules to use AWS Systems Manager Automation runbooks to remediate the ﬁndings.,,,A,0,1,,,,0,0,,,,,2.4,Alerting & Remediation,,62,SCS-C02,AWS Certified Security - Specialty,,"Security Hub emits findings to EventBridge, so creating a rule that matches new findings and triggers a Lambda function enables fully automated remediation. Security Hub custom actions are manual (not automatic), and AWS Config/Automation runbooks don’t automatically respond to third‑party Security Hub findings."
3.2,IAM Identity Center (SSO),"A security engineer needs to implement a write-once-read-many (WORM) model for data that a company will store in Amazon S3 buckets. The company uses the S3 Standard storage class for all of its S3 buckets. The security engineer must ensure that objects cannot be overwritten or deleted by any user, including the AWS account root user. Which solution will meet these requirements?",Create new S3 buckets with S3 Object Lock enabled in compliance mode. Place objects in the S3 buckets.,Use S3 Glacier Vault Lock to attach a Vault Lock policy to new S3 buckets. Wait 24 hours to complete the Vault Lock process. Place objects in the S3 buckets.,Create new S3 buckets with S3 Object Lock enabled in governance mode. Place objects in the S3 buckets.,Create new S3 buckets with S3 Object Lock enabled in governance mode. Add a legal hold to the S3 buckets. Place objects in the S3 buckets.,,,A,0,1,,,,0,0,,,,,5.1,Encryption at Rest,,102,SCS-C02,AWS Certified Security - Specialty,,"S3 Object Lock in compliance mode enforces WORM such that no user, including the root account, can overwrite or delete objects until the retention period expires. Governance mode can be bypassed by users with the s3:BypassGovernanceRetention permission, and legal holds are per object. Glacier Vault Lock applies to Glacier vaults, not S3 Standard buckets."
3.2,IAM Identity Center (SSO),"A company is testing its incident response plan for compromised credentials. The company runs a database on an Amazon EC2 instance and stores the sensitive database credentials as a secret in AWS Secrets Manager. The secret has rotation conﬁgured with an AWS Lambda function that uses the generic rotation function template. The EC2 instance and the Lambda function are deployed in the same private subnet. The VPC has a Secrets Manager VPC endpoint. A security engineer discovers that the secret cannot rotate. The security engineer determines that the VPC endpoint is working as intended. The Amazon CloudWatch logs contain the following error: ""setSecret: Unable to log into database"". Which solution will resolve this error?",Use the AWS Management Console to edit the JSON structure of the secret in Secrets Manager so that the secret automatically conforms with the structure that the database requires.,Ensure that the security group that is attached to the Lambda function allows outbound connections to the EC2 instance. Ensure that the security group that is attached to the EC2 instance allows inbound connections from the security group that is attached to the Lambda function.,Use the Secrets Manager list-secrets command in the AWS CLI to list the secret. Identify the database credentials. Use the Secrets Manager rotate-secret command in the AWS CLI to force the immediate rotation of the secret.,Add an internet gateway to the VPC. Create a NAT gateway in a public subnet. Update the VPC route tables so that traﬃc from the Lambda function and traﬃc from the EC2 instance can reach the Secrets Manager public endpoint.,,,B,0,1,,,,0,0,,,,,1.2,Detection & Investigation,,113,SCS-C02,AWS Certified Security - Specialty,,"The rotation Lambda must connect to the database instance to set a new password; the error indicates it cannot log into the DB, which is typically a connectivity or security group issue, not a Secrets Manager issue. Ensuring the Lambda’s security group can egress to the EC2 instance and the EC2 security group allows inbound from the Lambda’s security group enables the needed DB connection for rotation. Adding NAT or changing the secret structure would not fix the underlying connectivity problem."
3.2,IAM Identity Center (SSO),"A company is developing a mechanism that will help data scientists use Amazon SageMaker to read, process, and output data to an Amazon S3 bucket. Data scientists will have access to a dedicated S3 preﬁx for each of their projects. The company will implement bucket policies that use the dedicated S3 preﬁxes to restrict access to the S3 objects. The projects can last up to 60 days. The company's security team mandates that data cannot remain in the S3 bucket after the end of the projects that use the data. Which solution will meet these requirements MOST cost-effectively?",Create an AWS Lambda function to identify and delete objects in the S3 bucket that have not been accessed for 60 days. Create an Amazon EventBridge scheduled rule that runs every day to invoke the Lambda function.,Create a new S3 bucket. Conﬁgure the new S3 bucket to use S3 Intelligent-Tiering. Copy the objects to the new S3 bucket.,Create an S3 Lifecycle conﬁguration for each S3 bucket preﬁx for each project. Set the S3 Lifecycle conﬁgurations to expire objects after 60 days.,Create an AWS Lambda function to delete objects that have not been accessed for 60 days. Create an S3 event notiﬁcation for S3 Intelligent-Tiering automatic archival events to invoke the Lambda function.,,,C,0,1,,,,0,0,,,,,5.4,Data Classification & Access Control,,118,SCS-C02,AWS Certified Security - Specialty,,"S3 Lifecycle policies can be applied per project prefix to automatically expire (delete) objects after 60 days, meeting the data-retention mandate with a native, low-cost, maintenance-free solution. Lambda/EventBridge approaches add unnecessary compute and operational overhead based on access times, and Intelligent-Tiering reduces storage cost but does not delete data."
3.2,IAM Identity Center (SSO),A company recently adopted new compliance standards that require all user actions in AWS to be logged. The user actions must be logged for all accounts that belong to an organization in AWS Organizations. The company needs to set alarms that respond when speciﬁed actions occur. The alarms must forward alerts to an email distribution list. The alerts must occur in as close to real time as possible. Which solution will meet these requirements?,"Implement an AWS CloudTrail trail as an organizational trail. Conﬁgure the trail with Amazon CloudWatch Logs forwarding. In CloudWatch Logs, set a metric ﬁlter for any user action events that the company speciﬁes. Create an Amazon CloudWatch alarm to provide alerts for occurrences within a reported period and to publish messages to an Amazon Simple Notiﬁcation Service (Amazon SNS) topic.","Implement an AWS CloudTrail trail. Conﬁgure the trail with Amazon CloudWatch Logs forwarding. In CloudWatch Logs, set a metric ﬁlter for any user action events that the company speciﬁes. Create an Amazon CloudWatch alarm to provide alerts for occurrences within a reported period and to send messages to an Amazon Simple Queue Service (Amazon SQS) queue.",Implement an AWS CloudTrail trail as an organizational trail. Conﬁgure the trail to store logs in an Amazon S3 bucket. Conﬁgure an Amazon EC2 instance to mount the S3 bucket as a ﬁle system to ingest new log ﬁles that are pushed to the S3 bucket. Conﬁgure the EC2 instance also to publish a message to an Amazon Simple Notiﬁcation Service (Amazon SNS) topic when one of the speciﬁed actions is found in the logs.,"Implement an AWS CloudTrail trail. Conﬁgure the trail to store logs in an Amazon S3 bucket. Each hour, create an AWS Glue Data Catalog that references the S3 bucket. Conﬁgure Amazon Athena to initiate queries against the Data Catalog to identify the speciﬁed actions in the logs.",,,A,0,1,,,,0,0,,,,,2.4,Alerting & Remediation,,130,SCS-C02,AWS Certified Security - Specialty,,"An organizational CloudTrail trail captures user actions across all AWS Organizations accounts, and forwarding to CloudWatch Logs allows metric filters and CloudWatch Alarms for near–real-time detection. The alarm can publish to Amazon SNS for email notifications. Other options either lack org-wide coverage or rely on batch/EC2 processing or SQS, which don’t provide near‑real‑time email alerts."
3.2,IAM Identity Center (SSO),A company uses user data scripts that contain sensitive information to bootstrap Amazon EC2 instances. A security engineer discovers that this sensitive information is viewable by people who should not have access to it. What is the MOST secure way to protect the sensitive information used to bootstrap the instances?,Store the scripts in the AMI and encrypt the sensitive data using AWS KMS. Use the instance role proﬁle to control access to the KMS keys needed to decrypt the data.,Store the sensitive data in AWS Systems Manager Parameter Store using the encrypted string parameter and assign the GetParameters permission to the EC2 instance role.,Externalize the bootstrap scripts in Amazon S3 and encrypt them using AWS KMS. Remove the scripts from the instance and clear the logs after the instance is conﬁgured.,Block user access of the EC2 instance's metadata service using IAM policies. Remove all scripts and clear the logs after the scripts have completed.,,,B,0,1,,,,0,0,,,,,5.1,Encryption at Rest,,140,SCS-C02,AWS Certified Security - Specialty,,"Storing secrets as SecureString parameters in AWS Systems Manager Parameter Store keeps them encrypted with KMS and offloads them from user data, AMIs, and logs, preventing unintended exposure. The EC2 instance role can be granted least-privilege GetParameters access to retrieve and decrypt at boot, providing controlled, auditable access. Other options still risk exposure via user data, AMIs, or are operationally weaker."
3.2,IAM Identity Center (SSO),A company needs a solution to protect critical data from being permanently deleted. The data is stored in Amazon S3 buckets. The company needs to replicate the S3 objects from the company's primary AWS Region to a secondary Region to meet disaster recovery requirements. The company must also ensure that users who have administrator access cannot permanently delete the data in the secondary Region. Which solution will meet these requirements?,Conﬁgure AWS Backup to perform cross-Region S3 backups. Select a backup vault in the secondary Region. Enable AWS Backup Vault Lock in governance mode for the backups in the secondary Region.,Implement S3 Object Lock in compliance mode in the primary Region. Conﬁgure S3 replication to replicate the objects to an S3 bucket in the secondary Region.,Conﬁgure S3 replication to replicate the objects to an S3 bucket in the secondary Region. Create an S3 bucket policy to deny the s3:ReplicateDelete action on the S3 bucket in the secondary Region.,Conﬁgure S3 replication to replicate the objects to an S3 bucket in the secondary Region. Conﬁgure S3 object versioning on the S3 bucket in the secondary Region.,,,B,0,1,,,,0,0,,,,,5.4,Data Classification & Access Control,,149,SCS-C02,AWS Certified Security - Specialty,,"S3 Object Lock in compliance mode enforces WORM retention that cannot be bypassed, even by administrators or the root user, preventing permanent deletion until the retention period expires. Configuring cross-Region replication ensures the objects (and their retention settings) are replicated to the secondary Region to meet disaster recovery requirements. The other options either allow admins to delete, don’t provide WORM guarantees, or don’t adequately protect against permanent deletion."
3.2,IAM Identity Center (SSO),A company needs to create a centralized solution to analyze log ﬁles. The company uses an organization in AWS Organizations to manage its AWS accounts. The solution must aggregate and normalize events from the following sources: • The entire organization in Organizations • All AWS Marketplace offerings that run in the company’s AWS accounts • The company's on-premises systems Which solution will meet these requirements?,"Conﬁgure a centralized Amazon S3 bucket for the logs. Enable VPC Flow Logs, AWS CloudTrail. and Amazon Route 53 logs in all accounts. Conﬁgure all accounts to use the centralized S3 bucket. Conﬁgure AWS Glue crawlers to parse the log ﬁles. Use Amazon Athena to query the log data.",Conﬁgure log streams in Amazon CloudWatch Logs for the sources that need monitoring Create log subscription ﬁlters for each log stream. Forward the messages to Amazon OpenSearch Service for analysis.,Set up a delegated Amazon Security Lake administrator account in Organizations. Enable and conﬁgure Security Lake for the organization. Add the accounts that need monitoring. Use Amazon Athena to query the log data.,Apply an SCP to conﬁgure all member accounts and services to deliver log ﬁles to a centralized Amazon S3 bucket. Use Amazon OpenSearch Service to query the centralized S3 bucket for log entries.,,,C,0,1,,,,0,0,,,,,2.2,Centralization & Retention,,160,SCS-C02,AWS Certified Security - Specialty,,"Amazon Security Lake supports delegated administration across AWS Organizations, aggregates logs from AWS services, AWS partner/Marketplace integrations, and custom/on-prem sources, and normalizes events to OCSF. It stores the data in S3 and can be queried directly with Athena, meeting the centralized analysis requirement. The other options don’t provide organization-wide, OCSF-normalized ingestion from Marketplace and on-prem sources."
3.2,IAM Identity Center (SSO),A company uses an organization in AWS Organizations to manage its AWS accounts. The company has implemented an SCP in the root account to prevent resources from being shared with external accounts. The company now needs to allow applications in its marketing team's AWS account to share resources with external accounts. The company must continue to prevent all the other accounts in the organization from sharing resources with external accounts. All the accounts in the organization are members of the same OU. Which solution will meet these requirements?,Create a new SCP in the marketing team's account Conﬁgure the SCP to explicitly allow resource sharing.,Edit the existing SCP to add a Condition statement that excludes the marketing team's account.,Edit the existing SCP to include an Allow statement that speciﬁes the marketing team's account.,Create an IAM permissions boundary policy to explicitly allow resource sharing Attach the policy to IAM users in the marketing team's account.,,,B,0,1,,,,0,0,,,,,4.2,Cross-Account & Federation,,165,SCS-C02,AWS Certified Security - Specialty,,"SCPs are enforced at the organization level and an explicit Deny overrides any IAM permissions, so you can’t bypass it with IAM policies or a separate Allow. The right solution is to modify the existing root SCP’s deny statement with a Condition (or NotPrincipal) that excludes the marketing account, enabling sharing only for that account while keeping the restriction for all others."
3.2,IAM Identity Center (SSO),"A security engineer is designing an IAM policy for a script that will use the AWS CLI. The script currently assumes an IAM role that is attached to three AWS managed IAM policies: AmazonEC2FullAccess, AmazonDynamoDBFullAccess, and AmazonVPCFullAccess. The security engineer needs to construct a least privilege IAM policy that will replace the AWS managed IAM policies that are attached to this role. Which solution will meet these requirements in the MOST operationally eﬃcient way?","In AWS CloudTrail, create a trail for management events. Run the script with the existing AWS managed IAM policies. Use IAM Access Analyzer to generate a new IAM policy that is based on access activity in the trail. Replace the existing AWS managed IAM policies with the generated IAM policy for the role.",Remove the existing AWS managed IAM policies from the role. Attach the IAM Access Analyzer Role Policy Generator to the role. Run the script. Return to IAM Access Analyzer and generate a least privilege IAM policy. Attach the new IAM policy to the role.,Create an account analyzer in IAM Access Analyzer. Create an archive rule that has a ﬁlter that checks whether the PrincipalArn value matches the ARN of the role. Run the script. Remove the existing AWS managed IAM policies from the role.,"In AWS CloudTrail, create a trail for management events. Remove the existing AWS managed IAM policies from the role. Run the script. Find the authorization failure in the trail event that is associated with the script. Create a new IAM policy that includes the action and resource that caused the authorization failure. Repeat the process until the script succeeds. Attach the new IAM policy to the role.",,,A,0,1,,,,0,0,,,,,4.1,Authentication & Authorization,,177,SCS-C02,AWS Certified Security - Specialty,,"IAM Access Analyzer can automatically generate least-privilege policies from CloudTrail management event logs based on the exact API calls the script makes. Keeping the existing managed policies while running the script captures all required actions, making this approach accurate and operationally efficient. The other options either remove permissions prematurely or rely on manual, iterative tuning, which is less efficient and more error-prone."
3.2,IAM Identity Center (SSO),A company operates a web application that runs on Amazon EC2 instances. The application listens on port 80 and port 443. The company uses an Application Load Balancer (ALB) with AWS WAF to terminate SSL and to forward traﬃc to the application instances only on port 80. The ALB is in public subnets that are associated with a network ACL that is named NACL1. The application instances are in dedicated private subnets that are associated with a network ACL that is named NACL2. An Amazon RDS for PostgreSQL DB instance that uses port 5432 is in a dedicated private subnet that is associated with a network ACL that is named NACL3. All the network ACLs currently allow all inbound and outbound traﬃc. Which set of network ACL changes will increase the security of the application while ensuring functionality?,Make the following changes to NACL3: • Add a rule that allows inbound traﬃc on port 5432 from NACL2. • Add a rule that allows outbound traﬃc on ports 1024-65536 to NACL2. • Remove the default rules that allow all inbound and outbound traﬃc.,Make the following changes to NACL3: • Add a rule that allows inbound traﬃc on port 5432 from the Cl DR blocks of the application instance subnets. • Add a rule that allows outbound traﬃc on ports 1024-65536 to the application instance subnets. • Remove the default rules that allow all inbound and outbound traﬃc.,Make the following changes to NACL2: • Add a rule that allows outbound traﬃc on port 5432 to the CIDR blocks of the RDS subnets. • Remove the default rules that allow all inbound and outbound traﬃc.,Make the following changes to NACL2: • Add a rule that allows inbound traﬃc on port 5432 from the CIDR blocks of the RDS subnets. • Add a rule that allows outbound traﬃc on port 5432 to the RDS subnets.,,,B,0,1,,,,0,0,,,,,3.1,Network Architecture Security,,191,SCS-C02,AWS Certified Security - Specialty,,"Network ACLs are stateless and use CIDR-based rules, so the RDS subnet’s NACL must explicitly allow inbound 5432 from the application subnets and outbound ephemeral ports (1024–65535) back to those subnets to maintain the return path. Option B does exactly this and removes the default allow-all rules, tightening access to only what’s required. Option A is invalid because NACLs don’t reference other NACLs, and C/D either leave the DB subnet open or break return traffic."
3.2,IAM Identity Center (SSO),A healthcare company has multiple AWS accounts in an organization in AWS Organizations. The company uses Amazon S3 buckets to store sensitive information of patients. The company needs to restrict users from deleting any S3 bucket across the organization. What is the MOST scalable solution that meets these requirements?,Permissions boundaries in AWS Identity and Access Management (IAM),S3 bucket policies,Tag policies,SCPs,,,D,0,1,,,,0,0,,,,,4.3,Permission Boundaries,,206,SCS-C02,AWS Certified Security - Specialty,,"Service control policies (SCPs) provide organization-wide guardrails that can explicitly deny actions like s3:DeleteBucket across all accounts, regardless of individual IAM user or role permissions. This is the most scalable way to prevent bucket deletion everywhere. Permissions boundaries and bucket policies must be applied per principal or bucket, and tag policies do not control access."
3.2,IAM Identity Center (SSO),"A security administrator is restricting the capabilities of company root user accounts. The company uses AWS Organizations and has all features enabled. The management account is used for billing and administrative purposes, but it is not used for operational AWS resource purposes. How can the security administrator restrict usage of member root user accounts across the organization?",Disable the use of the root user account at the organizational root. Enable multi-factor authentication (MFA) of the root user account for each organization member account.,Conﬁgure IAM user policies to restrict root account capabilities for each organization member account.,"Create an OU in Organizations, and attach an SCP that controls usage of the root user. Add all member accounts to the new OU.",Conﬁgure AWS CloudTrail to integrate with Amazon CloudWatch Logs. Create a metric ﬁlter for RootAccountUsage.,,,C,0,1,,,,0,0,,,,,4.1,Authentication & Authorization,,228,SCS-C02,AWS Certified Security - Specialty,,"Service control policies (SCPs) in AWS Organizations can restrict permissions for all principals in member accounts—including the root user—when all features are enabled. Placing accounts in an OU and attaching an SCP that denies root-user actions enforces restrictions organization-wide. The other options either cannot restrict the root user (IAM policies), only monitor activity (CloudTrail/CloudWatch), or are not possible (disabling root) and only add protection (MFA)."
3.2,IAM Identity Center (SSO),A company is implementing a customized notiﬁcation solution to detect repeated unauthorized authentication attempts to bastion hosts. The company’s security engineer needs to implement a solution that will provide notiﬁcation when 5 failed attempts occur within a 5- minute period. The solution must use native AWS services and must notify only the designated system administrator who is assigned to the speciﬁc bastion host. Which solution will meet these requirements?,Use the Amazon CloudWatch agent to collect operating system logs. Use Amazon EventBridge to conﬁgure an alarm based on a metric ﬁlter for failed login attempts. Send an alert to Amazon Simple Notiﬁcation Service (Amazon SNS) when the deﬁned threshold for the alarm is exceeded. Use Amazon EC2 instance tags to determine which SNS topics receive notiﬁcations.,Use AWS Systems Manager Agent to collect operating system logs. Use the Systems Manager Run Command AWS-ConﬁgureCloudWatch document to conﬁgure an Amazon EventBridge event based on a metric ﬁlter for failed login attempts. Send an alert to Amazon Simple Notiﬁcation Service (Amazon SNS) when the deﬁned threshold for the alarm is exceeded. Use SNS messaging ﬁlters to control who receives notiﬁcations.,Use the Amazon CloudWatch agent to collect operating system logs. Create a CloudWatch alarm based on a metric ﬁlter for failed login attempts. Send an alert to Amazon Simple Notiﬁcation Servige (Amazon SNS) when the deﬁned threshold for the alarm is exceeded. Use SNS messaging ﬁlters to control who receives notiﬁcations.,Use AWS Systems Manager Agent to collect operating system logs. Use the Systems Manager Run Command AWS-ConﬁgureCloudWatch document to conﬁgure an Amazon CloudWatch alarm based on a metric ﬁlter for failed login attempts. Send an alert to Amazon Simple Notiﬁcation Service (Amazon SNS) when the deﬁned threshold for the alarm is exceeded. Use EC2 instance tags to determine which SNS topics receive notiﬁcations.,,,C,0,1,,,,0,0,,,,,2.3,Monitoring & Detection,,240,SCS-C02,AWS Certified Security - Specialty,,"The CloudWatch agent ships OS auth logs to CloudWatch Logs, enabling a metric filter and CloudWatch alarm to detect 5 failures within a 5‑minute period, and SNS can deliver alerts. SNS message filtering lets you notify only the designated admin for each bastion host. The other options misuse EventBridge for metric alarms or rely on EC2 tags for SNS routing, and Systems Manager Agent isn’t required for this log ingestion."
3.2,IAM Identity Center (SSO),"A company has an organization in AWS Organizations that includes dedicated accounts for each of its business units. The company is collecting all AWS CloudTrail logs from the accounts in a single Amazon S3 bucket in the top-level account. The company’s IT governance team has access to the top-level account. A security engineer needs to allow each business unit to access its own CloudTrail logs. The security engineer creates an IAM role in the top-level account for each of the other accounts. For each role, the security engineer creates an IAM policy to allow read-only permissions to objects in the S3 bucket with the preﬁx of the respective logs. Which action must the security engineer take in each business unit account to allow an IAM user in that account to read the logs?",Attach a policy to the IAM user to allow the user to assume the role that was created in the top-level account. Specify the role’s ARN in the policy.,Create an SCP that grants permissions to the top-level account.,Use the root account of the business unit account to assume the role that was created in the top-level account. Specify the role’s ARN in the policy.,Forward the credentials of the IAM role in the top-level account to the IAM user in the business unit account.,,,A,0,1,,,,0,0,,,,,4.1,Authentication & Authorization,,250,SCS-C02,AWS Certified Security - Specialty,,"Cross-account access to the centralized S3 bucket is granted by assuming a role in the top-level account that has the necessary S3 read permissions. Therefore, each IAM user in a business unit account needs a policy allowing sts:AssumeRole on the specific role ARN. SCPs don’t grant permissions, root should not be used for this, and role credentials must not be shared."
3.2,IAM Identity Center (SSO),A security engineer is implementing authentication for a multi-account environment by using federated access with SAML 2.0. The security engineer has conﬁgured AWS IAM Identity Center as an identity provider (IdP). The security engineer also has created IAM roles to grant access to the AWS accounts. A federated user reports an authentication failure when the user attempts to authenticate with the new system. What should the security engineer do to troubleshoot this issue in the MOST operationally eﬃcient way?,Review the SAML IdP logs to identify errors. Check AWS CloudTrail to verify the API calls that the user made.,Review the SAML IdP logs to identify errors. Use the IAM policy simulator to validate access to the IAM roles.,Use IAM access advisor to review recent service access. Use the IAM policy simulator to validate access to the IAM roles.,Recreate the SAML IdP in a separate account to conﬁrm the behavior that the user is experiencing.,,,A,0,1,,,,0,0,,,,,4.2,Cross-Account & Federation,,264,SCS-C02,AWS Certified Security - Specialty,,"The quickest, most relevant troubleshooting for a SAML federation failure is to check the IdP’s SAML logs for assertion/attribute or mapping errors and use CloudTrail to verify STS/assume role calls and resulting failures. Tools like the IAM policy simulator or access advisor don’t diagnose SAML authentication issues, and recreating the IdP in another account is not operationally efficient."
3.2,IAM Identity Center (SSO),"A company is running workloads on AWS. The workloads are in separate AWS accounts for development, testing, and production. All the company’s developers can access the development account. A subset of the developers can access the testing account and the production account. The company is spending too much time managing individual credentials for every developer across every environment. A security engineer must implement a more scalable solution that the company can use when a developer needs different access. The solution must allow developers to access resources across multiple accounts. The solution also must minimize credential sharing. Which solution will meet these requirements?",Use AWS Identity and Access Management Access Analyzer to identify the permissions that the developers need on each account. Conﬁgure IAM Access Analyzer to automatically provision the correct access for each developer.,Create an Amazon Simple Workﬂow Service (Amazon SWF) workﬂow. Instruct the developers to use the workﬂow to request access to other accounts when additional access is necessary.,Create IAM roles in the testing account and production account. Add a policy that allows the sts:AssumeRole action to the roles. Create IAM roles in the development account for the developers who have access to the testing and production accounts. Add these roles to the trust policy on the new roles in the testing and production accounts.,Create service accounts in the testing environment and production environment. Give the access keys for the service accounts to developers who require access to the testing account and the production account. Rotate the access keys for the service accounts periodically.,,,C,0,1,,,,0,0,,,,,4.2,Cross-Account & Federation,,275,SCS-C02,AWS Certified Security - Specialty,,"Create cross-account IAM roles in the test and prod accounts that trust roles in the dev account and allow sts:AssumeRole, so developers obtain temporary credentials and least-privilege access without sharing long-term keys. The other options either don’t provision access (Access Analyzer), are not an auth mechanism (SWF), or require sharing static credentials (service accounts)."
3.2,IAM Identity Center (SSO),A company runs a custom online gaming application. The company uses Amazon Cognito for user authentication and authorization. A security engineer wants to use AWS to implement ﬁne-grained authorization on resources in the custom application. The security engineer must implement a solution that uses the user attributes that exist in Cognito. The company has already set up a user pool and an identity pool in Cognito. Which solution will meet these requirements?,Create a set of IAM roles and IAM policies. Conﬁgure the Cognito identity pool to assign users to the IAM roles.,Create a policy store in Amazon Veriﬁed Permissions. Conﬁgure Cognito as the identity source. Map Cognito access tokens to the Veriﬁed Permissions schema.,Create customer managed permissions by using AWS Resource Access Manager (AWS RAM). Conﬁgure the Cognito identity pool to assign users to the customer managed permissions.,Create a set of IAM users and IAM policies. Conﬁgure the Cognito user pool to assign users to the IAM users.,,,B,0,1,,,,0,0,,,,,4.1,Authentication & Authorization,,280,SCS-C02,AWS Certified Security - Specialty,,"Amazon Verified Permissions enables fine-grained, policy-based authorization for application resources and integrates directly with Amazon Cognito as an identity source, evaluating user attributes/claims from Cognito tokens. Mapping Cognito access tokens to the AVP schema enables ABAC across app resources, which IAM roles/users or AWS RAM cannot provide in this context."
3.2,IAM Identity Center (SSO),A company controls user access by using IAM users and groups in AWS accounts across an organization in AWS Organizations. The company uses an external identity provider (IdP) for workforce single sign-on (SSO). The company needs to implement a solution to provide a single management portal to access accounts within the organization. The solution must support the external IdP as a federation source. Which solution will meet these requirements?,Enable AWS IAM Identity Center. Specify the external IdP as the identity source.,Enable federation with AWS Identity and Access Management (IAM). Specify the external IdP as the identity source.,Migrate to Amazon Veriﬁed Permissions. Implement ﬁne-grained access to AWS by using policy-based access control (PBAC).,Migrate users to AWS Directory Service. Use AWS Control Tower to centralize security across the organization.,,,A,0,1,,,,0,0,,,,,4.2,Cross-Account & Federation,,290,SCS-C02,AWS Certified Security - Specialty,,"AWS IAM Identity Center (successor to AWS SSO) provides a single user portal for accessing multiple AWS accounts in an AWS Organizations organization and supports using an external IdP as the identity source via SAML/OIDC. IAM federation alone lacks a centralized portal, and Verified Permissions or Directory Service/Control Tower don’t solve workforce SSO to AWS accounts."
3.2,IAM Identity Center (SSO),A company uses Amazon Cognito for external user authentication for a web application. External users report that they can no longer log in to the application. What is the FIRST step that a security engineer should take to troubleshoot the problem?,Review AWS CloudTrail logs to identify authentication errors that relate to Cognito users.,Use AWS Identity and Access Management Access Analyzer to delete all unused IAM roles and users.,"Review any recent changes in Cognito conﬁguration, IAM policies, and role trust policies to identify issues.",Write a script that uses CLI commands to reset all user passwords in the Cognito user pool.,,,C,0,1,,,,0,0,,,,,4.1,Authentication & Authorization,,300,SCS-C02,AWS Certified Security - Specialty,,"A sudden, widespread login failure typically points to a recent configuration or policy change affecting the Cognito user pool or its IAM roles and trust relationships. The first troubleshooting step is to review recent changes in Cognito configuration, IAM policies, and role trust policies to quickly identify and revert misconfigurations. CloudTrail can help later for deeper forensics, and resetting passwords or deleting IAM resources is inappropriate and risky."
3.3,Resource-based Policies,"A company used AWS Organizations to set up an environment with multiple AWS accounts. The company's organization currently has two AWS accounts, and the company expects to add more than 50 AWS accounts during the next 12 months. The company will require all existing and future AWS accounts to use Amazon GuardDuty. Each existing AWS account has GuardDuty active. The company reviews GuardDuty ﬁndings by logging into each AWS account individually. The company wants a centralized view of the GuardDuty ﬁndings for the existing AWS accounts and any future AWS accounts. The company also must ensure that any new AWS account has GuardDuty automatically turned on. Which solution will meet these requirements?",Enable AWS Security Hub in the organization's management account. Conﬁgure GuardDuty within the management account to send all GuardDuty ﬁndings to Security Hub.,Create a new AWS account in the organization. Enable GuardDuty in the new account. Designate the new account as the delegated administrator account for GuardDuty. Conﬁgure GuardDuty to add existing accounts as member accounts. Select the option to automatically add new AWS accounts to the organization.,Create a new AWS account in the organization. Enable GuardDuty in the new account. Enable AWS Security Hub in each account. Select the option to automatically add new AWS accounts to the organization.,Enable AWS Security Hub in the organization's management account. Designate the management account as the delegated administrator account for Security Hub. Add existing accounts as member accounts. Select the option to automatically add new AWS accounts to the organization. Send all Security Hub ﬁndings to the organization's GuardDuty account.,,,B,0,1,,,,0,0,,,,,4.2,Cross-Account & Federation,,186,SCS-C02,AWS Certified Security - Specialty,,"Using a delegated administrator account for GuardDuty lets you centrally aggregate findings from member accounts and auto-enroll/auto-enable GuardDuty for any new AWS Organizations accounts. Option B sets up a dedicated security account as the GuardDuty admin, adds existing accounts as members, and enables automatic enrollment—meeting both central visibility and automatic activation. The Security Hub options (A, C, D) don’t centrally administer GuardDuty or ensure it’s auto-enabled across new accounts."
3.4,Federation (SAML/OpenID),A company is running internal microservices on Amazon Elastic Container Service (Amazon ECS) with the Amazon EC2 launch type. The company is using Amazon Elastic Container Registry (Amazon ECR) private repositories. A security engineer needs to encrypt the private repositories by using AWS Key Management Service (AWS KMS). The security engineer also needs to analyze the container images for any common vulnerabilities and exposures (CVEs). Which solution will meet these requirements?,Enable KMS encryption on the existing ECR repositories. Install Amazon Inspector Agent from the ECS container instances’ user data. Run an assessment with the CVE rules.,Recreate the ECR repositories with KMS encryption and ECR scanning enabled. Analyze the scan report after the next push of images.,Recreate the ECR repositories with KMS encryption and ECR scanning enabled. Install AWS Systems Manager Agent on the ECS container instances. Run an inventory report.,Enable KMS encryption on the existing ECR repositories. Use AWS Trusted Advisor to check the ECS container instances and to verify the ﬁndings against a list of current CVEs.,,,B,0,1,,,,0,0,,,,,3.4,Container & Serverless Security,,37,SCS-C02,AWS Certified Security - Specialty,,"Amazon ECR KMS encryption must be configured at repository creation time, so existing repositories cannot simply be “enabled” for KMS; they must be recreated with the desired KMS key. ECR also provides built-in image vulnerability scanning (scan on push) that reports CVEs after images are pushed, satisfying the CVE analysis requirement."
3.4,Federation (SAML/OpenID),"A security engineer is working with a product team building a web application on AWS. The application uses Amazon S3 to host the static content, Amazon API Gateway to provide RESTful services, and Amazon DynamoDB as the backend data store. The users already exist in a directory that is exposed through a SAML identity provider. Which combination of the following actions should the engineer take to allow users to be authenticated into the web application and call APIs? (Choose three.)",Create a custom authorization service using AWS Lambda.,Conﬁgure a SAML identity provider in Amazon Cognito to map attributes to the Amazon Cognito user pool attributes.,Conﬁgure the SAML identity provider to add the Amazon Cognito user pool as a relying party.,Conﬁgure an Amazon Cognito identity pool to integrate with social login providers.,Update DynamoDB to store the user email addresses and passwords.,Update API Gateway to use a COGNITO_USER_POOLS authorizer.,"B, C, F",1,1,,,,0,0,,,,,4.1,Authentication & Authorization,,87,SCS-C02,AWS Certified Security - Specialty,,"Cognito user pools can federate with a SAML IdP, so you configure the IdP to trust (relying party) the Cognito user pool (C) and map SAML attributes to user pool attributes (B). After authentication, API Gateway should use a Cognito User Pools authorizer to validate the JWT tokens on API calls (F). Other options either duplicate functionality or store credentials insecurely."
3.4,Federation (SAML/OpenID),A security analyst attempted to troubleshoot the monitoring of suspicious security group changes. The analyst was told that there is an Amazon CloudWatch alarm in place for these AWS CloudTrail log events. The analyst tested the monitoring setup by making a conﬁguration change to the security group but did not receive any alerts. Which of the following troubleshooting steps should the analyst perform?,Ensure that CloudTrail and S3 bucket access logging is enabled for the analyst's AWS account.,Verify that a metric ﬁlter was created and then mapped to an alarm. Check the alarm notiﬁcation action.,Check the CloudWatch dashboards to ensure that there is a metric conﬁgured with an appropriate dimension for security group changes.,Verify that the analyst's account is mapped to an IAM policy that includes permissions for cloudwatch:GetMetricStatistics and cloudwatch:ListMetrics.,,,B,0,1,,,,0,0,,,,,2.4,Alerting & Remediation,,201,SCS-C02,AWS Certified Security - Specialty,,"CloudTrail events generate CloudWatch Logs entries that must be matched by a metric filter, which then publishes a custom metric that the alarm monitors. Verifying the metric filter exists, is correctly mapped to the alarm, and that the alarm’s notification action (e.g., SNS) is configured explains why no alert was received. The other options don’t address the log-to-metric-to-alarm linkage required for alerts."
4.1,KMS & CMEK,A company has an application that uses dozens of Amazon DynamoDB tables to store data. Auditors ﬁnd that the tables do not comply with the company's data protection policy. The company's retention policy states that all data must be backed up twice each month: once at midnight on the 15th day of the month and again at midnight on the 25th day of the month. The company must retain the backups for 3 months. Which combination of steps should a security engineer take to meet these requirements? (Choose two.),Use the DynamoDB on-demand backup capability to create a backup plan. Conﬁgure a lifecycle policy to expire backups after 3 months.,Use AWS DataSync to create a backup plan. Add a backup rule that includes a retention period of 3 months.,Use AWS Backup to create a backup plan. Add a backup rule that includes a retention period of 3 months.,Set the backup frequency by using a cron schedule expression. Assign each DynamoDB table to the backup plan.,Set the backup frequency by using a rate schedule expression. Assign each DynamoDB table to the backup plan.,,"C, D",1,1,,,,0,0,,,,,5.1,Encryption at Rest,,6,SCS-C02,AWS Certified Security - Specialty,,"AWS Backup supports creating backup plans for DynamoDB with backup rules that enforce a 3‑month retention period (C). To run exactly at midnight on the 15th and 25th, you schedule the plan with cron expressions (D), since AWS Backup does not use rate expressions for precise calendar dates, and DynamoDB on-demand backups/DataSync do not provide the required scheduling and lifecycle retention controls."
4.1,KMS & CMEK,A company is designing a multi-account structure for its development teams. The company is using AWS Organizations and AWS IAM Identity Center (AWS Single Sign-On). The company must implement a solution so that the development teams can use only speciﬁc AWS Regions and so that each AWS account allows access to only speciﬁc AWS services. Which solution will meet these requirements with the LEAST operational overhead?,"Use IAM Identity Center to set up service-linked roles with IAM policy statements that include the Condition, Resource, and NotAction elements to allow access to only the Regions and services that are needed.",Deactivate AWS Security Token Service (AWS STS) in Regions that the developers are not allowed to use.,"Create SCPs that include the Condition, Resource, and NotAction elements to allow access to only the Regions and services that are needed.","For each AWS account, create tailored identity-based policies for IAM Identity Center. Use statements that include the Condition, Resource, and NotAction elements to allow access to only the Regions and services that are needed.",,,C,0,1,,,,0,0,,,,,4.1,Authentication & Authorization,,15,SCS-C02,AWS Certified Security - Specialty,,"Service control policies (SCPs) are the correct, centralized way to enforce organization-wide guardrails on which AWS services and Regions can be used across all accounts, including root, with minimal per-account management. Using Conditions (e.g., aws:RequestedRegion) and NotAction in SCPs lets you allow only specific services and Regions. Identity-based policies or disabling STS per-Region require more ongoing maintenance and don’t comprehensively enforce allowed services."
4.1,KMS & CMEK,"A company's public Application Load Balancer (ALB) recently experienced a DDoS attack. To mitigate this issue, the company deployed Amazon CloudFront in front of the ALB so that users would not directly access the Amazon EC2 instances behind the ALB. The company discovers that some traﬃc is still coming directly into the ALB and is still being handled by the EC2 instances. Which combination of steps should the company take to ensure that the EC2 instances will receive traﬃc only from CloudFront? (Choose two.)",Conﬁgure CloudFront to add a cache key policy to allow a custom HTTP header that CloudFront sends to the ALB.,Conﬁgure CloudFront to add a custom HTTP header to requests that CloudFront sends to the ALB.,Conﬁgure the ALB to forward only requests that contain the custom HTTP header.,Conﬁgure the ALB and CloudFront to use the X-Forwarded-For header to check client IP addresses.,Conﬁgure the ALB and CloudFront to use the same X.509 certiﬁcate that is generated by AWS Certiﬁcate Manager (ACM).,,"B, C",1,1,,,,0,0,,,,,3.2,Edge & Web Protection,,48,SCS-C02,AWS Certified Security - Specialty,,"By having CloudFront add a unique custom header on origin requests (B) and configuring the ALB to only forward requests that contain that header (C), only traffic that has passed through CloudFront will be accepted by the ALB. Direct client requests won’t include the secret header and will be rejected. The other options don’t enforce origin-only access (e.g., X-Forwarded-For can be spoofed, and cache key policies/certificates don’t gate origin access)."
4.1,KMS & CMEK,"A company that operates in a hybrid cloud environment must meet strict compliance requirements. The company wants to create a report that includes evidence from on-premises workloads alongside evidence from AWS resources. A security engineer must implement a solution to collect, review, and manage the evidence to demonstrate compliance with company policy. Which solution will meet these requirements?",Create an assessment in AWS Audit Manager from a prebuilt framework or a custom framework. Upload manual evidence from the on- premises workloads. Add the evidence to the assessment. Generate an assessment report after Audit Manager collects the necessary evidence from the AWS resources.,Install the Amazon CloudWatch agent on the on-premises workloads. Use AWS Conﬁg to deploy a conformance pack from a sample conformance pack template or a custom YAML template. Generate an assessment report after AWS Conﬁg identiﬁes noncompliant workloads and resources.,Set up the appropriate security standard in AWS Security Hub. Upload manual evidence from the on-premises workloads. Wait for Security Hub to collect the evidence from the AWS resources. Download the list of controls as a .csv ﬁle.,Install the Amazon CloudWatch agent on the on-premises workloads. Create a CloudWatch dashboard to monitor the on-premises workloads and the AWS resources. Run a query on the workloads and resources. Download the results.,,,A,0,1,,,,0,0,,,,,6.2,Compliance Validation,,56,SCS-C02,AWS Certified Security - Specialty,,"AWS Audit Manager is designed to collect, manage, and map audit evidence to compliance frameworks, and it supports both automated evidence from AWS resources and manual evidence uploads for on-premises systems. It can generate assessment reports aligned to prebuilt or custom frameworks, meeting hybrid compliance needs. Other services like Config, Security Hub, and CloudWatch are not intended for comprehensive audit evidence management and reporting."
4.1,KMS & CMEK,A company is using Amazon Route 53 Resolver for its hybrid DNS infrastructure. The company has set up Route 53 Resolver forwarding rules for authoritative domains that are hosted on on-premises DNS servers. A new security mandate requires the company to implement a solution to log and query DNS traﬃc that goes to the on-premises DNS servers. The logs must show details of the source IP address of the instance from which the query originated. The logs also must show the DNS name that was requested in Route 53 Resolver. Which solution will meet these requirements?,"Use VPC Traﬃc Mirroring. Conﬁgure all relevant elastic network interfaces as the traﬃc source, include amazon-dns in the mirror ﬁlter, and set Amazon CloudWatch Logs as the mirror target. Use CloudWatch Insights on the mirror session logs to run queries on the source IP address and DNS name.",Conﬁgure VPC ﬂow logs on all relevant VPCs. Send the logs to an Amazon S3 bucket. Use Amazon Athena to run SQL queries on the source IP address and DNS name.,Conﬁgure Route 53 Resolver query logging on all relevant VPCs. Send the logs to Amazon CloudWatch Logs. Use CloudWatch Insights to run queries on the source IP address and DNS name.,Modify the Route 53 Resolver rules on the authoritative domains that forward to the on-premises DNS servers. Send the logs to an Amazon S3 bucket. Use Amazon Athena to run SQL queries on the source IP address and DNS name.,,,C,0,1,,,,0,0,,,,,2.1,Logging Configuration,,65,SCS-C02,AWS Certified Security - Specialty,,"Route 53 Resolver query logging natively captures DNS queries resolved or forwarded by the Resolver, including the querying source IP and the requested domain name. Logs can be sent to CloudWatch Logs and queried with CloudWatch Logs Insights. Other options either don’t capture DNS names (VPC flow logs), are overly complex and not supported as described (Traffic Mirroring), or misunderstand where logging is configured (not on Resolver rules)."
4.1,KMS & CMEK,"An international company wants to combine AWS Security Hub ﬁndings across all the company's AWS Regions and from multiple accounts. In addition, the company wants to create a centralized custom dashboard to correlate these ﬁndings with operational data for deeper analysis and insights. The company needs an analytics tool to search and visualize Security Hub ﬁndings. Which combination of steps will meet these requirements? (Chose three.)","Designate an AWS account as a delegated administrator for Security Hub. Publish events to Amazon CloudWatch from the delegated administrator account, all member accounts, and required Regions that are enabled for Security Hub ﬁndings.","Designate an AWS account in an organization in AWS Organizations as a delegated administrator for Security Hub. Publish events to Amazon EventBridge from the delegated administrator account, all member accounts, and required Regions that are enabled for Security Hub ﬁndings.","In each Region, create an Amazon EventBridge rule to deliver ﬁndings to an Amazon Kinesis data stream. Conﬁgure the Kinesis data streams to output the logs to a single Amazon S3 bucket.","In each Region, create an Amazon EventBridge rule to deliver ﬁndings to an Amazon Kinesis Data Firehose delivery stream. Conﬁgure the Kinesis Data Firehose delivery streams to deliver the logs to a single Amazon S3 bucket.",Use AWS Glue DataBrew to crawl the Amazon S3 bucket and build the schema. Use AWS Glue Data Catalog to query the data and create views to ﬂatten nested attributes. Build Amazon QuickSight dashboards by using Amazon Athena.,Partition the Amazon S3 data. Use AWS Glue to crawl the S3 bucket and build the schema. Use Amazon Athena to query the data and create views to ﬂatten nested attributes. Build Amazon QuickSight dashboards that use the Athena views.,"B, D, F",1,1,,,,0,0,,,,,2.2,Centralization & Retention,,116,SCS-C02,AWS Certified Security - Specialty,,"B centralizes Security Hub administration across accounts and Regions using AWS Organizations and EventBridge to capture findings. D reliably delivers those findings from each Region to a single S3 bucket via Kinesis Data Firehose without custom consumers. F enables analytics by partitioning S3 data, using Glue to catalog the schema, querying and flattening with Athena, and visualizing in QuickSight."
4.1,KMS & CMEK,A company is investigating controls to protect sensitive data. The company uses Amazon Simple Notiﬁcation Service (Amazon SNS) topics to publish messages from application components to custom logging services. The company is concerned that an application component might publish sensitive data that will be accidentally exposed in transaction logs and debug logs. Which solution will protect the sensitive data in these messages from accidental exposure?,Use Amazon Made to scan the SNS topics for sensitive data elements in the SNS messages. Create an AWS Lambda function that masks sensitive data inside the messages when Macie records a new ﬁnding.,"Conﬁgure an inbound message data protection policy. In the policy, include the De-identify operation to mask the sensitive data inside the messages. Apply the policy to the SNS topics.",Conﬁgure the SNS topics with an AWS Key Management Service (AWS KMS) customer managed key to encrypt the data elements inside the messages. Grant permissions to all message publisher IAM roles to allow access to the key to encrypt data.,Create an Amazon GuardDuty ﬁnding for sensitive data that is transmitted to the SNS topics. Create an AWS Security Hub custom remediation action to block messages that contain sensitive data from being delivered to subscribers of the SNS topics.,,,B,0,1,,,,0,0,,,,,5.4,Data Classification & Access Control,,163,SCS-C02,AWS Certified Security - Specialty,,"Amazon SNS supports message data protection with inbound data protection policies that can detect and de-identify (mask) sensitive data in messages before they are stored or delivered. This proactively prevents accidental exposure in logs. KMS only encrypts at rest/in transit, and Macie/GuardDuty are not designed to mask SNS message content or provide real-time prevention."
4.1,KMS & CMEK,"A company needs to implement DNS Security Extensions (DNSSEC) for a speciﬁc subdomain. The subdomain is already registered with Amazon Route 53. A security engineer has enabled DNSSEC signing and has created a key-signing key (KSK). When the security engineer tries to test the conﬁguration, the security engineer receives an error for a broken trust chain. What should the security engineer do to resolve this error?",Replace the KSK with a zone-signing key (ZSK).,Deactivate and then activate the KSK.,Create a Delegation Signer (DS) record in the parent hosted zone.,Create a Delegation Signer (DS) record in the subdomain.,,,C,0,1,,,,0,0,,,,,3.2,Edge & Web Protection,,185,SCS-C02,AWS Certified Security - Specialty,,"DNSSEC’s chain of trust requires the parent zone to publish a DS record that references the child zone’s KSK. Without this DS record in the parent hosted zone, validators cannot establish trust to the signed subdomain, resulting in a broken trust chain."
4.1,KMS & CMEK,"A security engineer for a large company is managing a data processing application used by 1,500 subsidiary companies. The parent and subsidiary companies all use AWS. The application uses TCP port 443 and runs on Amazon C2 behind a Network Load Balancer (NLB). For compliance reasons, the application should only be accessible to the subsidiaries and should not be available on the public internet. To meet the compliance requirements for restricted access, the engineer has received the public and private CIDR block ranges for each subsidiary. What solution should the engineer use to implement the appropriate access restrictions for the application?","Create a NACL to allow access on TCP port 443 from the 1,500 subsidiary CIDR block ranges. Associate the NACL to both the NLB and EC2 instances.","Create an AWS security group to allow access on TCP port 443 from the 1,500 subsidiary CIDR block ranges. Associate the security group to the NLCreate a second security group for EC2 instances with access on TCP port 443 from the NLB security group.","Create an AWS PrivateLink endpoint service in the parent company account attached to the NLB. Create an AWS security group for the instances to allow access on TCP port 443 from the AWS PrivateLink endpoint. Use AWS PrivateLink interface endpoints in the 1,500 subsidiary AWS accounts to connect to the data processing application.","Create an AWS security group to allow access on TCP port 443 from the 1,500 subsidiary CIDR block ranges. Associate the security group with EC2 instances.",,,C,0,1,,,,0,0,,,,,3.1,Network Architecture Security,,210,SCS-C02,AWS Certified Security - Specialty,,"AWS PrivateLink allows the parent account to expose the NLB as a private endpoint service and subsidiaries to connect via interface endpoints over the AWS network, keeping the application off the public internet. This scales to 1,500 accounts without massive CIDR lists or NACL/SG rule limits, and security groups can restrict instance access to traffic coming only from the PrivateLink endpoint."
4.1,KMS & CMEK,A company hosts its public website on Amazon EC2 instances behind an Application Load Balancer (ALB). The website is experiencing a global DDoS attack by a speciﬁc IoT device brand that has a unique user agent. A security engineer is creating an AWS WAF web ACL and will associate the web ACL with the ALB. The security engineer must implement a rule statement as part of the web ACL to block the requests. The rule statement must mitigate the current attack and future attacks from these IoT devices without blocking requests from customers. Which rule statement will meet these requirements?,Use an IP set match rule statement that includes the IP address for IoT devices from the user agent.,Use a geographic match rule statement. Conﬁgure the statement to block countries that the IoT devices are located in.,Use a rate-based rule statement. Set a rate limit that is equal to the number of requests that are coming from the IoT devices.,Use a string match rule statement that includes details of the IoT device brand from the user agent.,,,D,0,1,,,,0,0,,,,,2.3,Monitoring & Detection,,226,SCS-C02,AWS Certified Security - Specialty,,"Because the IoT devices share a unique User-Agent string, a WAF string match on the User-Agent header precisely targets and blocks that device brand without affecting legitimate users. IP- or geo-based rules are too broad for a globally distributed botnet, and a rate-based rule risks blocking real customers and may miss low-and-slow distributed attacks."
4.1,KMS & CMEK,"A security engineer is implementing a logging solution for a company’s AWS environment. The security engineer has conﬁgured an AWS CloudTrail trail in the company’s AWS account. The logs are stored in an Amazon S3 bucket for a third-party service provider to monitor. The service provider has a designated IAM role to access the S3 bucket. The company requires all logs to be encrypted at rest with a customer managed key. The security engineer uses AWS Key Management Service (AWS KMS) to create the customer managed key and key policy. The security engineer also conﬁgures CloudTrail to use the key to encrypt the trail. When the security engineer implements this conﬁguration, the service provider no longer can read the logs. What should the security engineer do to allow the service provider to read the logs?",Ensure that the S3 bucket policy allows access to the service provider’s role to decrypt objects.,Add a statement to the key policy to allow the service provider’s role the kms:Decrypt action for the key.,Add the AWSKeyManagementServicePowerUser AWS managed policy to the service provider’s role.,Migrate the key to AWS Certiﬁcate Manager (ACM) to create a shared endpoint for access to the key.,,,B,0,1,,,,0,0,,,,,5.3,Key Management,,278,SCS-C02,AWS Certified Security - Specialty,,"When CloudTrail encrypts S3 objects with SSE-KMS using a customer managed CMK, readers need both S3 read permissions and KMS permissions on the CMK. Adding a key policy statement granting the service provider role kms:Decrypt (and typically kms:DescribeKey) enables decryption; bucket policies can’t grant KMS decrypt and ACM is unrelated."
4.2,S3 Encryption & Access,"A security engineer creates an Amazon S3 bucket policy that denies access to all users. A few days later, the security engineer adds an additional statement to the bucket policy to allow read-only access to one other employee. Even after updating the policy, the employee sill receives an access denied message. What is the likely cause of this access denial?",The ACL in the bucket needs to be updated.,The IAM policy does not allow the user to access the bucket.,It takes a few minutes for a bucket policy to take effect.,The allow permission is being overridden by the deny.,,,D,0,1,,,,0,0,,,,,4.1,Authentication & Authorization,,22,SCS-C02,AWS Certified Security - Specialty,,"AWS policy evaluation applies the rule that an explicit Deny always overrides any Allow. Since the bucket policy already had a blanket deny to all principals, the later allow statement cannot grant access unless the deny is removed or scoped with conditions. IAM policies or ACLs cannot override an explicit deny, and bucket policy changes take effect quickly."
4.2,S3 Encryption & Access,An ecommerce company has a web application architecture that runs primarily on containers. The application containers are deployed on Amazon Elastic Container Service (Amazon ECS). The container images for the application are stored in Amazon Elastic Container Registry (Amazon ECR). The company's security team is performing an audit of components of the application architecture. The security team identiﬁes issues with some container images that are stored in the container repositories. The security team wants to address these issues by implementing continual scanning and on-push scanning of the container images. The security team needs to implement a solution that makes any ﬁndings from these scans visible in a centralized dashboard. The security team plans to use the dashboard to view these ﬁndings along with other security-related ﬁndings that they intend to generate in the future. There are speciﬁc repositories that the security team needs to exclude from the scanning process. Which solution will meet these requirements?,Use Amazon Inspector. Create inclusion rules in Amazon ECR to match repositories that need to be scanned. Push Amazon Inspector ﬁndings to AWS Security Hub.,Use ECR basic scanning of container images. Create inclusion rules in Amazon ECR to match repositories that need to be scanned. Push ﬁndings to AWS Security Hub.,Use ECR basic scanning of container images. Create inclusion rules in Amazon ECR to match repositories that need to be scanned. Push ﬁndings to Amazon Inspector.,Use Amazon Inspector. Create inclusion rules in Amazon Inspector to match repositories that need to be scanned. Push Amazon Inspector ﬁndings to AWS Conﬁg.,,,A,0,1,,,,0,0,,,,,3.4,Container & Serverless Security,,32,SCS-C02,AWS Certified Security - Specialty,,"Amazon Inspector’s enhanced ECR scanning provides both on-push and continuous scanning, and its findings natively integrate with AWS Security Hub for a centralized dashboard. Repository filter rules are configured in Amazon ECR to include/exclude specific repositories. ECR basic scanning only supports on-push and lacks continuous scanning and direct Security Hub integration."
4.2,S3 Encryption & Access,A company uses Amazon RDS for MySQL as a database engine for its applications. A recent security audit revealed an RDS instance that is not compliant with company policy for encrypting data at rest. A security engineer at the company needs to ensure that all existing RDS databases are encrypted using server-side encryption and that any future deviations from the policy are detected. Which combination of steps should the security engineer take to accomplish this? (Choose two.),Create an AWS Conﬁg rule to detect the creation of unencrypted RDS databases. Create an Amazon EventBridge rule to trigger on the AWS Conﬁg rules compliance state change and use Amazon Simple Notiﬁcation Service (Amazon SNS) to notify the security operations team.,Use AWS System Manager State Manager to detect RDS database encryption conﬁguration drift. Create an Amazon EventBridge rule to track state changes and use Amazon Simple Notiﬁcation Service (Amazon SNS) to notify the security operations team.,"Create a read replica for the existing unencrypted RDS database and enable replica encryption in the process. Once the replica becomes active, promote it into a standalone database instance and terminate the unencrypted database instance.",Take a snapshot of the unencrypted RDS database. Copy the snapshot and enable snapshot encryption in the process. Restore the database instance from the newly created encrypted snapshot. Terminate the unencrypted database instance.,Enable encryption for the identiﬁed unencrypted RDS instance by changing the conﬁgurations of the existing database.,,"A, D",1,1,,,,0,0,,,,,5.1,Encryption at Rest,,44,SCS-C02,AWS Certified Security - Specialty,,"RDS encryption at rest cannot be enabled in place; you must take a snapshot, copy it with encryption, and restore a new encrypted instance (D). To prevent and detect future noncompliant instances, use AWS Config to evaluate RDS storage-encrypted compliance and EventBridge to notify via SNS on compliance state changes (A)."
4.2,S3 Encryption & Access,A company plans to use AWS Key Management Service (AWS KMS) to implement an encryption strategy to protect data at rest. The company requires client-side encryption for company projects. The company is currently conducting multiple projects to test the company's use of AWS KMS. These tests have led to a sudden increase in the company's AWS resource consumption. The test projects include applications that issue multiple requests each second to KMS endpoints for encryption activities. The company needs to develop a solution that does not throttle the company's ability to use AWS KMS. The solution must improve key usage for client-side encryption and must be cost optimized. Which solution will meet these requirements?,Use keyrings with the AWS Encryption SDK. Use each keyring individually or combine keyrings into a multi-keyring. Decrypt the data by using a keyring that has the primary key in the multi-keyring.,Use data key caching. Use the local cache that the AWS Encryption SDK provides with a caching cryptographic materials manager.,Use KMS key rotation. Use a local cache in the AWS Encryption SDK with a caching cryptographic materials manager.,Use keyrings with the AWS Encryption SDK. Use each keyring individually or combine keyrings into a multi-keyring. Use any of the wrapping keys in the multi-keyring to decrypt the data.,,,B,0,1,,,,0,0,,,,,5.3,Key Management,,72,SCS-C02,AWS Certified Security - Specialty,,"Data key caching with the AWS Encryption SDK’s caching cryptographic materials manager lets clients securely reuse data keys within configurable TTL and byte-use limits, reducing frequent GenerateDataKey/Decrypt calls to KMS. This minimizes throttling and lowers KMS costs while maintaining client-side encryption. Keyrings or KMS key rotation do not reduce KMS call volume."
4.2,S3 Encryption & Access,"A company is evaluating its security posture. In the past, the company has observed issues with speciﬁc hosts and host header combinations that affected the company's business. The company has conﬁgured AWS WAF web ACLs as an initial step to mitigate these issues. The company must create a log analysis solution for the AWS WAF web ACLs to monitor problematic activity. The company wants to process all the AWS WAF logs in a central location. The company must have the ability to ﬁlter out requests based on speciﬁc hosts. A security engineer starts to enable access logging for the AWS WAF web ACLs. What should the security engineer do next to meet these requirements with the MOST operational eﬃciency?",Specify Amazon Redshift as the destination for the access logs. Deploy the Amazon Athena Redshift connector. Use Athena to query the data from Amazon Redshift and to ﬁlter the logs by host.,Specify Amazon CloudWatch as the destination for the access logs. Use Amazon CloudWatch Logs Insights to design a query to ﬁlter the logs by host.,Specify Amazon CloudWatch as the destination for the access logs. Export the CloudWatch logs to an Amazon S3 bucket. Use Amazon Athena to query the logs and to ﬁlter the logs by host.,Specify Amazon CloudWatch as the destination for the access logs. Use Amazon Redshift Spectrum to query the logs and to ﬁlter the logs by host.,,,B,0,1,,,,0,0,,,,,2.2,Centralization & Retention,,82,SCS-C02,AWS Certified Security - Specialty,,"AWS WAF can natively deliver access logs to Amazon CloudWatch Logs, and CloudWatch Logs Insights lets you query the JSON fields (including the host header) directly without extra data movement. This provides centralized, real-time filtering with minimal setup and operational overhead versus exporting to S3/Athena or using Redshift."
4.2,S3 Encryption & Access,"A company is running an Amazon RDS for MySQL DB instance in a VPC. The VPC must not send or receive network traﬃc through the internet. A security engineer wants to use AWS Secrets Manager to rotate the DB instance credentials automatically. Because of a security policy, the security engineer cannot use the standard AWS Lambda function that Secrets Manager provides to rotate the credentials. The security engineer deploys a custom Lambda function in the VPC. The custom Lambda function will be responsible for rotating the secret in Secrets Manager. The security engineer edits the DB instance's security group to allow connections from this function. When the function is invoked, the function cannot communicate with Secrets Manager to rotate the secret properly. What should the security engineer do so that the function can rotate the secret?",Add an egress-only internet gateway to the VPC. Allow only the Lambda function's subnet to route traﬃc through the egress-only internet gateway.,Add a NAT gateway to the VPC. Conﬁgure only the Lambda function's subnet with a default route through the NAT gateway.,Conﬁgure a VPC peering connection to the default VPC for Secrets Manager. Conﬁgure the Lambda function's subnet to use the peering connection for routes.,Conﬁgure a Secrets Manager interface VPC endpoint. Include the Lambda function's private subnet during the conﬁguration process.,,,D,0,1,,,,0,0,,,,,3.1,Network Architecture Security,,94,SCS-C02,AWS Certified Security - Specialty,,"The Lambda function in a VPC without internet cannot reach the public Secrets Manager endpoints. An interface VPC endpoint (AWS PrivateLink) for Secrets Manager provides private connectivity via ENIs and VPC DNS, allowing the function to access Secrets Manager without internet. NAT or egress-only internet gateways would introduce internet paths, and there is no VPC to peer with for Secrets Manager."
4.2,S3 Encryption & Access,A company needs complete encryption of the traﬃc between external users and an application. The company hosts the application on a ﬂeet of Amazon EC2 instances that run in an Auto Scaling group behind an Application Load Balancer (ALB). How can a security engineer meet these requirements?,Create a new Amazon-issued certiﬁcate in AWS Secrets Manager. Export the certiﬁcate from Secrets Manager. Import the certiﬁcate into the ALB and the EC2 instances.,Create a new Amazon-issued certiﬁcate in AWS Certiﬁcate Manager (ACM). Associate the certiﬁcate with the ALExport the certiﬁcate from ACM. Install the certiﬁcate on the EC2 instances.,Import a new third-party certiﬁcate into AWS Identity and Access Management (IAM). Export the certiﬁcate from IAM. Associate the certiﬁcate with the ALB and the EC2 instances.,Import a new third-party certiﬁcate into AWS Certiﬁcate Manager (ACM). Associate the certiﬁcate with the ALB. Install the certiﬁcate on the EC2 instances.,,,D,0,1,,,,0,0,,,,,5.2,Encryption in Transit,,103,SCS-C02,AWS Certified Security - Specialty,,"End-to-end encryption requires HTTPS from clients to the ALB and from the ALB to the EC2 instances, so both the ALB and the instances need certificates. ACM can attach a certificate to the ALB, but ACM-issued public certificates cannot be exported for use on the instances; therefore, you import a third‑party certificate into ACM for the ALB and install the certificate directly on the instances. Secrets Manager/IAM are not appropriate for this use."
4.2,S3 Encryption & Access,"A security engineer wants to forward custom application-security logs from an Amazon EC2 instance to Amazon CloudWatch. The security engineer installs the CloudWatch agent on the EC2 instance and adds the path of the logs to the CloudWatch conﬁguration ﬁle. However, CloudWatch does not receive the logs. The security engineer veriﬁes that the awslogs service is running on the EC2 instance. What should the security engineer do next to resolve the issue?",Add AWS CloudTrail to the trust policy of the EC2 in stance. Send the custom logs to CloudTrail instead of CloudWatch.,Add Amazon S3 to the trust policy of the EC2 instance. Conﬁgure the application to write the custom logs to an S3 bucket that CloudWatch can use to ingest the logs.,Add Amazon Inspector to the trust policy of the EC2 instance. Use Amazon Inspector instead of the CloudWatch agent to collect the custom logs.,Attach the CloudWatchAgentServerPolicy AWS managed policy to the EC2 instance role.,,,D,0,1,,,,0,0,,,,,2.1,Logging Configuration,,106,SCS-C02,AWS Certified Security - Specialty,,The CloudWatch agent must have IAM permissions to create log groups/streams and put log events in CloudWatch Logs. Attaching the AWS managed CloudWatchAgentServerPolicy to the EC2 instance role grants these required permissions so logs can be forwarded. The other options involve unrelated services or incorrect trust policy changes and will not enable log ingestion.
4.2,S3 Encryption & Access,"A security engineer is creating an AWS Lambda function. The Lambda function needs to use a role that is named LambdaAuditRole to assume a role that is named AcmeAuditFactoryRole in a different AWS account. When the code is processed, the following error message appears: ""An error occurred (AccessDenied) when calling the AssumeRole operation."" Which combination of steps should the security engineer take to resolve this error? (Choose two.)",Ensure that LambdaAuditRole has the sts:AssumeRole permission for AcmeAuditFactoryRole.,Ensure that LambdaAuditRole has the AWSLambdaBasicExecutionRole managed policy attached.,Ensure that the trust policy for AcmeAuditFactoryRole allows the sts:AssumeRole action from LambdaAuditRole.,Ensure that the trust policy for LambdaAuditRole allows the sts:AssumeRole action from the lambda.amazonaws.com service.,Ensure that the sts:AssumeRole API call is being issued to the us-east-1 Region endpoint.,,"A, C",1,1,,,,0,0,,,,,4.2,Cross-Account & Federation,,111,SCS-C02,AWS Certified Security - Specialty,,"AssumeRole requires both sides to be configured: the caller’s role (LambdaAuditRole) must have IAM permission to call sts:AssumeRole on the target role (A), and the target role’s trust policy must allow that principal to assume it (C). The other options don’t fix an AssumeRole AccessDenied: AWSLambdaBasicExecutionRole is for logging, the Lambda service trust is for Lambda assuming its execution role (not cross-account), and STS is a global service so the Region endpoint isn’t the issue."
4.2,S3 Encryption & Access,"A developer operations team uses AWS Identity and Access Management (IAM) to manage user permissions. The team created an Amazon EC2 instance proﬁle role that uses an AWS managed ReadOnlyAccess policy. When an application that is running on Amazon EC2 tries to read a ﬁle from an encrypted Amazon S3 bucket, the application receives an AccessDenied error. The team administrator has veriﬁed that the S3 bucket policy allows everyone in the account to access the S3 bucket. There is no object ACL that is attached to the ﬁle. What should the administrator do to ﬁx the IAM access issue?",Edit the ReadOnlyAccess policy to add kms:Decrypt actions,Add the EC2 IAM role as the authorized Principal to the S3 bucket policy,Attach an inline policy with kms:Decrypt permissions to the IAM role,Attach an inline policy with S3:* permissions to the IAM role,,,C,0,1,,,,0,0,,,,,4.1,Authentication & Authorization,,145,SCS-C02,AWS Certified Security - Specialty,,"The object is encrypted with SSE-KMS, so in addition to S3 read permissions the instance role must have permission to use the KMS key (kms:Decrypt). The AWS managed ReadOnlyAccess policy doesn’t include KMS decrypt, and you cannot modify it, so attaching an inline policy granting kms:Decrypt (to the specific CMK) to the role resolves the AccessDenied. Changing the bucket policy or adding broader S3 permissions won’t satisfy the KMS requirement."
4.2,S3 Encryption & Access,"A company in France uses Amazon Cognito with the Cognito Hosted UI as an identity broker for sign-in and sign-up processes. The company is marketing an application and expects that all the application’s users will come from France. When the company launches the application, the company’s security team observes fraudulent sign-ups for the application. Most of the fraudulent registrations are from users outside of France. The security team needs a solution to perform custom validation at sign-up. Based on the results of the validation, the solution must accept or deny the registration request. Which combination of steps will meet these requirements? (Choose two.)",Create a pre sign-up AWS Lambda trigger. Associate the Amazon Cognito function with the Amazon Cognito user pool.,Use a geographic match rule statement to conﬁgure an AWS WAF web ACL Associate the web ACL with the Amazon Cognito user pool.,Conﬁgure an app client for the application's Amazon Cognito user pool. Use the app client ID to validate the requests in the hosted UI.,Update the application’s Amazon Cognito user pool to conﬁgure a geographic restriction setting.,Use Amazon Cognito to conﬁgure a social identity provider (IdP) to validate the requests on the hosted UI.,,"A, B",1,1,,,,0,0,,,,,4.1,Authentication & Authorization,,150,SCS-C02,AWS Certified Security - Specialty,,"A is correct because a pre sign-up Lambda trigger lets you run custom validation logic during registration and explicitly accept or reject the sign-up. B is correct because you can associate an AWS WAF web ACL with the Cognito Hosted UI and use a geo match rule to block requests from outside France, reducing fraudulent sign-ups before they reach Cognito."
4.2,S3 Encryption & Access,"A company wants to implement host-based security for Amazon EC2 instances and containers in Amazon Elastic Container Registry (Amazon ECR). The company has deployed AWS Systems Manager Agent (SSM Agent) on the EC2 instances. All the company's AWS accounts are in one organization in AWS Organizations. The company will analyze the workloads for software vulnerabilities and unintended network exposure. The company will push any ﬁndings to AWS Security Hub, which the company has conﬁgured for the organization. The company must deploy the solution to all member accounts, including new accounts, automatically. When new workloads come online, the solution must scan the workloads. Which solution will meet these requirements?",Use SCPs to conﬁgure scanning of EC2 instances and ECR containers for all accounts in the organization.,Conﬁgure a delegated administrator for Amazon GuardDuty for the organization. Create an Amazon EventBridge rule to initiate analysis of ECR containers,Conﬁgure a delegated administrator for Amazon Inspector for the organization. Conﬁgure automatic scanning for new member accounts.,Conﬁgure a delegated administrator for Amazon Inspector for the organization. Create an AWS Conﬁg rule to initiate analysis of ECR containers.,,,C,0,1,,,,0,0,,,,,3.3,Host & Instance Hardening,,153,SCS-C02,AWS Certified Security - Specialty,,"Amazon Inspector (v2) provides automated host-based vulnerability scanning for EC2 instances (via SSM Agent) and container image scanning for ECR, with built-in integration to Security Hub. It supports an organization-level delegated administrator and auto-enablement for new member accounts and new resources, ensuring new workloads are scanned automatically. SCPs and AWS Config cannot initiate such scans, and GuardDuty focuses on threat detection, not software vulnerability scanning."
4.2,S3 Encryption & Access,A company has AWS accounts in an organization in AWS Organizations. The company needs to install a corporate software package on all Amazon EC2 instances for all the accounts in the organization. A central account provides base AMIs for the EC2 instances. The company uses AWS Systems Manager for software inventory and patching operations. A security engineer must implement a solution that detects EC2 instances that do not have the required software. The solution also must automatically install the software if the software is not present. Which solution will meet these requirements?,Provide new AMIs that have the required software pre-installed. Apply a tag to the AMIs to indicate that the AMIs have the required software. Conﬁgure an SCP that allows new EC2 instances to be launched only if the instances have the tagged AMIs. Tag all existing EC2 instances.,Conﬁgure a custom patch baseline in Systems Manager Patch Manager. Add the package name for the required software to the approved packages list. Associate the new patch baseline with all EC2 instances. Set up a maintenance window for software deployment.,Centrally enable AWS Conﬁg. Set up the ec2-managedinstance-applications-required AWS Conﬁg rule for all accounts. Create an Amazon EventBridge rule that reacts to AWS Conﬁg events. Conﬁgure the EventBridge rule to invoke an AWS Lambda function that uses Systems Manager Run Command to install the required software.,Create a new Systems Manager Distributor package for the required software. Specify the download location. Select all EC2 instances in the different accounts. Install the software by using Systems Manager Run Command.,,,C,0,1,,,,0,0,,,,,1.2,Detection & Investigation,,158,SCS-C02,AWS Certified Security - Specialty,,"Option C uses AWS Config’s ec2-managedinstance-applications-required rule to continuously detect instances missing the software across the organization, then automatically remediates via EventBridge and a Lambda that runs SSM Run Command to install it. This provides centralized, ongoing compliance and auto-installation. The other options either only address new instances (A), misuse Patch Manager for custom software (B), or are a one-time/manual action without continuous detection (D)."
4.2,S3 Encryption & Access,"A company hosts an application on Amazon EC2 instances. The application also uses Amazon S3 and Amazon Simple Queue Service (Amazon SQS). The application is behind an Application Load Balancer (ALB) and scales with AWS Auto Scaling. The company's security policy requires the use of least privilege access, which has been applied to all existing AWS resources. A security engineer needs to implement private connectivity to AWS services. Which combination of steps should the security engineer take to meet this requirement? (Choose three.)",Use an interface VPC endpoint for Amazon SQS.,Conﬁgure a connection to Amazon S3 through AWS Transit Gateway.,Use a gateway VPC endpoint for Amazon S3.,Modify the IAM role applied to the EC2 instances in the Auto Scaling group to allow outbound traﬃc to the interface endpoints.,Modify the endpoint policies on all VPC endpoints. Specify the SQS and S3 resources that the application uses.,Conﬁgure a connection to Amazon S3 through AWS Firewall Manager.,"A, C, E",1,1,,,,0,0,,,,,3.1,Network Architecture Security,,200,SCS-C02,AWS Certified Security - Specialty,,"Private connectivity to AWS services is provided via VPC endpoints: use an interface VPC endpoint for SQS and a gateway VPC endpoint for S3. To maintain least privilege, restrict access with endpoint policies that allow only the specific S3 buckets and SQS queues the app uses. IAM roles don’t control network paths, and Transit Gateway or Firewall Manager are not required for this use case."
4.2,S3 Encryption & Access,A company is migrating its Amazon EC2 based applications to use Instance Metadata Service Version 2 (IMDSv2). A security engineer needs to determine whether any of the EC2 instances are still using Instance Metadata Service Version 1 (IMDSv1). What should the security engineer do to conﬁrm that the IMDSv1 endpoint is no longer being used?,Conﬁgure logging on the Amazon CloudWatch agent for IMDSv1 as part of EC2 instance startup. Create a metric ﬁlter and a CloudWatch dashboard. Track the metric in the dashboard.,Create an Amazon CloudWatch dashboard. Verify that the EC2:MetadataNoToken metric is zero across all EC2 instances. Monitor the dashboard.,Create a security group that blocks access to HTTP for the IMDSv1 endpoint. Attach the security group to all EC2 instances.,Conﬁgure user data scripts for all EC2 instances to send logging information to AWS CloudTrail when IMDSV1 is used. Create a metric ﬁlter and an Amazon CloudWatch dashboard. Track the metric in the dashboard.,,,B,0,1,,,,0,0,,,,,3.1,Network Architecture Security,,221,SCS-C02,AWS Certified Security - Specialty,,"EC2 emits the EC2:MetadataNoToken CloudWatch metric, which counts metadata requests made without an IMDSv2 token (i.e., IMDSv1 usage). If this metric is zero across all instances, IMDSv1 is no longer being used. Other options are ineffective: security groups can’t block the link-local metadata endpoint, and CloudTrail/CloudWatch Agent don’t natively log IMDS calls."
4.2,S3 Encryption & Access,A company uses Amazon Elastic Container Registry (Amazon ECR) as the repository for its production applications. A security engineer must implement an automated solution to report any vulnerabilities that ECR enhanced scanning detects. The solution must provide notiﬁcation of vulnerability ﬁndings in an instant message to the company’s Slack account Which solution will meet these requirements with the MOST operational eﬃciency?,Activate Amazon Inspector scans for the ECR repository. Create an Amazon Simple Notiﬁcation Service (Amazon SNS) topic. Conﬁgure an AWS Chatbot client for Slack that consumes the SNS topic. Create an Amazon EventBridge rule for Amazon Inspector ﬁndings. Specify the SNS topic as the target for the rule.,Activate Amazon Inspector scans for the ECR repository. Write a script to use AWS CLI commands to retrieve image scan ﬁndings from Amazon Inspector. Conﬁgure the script to send the ﬁndings to a Slack endpoint. Launch an Amazon EC2 instance to run the script.,Activate Amazon Inspector scans for the ECR repository. Create an AWS Step Functions state machine. Set a ﬁrst step in the state machine to call the Amazon Inspector ListFindings API operation. Create an Amazon Simple Notiﬁcation Service (Amazon SNS) topic with Slack as the target. Add a second step in the state machine to call the Amazon SNS Publish API operation.,Activate AWS Security Hub scans for the ECR repository. Create a custom action in Security Hub for ﬁndings. Deﬁne an Amazon EventBridge rule for the custom action. Conﬁgure the EventBridge rule to redirect the ﬁndings to a Slack channel.,,,A,0,1,,,,0,0,,,,,3.4,Container & Serverless Security,,223,SCS-C02,AWS Certified Security - Specialty,,"ECR enhanced scanning is powered by Amazon Inspector, which natively emits findings to EventBridge; routing those findings to an SNS topic integrated with AWS Chatbot for Slack provides immediate notifications with minimal setup and maintenance. Option A uses fully managed services and event-driven integration, whereas B and C add unnecessary custom code/compute and D is incorrect because Security Hub does not perform ECR scanning."
4.2,S3 Encryption & Access,A company wants to store all objects that contain sensitive data in an Amazon S3 bucket. The company will use server-side encryption to encrypt the S3 bucket. The company’s operations team manages access to the company’s S3 buckets. The company’s security team manages access to encryption keys. The company wants to separate the duties of the two teams to ensure that conﬁguration errors by only one of these teams will not compromise the data by granting unauthorized access to plaintext data. Which solution will meet this requirement?,Ensure that the operations team conﬁgures default bucket encryption on the S3 bucket to use server-side encryption with Amazon S3 managed encryption keys (SSE-S3). Ensure that the security team creates an IAM policy that controls access to use the encryption keys.,Ensure that the operations team creates a bucket policy that requires requests to use server-side encryption with AWS KMS keys (SSE- KMS) that are customer managed. Ensure that the security team creates a key policy that controls access to the encryption keys.,Ensure that the operations team creates a bucket policy that requires requests to use server-side encryption with Amazon S3 managed keys (SSE-S3). Ensure that the security team creates an IAM policy that controls access to the encryption keys.,Ensure that the operations team creates a bucket policy that requires requests to use server-side encryption with customer-provided encryption keys (SSE-C). Ensure that the security team stores the customer-provided keys in AWS Key Management Service (AWS KMS). Ensure that the security team creates a key policy that controls access to the encryption keys.,,,B,0,1,,,,0,0,,,,,5.3,Key Management,,255,SCS-C02,AWS Certified Security - Specialty,,"Requiring SSE-KMS with a customer managed CMK and enforcing it via an S3 bucket policy ensures that all object PUTs are encrypted, while the KMS key policy (managed by the security team) controls who can use the key to decrypt. This creates true separation of duties: operations manage bucket access, security manages key usage, and both permissions are required to access plaintext. SSE-S3 lacks separate key control, and SSE-C cannot be centrally managed in KMS as described."
4.2,S3 Encryption & Access,"A company is planning to migrate its applications to AWS in a single AWS Region. The company’s applications will use a combination of Amazon EC2 instances, Elastic Load Balancing (ELB) load balancers, and Amazon S3 buckets. The company wants to complete the migration as quickly as possible. All the applications must meet the following requirements: • Data must be encrypted at rest. • Data must be encrypted in transit. • Endpoints must be monitored for anomalous network traﬃc. Which combination of steps should a security engineer take to meet these requirements with the LEAST effort? (Choose three.)",Install the Amazon Inspector agent on EC2 instances by using AWS Systems Manager Automation.,Enable Amazon GuardDuty in all AWS accounts.,Create VPC endpoints for Amazon EC2 and Amazon S3. Update VPC route tables to use only the secure VPC endpoints.,Conﬁgure AWS Certiﬁcate Manager (ACM). Conﬁgure the load balancers to use certiﬁcates from ACM.,Use AWS Key Management Service (AWS KMS) for key management. Create an S3 bucket policy to deny any PutObject command with a condition for x-amz-meta-side-encryption.,Use AWS Key Management Service (AWS KMS) for key management. Create an S3 bucket policy to deny any PutObject command with a condition for x-amz-server-side-encryption.,"B, D, F",1,1,,,,0,0,,,,,5.1,Encryption at Rest,,259,SCS-C02,AWS Certified Security - Specialty,,"GuardDuty provides managed anomaly detection for VPC, DNS, and CloudTrail activity, satisfying the endpoint monitoring requirement with minimal setup. ACM certificates on the load balancers enable TLS for encryption in transit without managing keys manually, and enforcing SSE-S3 via an S3 bucket policy that requires x-amz-server-side-encryption with KMS ensures data is encrypted at rest (Option E’s header is incorrect)."
4.2,S3 Encryption & Access,A company stores sensitive data in an Amazon S3 bucket. The company encrypts the data at rest by using server-side encryption with Amazon S3 managed keys (SSE-S3). A security engineer must prevent any modiﬁcations to the data in the S3 bucket. Which solution will meet this requirement?,Conﬁgure S3 bucket policies to deny DELETE and PUT object permissions.,Conﬁgure S3 Object Lock in compliance mode with S3 bucket versioning enabled.,Change the encryption on the S3 bucket to use AWS Key Management Service (AWS KMS) customer managed keys.,Conﬁgure the S3 bucket with multi-factor authentication (MFA) delete protection.,,,B,0,1,,,,0,0,,,,,5.4,Data Classification & Access Control,,265,SCS-C02,AWS Certified Security - Specialty,,"S3 Object Lock in compliance mode enforces WORM immutability, preventing overwrites and deletions for the retention period—even the root account cannot bypass it. It requires bucket versioning. Bucket policies can be changed, KMS settings don’t control mutability, and MFA Delete only partially protects deletions and doesn’t prevent writes."
4.2,S3 Encryption & Access,A company has an organization in AWS Organizations. The organization consists of multiple OUs. The company must prevent IAM principals from outside the organization from accessing the organization’s Amazon S3 buckets. The solution must not affect the existing access that the OUs have to the S3 buckets. Which solution will meet these requirements?,Conﬁgure S3 Block Public Access for all S3 buckets.,Conﬁgure S3 Block Public Access for all AWS accounts.,Deploy an SCP that includes the “aws:ResourceOrgPaths”: “${aws:PrincipalOrgPaths}” condition.,"Deploy an SCP that includes the “aws:ResourceOrgID”: “${aws:PrincipalOrgID}"" condition.",,,D,0,1,,,,0,0,,,,,4.1,Authentication & Authorization,,268,SCS-C02,AWS Certified Security - Specialty,,"An SCP that requires aws:ResourceOrgID to equal ${aws:PrincipalOrgID} ensures only principals from the same AWS Organization can access the S3 buckets, preserving existing access for accounts/OUs within the org. S3 Block Public Access only blocks public/anonymous access, and OrgPaths matching is unnecessary and could break valid cross-OU access."
4.2,S3 Encryption & Access,A company needs to log object-level activity in its Amazon S3 buckets. The company also needs to validate the integrity of the log ﬁle by using a digital signature. Which solution will meet these requirements?,Create an AWS CloudTrail trail with log ﬁle validation enabled. Enable data events. Specify Amazon S3 as the data event type.,Create a new S3 bucket for S3 server access logs. Conﬁgure the existing S3 buckets to send their S3 server access logs to the new S3 bucket.,Create an Amazon CloudWatch Logs log group. Conﬁgure the existing S3 buckets to send their S3 server access logs to the log group.,Create a new S3 bucket for S3 server access logs with log ﬁle validation enabled. Enable data events. Specify Amazon S3 as the data event type.,,,A,0,1,,,,0,0,,,,,2.1,Logging Configuration,,273,SCS-C02,AWS Certified Security - Specialty,,"CloudTrail data events capture object-level S3 API activity (e.g., GetObject, PutObject, DeleteObject), satisfying the need for object-level logging. CloudTrail also supports log file integrity validation via signed digest files. S3 server access logs (options B/D) don’t provide digital signature validation, and CloudWatch Logs (C) isn’t used for S3 server access logs."
4.2,S3 Encryption & Access,"A company has an application that needs to read objects from an Amazon S3 bucket. The company conﬁgures an IAM policy and attaches the policy to an IAM role that the application uses. When the application tries to read objects from the S3 bucket, the application receives AccessDenied errors. A security engineer must resolve this problem without decreasing the security of the S3 bucket or the application. Which solution will meet these requirements?",Attach a resource policy to the S3 bucket to grant read access to the role.,Launch a new deployment of the application in a different AWS Region. Attach the role to the application.,Review the IAM policy by using AWS Identity and Access Management Access Analyzer to ensure that the policy grants the right permissions. Validate that the application is assuming the role correctly.,Ensure that the S3 Block Public Access feature is disabled on the S3 bucket. Review AWS CloudTrail logs to validate that the application is assuming the role correctly.,,,C,0,1,,,,0,0,,,,,4.1,Authentication & Authorization,,305,SCS-C02,AWS Certified Security - Specialty,,"AccessDenied errors are typically due to missing or mis-scoped IAM permissions or the application not correctly assuming the role. Using IAM Access Analyzer to verify effective permissions and validating role assumption fixes the issue without broadening bucket access (A), changing Regions (B), or weakening security by disabling Block Public Access (D)."
4.3,EBS & EFS Encryption,"A company has an organization in AWS Organizations. The company wants to use AWS CloudFormation StackSets in the organization to deploy various AWS design patterns into environments. These patterns consist of Amazon EC2 instances, Elastic Load Balancing (ELB) load balancers, Amazon RDS databases, and Amazon Elastic Kubernetes Service (Amazon EKS) clusters or Amazon Elastic Container Service (Amazon ECS) clusters. Currently, the company’s developers can create their own CloudFormation stacks to increase the overall speed of delivery. A centralized CI/CD pipeline in a shared services AWS account deploys each CloudFormation stack. The company's security team has already provided requirements for each service in accordance with internal standards. If there are any resources that do not comply with the internal standards, the security team must receive notiﬁcation to take appropriate action. The security team must implement a notiﬁcation solution that gives developers the ability to maintain the same overall delivery speed that they currently have. Which solution will meet these requirements in the MOST operationally eﬃcient way?",Create an Amazon Simple Notiﬁcation Service (Amazon SNS) topic. Subscribe the security team's email addresses to the SNS topic. Create a custom AWS Lambda function that will run the aws cloudformation validate-template AWS CLI command on all CloudFormation templates before the build stage in the CI/CD pipeline. Conﬁgure the CI/CD pipeline to publish a notiﬁcation to the SNS topic if any issues are found.,"Create an Amazon Simple Notiﬁcation Service (Amazon SNS) topic. Subscribe the security team's email addresses to the SNS topic. Create custom rules in CloudFormation Guard for each resource conﬁguration. In the CI/CD pipeline, before the build stage, conﬁgure a Docker image to run the cfn-guard command on the CloudFormation template. Conﬁgure the CI/CD pipeline to publish a notiﬁcation to the SNS topic if any issues are found.",Create an Amazon Simple Notiﬁcation Service (Amazon SNS) topic and an Amazon Simple Queue Service (Amazon SQS) queue. Subscribe the security team's email addresses to the SNS topic. Create an Amazon S3 bucket in the shared services AWS account. Include an event notiﬁcation to publish to the SQS queue when new objects are added to the S3 bucket. Require the developers to put their CloudFormation templates in the S3 bucket. Launch EC2 instances that automatically scale based on the SQS queue depth. Conﬁgure the EC2 instances to use CloudFormation Guard to scan the templates and deploy the templates if there are no issues. Conﬁgure the CI/CD pipeline to publish a notiﬁcation to the SNS topic if any issues are found.,"Create a centralized CloudFormation stack set that includes a standard set of resources that the developers can deploy in each AWS account. Conﬁgure each CloudFormation template to meet the security requirements. For any new resources or conﬁgurations, update the CloudFormation template and send the template to the security team for review. When the review is completed, add the new CloudFormation stack to the repository for the developers to use.",,,B,0,0,1,0,0,0,3,,,,,6.4,Policy Automation,,4,SCS-C02,AWS Certified Security - Specialty,,"CloudFormation Guard lets you codify the security team’s requirements as policy-as-code and automatically validate templates in the CI/CD pipeline before deployment, preserving developer velocity while notifying via SNS on violations. Option A only checks template syntax, C adds heavy operational overhead, and D introduces manual review bottlenecks that slow delivery."
4.3,EBS & EFS Encryption,"A company has several workloads running on AWS. Employees are required to authenticate using on-premises ADFS and SSO to access the AWS Management Console. Developers migrated an existing legacy web application to an Amazon EC2 instance. Employees need to access this application from anywhere on the internet, but currently, there is no authentication system built into the application. How should the security engineer implement employee-only access to this system without changing the application?",Place the application behind an Application Load Balancer (ALB). Use Amazon Cognito as authentication for the ALB. Deﬁne a SAML- based Amazon Cognito user pool and connect it to ADFS.,"Implement AWS IAM Identity Center (AWS Single Sign-On) in the management account and link it to ADFS as an identity provider. Deﬁne the EC2 instance as a managed resource, then apply an IAM policy on the resource.","Deﬁne an Amazon Cognito identity pool, then install the connector on the Active Directory server. Use the Amazon Cognito SDK on the application instance to authenticate the employees using their Active Directory user names and passwords.",Create an AWS Lambda custom authorizer as the authenticator for a reverse proxy on Amazon EC2. Ensure the security group on Amazon EC2 only allows access from the Lambda function.,,,A,0,1,,,,0,0,,,,,4.1,Authentication & Authorization,,54,SCS-C02,AWS Certified Security - Specialty,,"Placing the app behind an ALB and enabling Amazon Cognito authentication provides federated SSO via ADFS (through a SAML federation to a Cognito user pool) without modifying the application. The ALB performs authentication and only forwards authenticated traffic to the EC2 instance. Other options either require app changes, misuse services (e.g., IAM Identity Center for EC2 app access), or are not applicable (Lambda authorizer for API Gateway)."
4.3,EBS & EFS Encryption,A company wants to create a log analytics solution for logs generated from its on-premises devices. The logs are collected from the devices onto a server on premises. The company wants to use AWS services to perform near real-time log analysis. The company also wants to store these logs for 365 days for pattern matching and substring search capabilities later. Which solution will meet these requirements with the LEAST development overhead?,Install Amazon Kinesis Agent on the on-premises server to send the logs to Amazon DynamoDB. Conﬁgure an AWS Lambda trigger on DynamoDB streams to perform near real-time log analysis. Export the DynamoDB data to Amazon S3 periodically. Run Amazon Athena queries for pattern matching and substring search. Set up S3 Lifecycle policies to delete the log data after 365 days.,Install Amazon Managed Streaming for Apache Kafka (Amazon MSK) on the on-premises server. Create an MSK cluster to collect the streaming data and analyze the data in real time. Set the data retention period to 365 days to store the logs persistently for pattern matching and substring search.,Install Amazon Kinesis Agent on the on-premises server to send the logs to Amazon Kinesis Data Firehose. Conﬁgure Amazon Managed Service for Apache Flink (previously known as Amazon Kinesis Data Analytics) as the destination for real-time processing. Store the logs in Amazon OpenSearch Service for pattern matching and substring search. Conﬁgure an OpenSearch Service Index State Management (ISM) policy to delete the data after 365 days.,Use Amazon API Gateway and AWS Lambda to write the logs from the on-premises server to Amazon DynamoDB. Conﬁgure a Lambda trigger on DynamoDB streams to perform near real-time log analysis. Run Amazon Athena federated queries on DynamoDB data for pattern matching and substring search. Set up TTL to delete data after 365 days.,,,C,0,1,,,,0,0,,,,,2.2,Centralization & Retention,,131,SCS-C02,AWS Certified Security - Specialty,,"Kinesis Agent to Firehose provides a managed, low-overhead way to stream on-prem logs, while Amazon Managed Service for Apache Flink enables near real-time processing. Storing in Amazon OpenSearch Service gives native full-text/substring search and ISM-based 365-day retention. Other choices either misuse DynamoDB for search/analytics or add complexity with MSK."
4.3,EBS & EFS Encryption,"A security engineer receives a notice about suspicious activity from a Linux-based Amazon EC2 instance that uses Amazon Elastic Block Store (Amazon EBS)-based storage. The instance is making connections to known malicious addresses. The instance is in a development account within a VPC that is in the us-east-1 Region. The VPC contains an internet gateway and has a subnet in us-east-1a and us-east-1b. Each subnet is associate with a route table that uses the internet gateway as a default route. Each subnet also uses the default network ACL. The suspicious EC2 instance runs within the us-east-1b subnet. During an initial investigation, a security engineer discovers that the suspicious instance is the only instance that runs in the subnet. Which response will immediately mitigate the attack and help investigate the root cause?",Log in to the suspicious instance and use the netstat command to identify remote connections. Use the IP addresses from these remote connections to create deny rules in the security group of the instance. Install diagnostic tools on the instance for investigation. Update the outbound network ACL for the subnet in us-east-1b to explicitly deny all connections as the ﬁrst rule during the investigation of the instance.,Update the outbound network ACL for the subnet in us-east-1 b to explicitly deny all connections as the ﬁrst rule. Replace the security group with a new security group that allows connections only from a diagnostics security group. Update the outbound network ACL for the us-east-1 b subnet to remove the deny all rule. Launch a new EC2 instance that has diagnostic tools. Assign the new security group to the new EC2 instance. Use the new EC2 instance to investigate the suspicious instance.,Ensure that the Amazon Elastic Block Store (Amazon EBS) volumes that are attached to the suspicious EC2 instance will not delete upon termination. Terminate the instance. Launch a new EC2 instance in us-east-1a that has diagnostic tools. Mount the EBS volumes from the terminated instance for investigation.,Create an AWS WAF web ACL that denies traﬃc to and from the suspicious instance. Attach the AWS WAF web ACL to the instance to mitigate the attack. Log in to the instance and install diagnostic tools to investigate the instance.,,,B,0,1,,,,0,0,,,,,1.3,Containment & Eradication,,143,SCS-C02,AWS Certified Security - Specialty,,"Option B immediately stops the malicious connections by denying all outbound traffic at the subnet level with a NACL, without needing to access the compromised host. It then re-enables controlled access via restrictive security groups and a clean diagnostics instance for forensically sound investigation. Other options are weaker: A relies on logging into a compromised host and blocking specific IPs, C destroys volatile evidence, and D misuses AWS WAF (not applicable to EC2 instances directly)."
4.3,EBS & EFS Encryption,"A security engineer is designing a cloud architecture to support an application. The application runs on Amazon EC2 instances and processes sensitive information, including credit card numbers. The application will send the credit card numbers to a component that is running in an isolated environment. The component will encrypt, store, and decrypt the numbers. The component then will issue tokens to replace the numbers in other parts of the application. The component of the application that manages the tokenization process will be deployed on a separate set of EC2 instances. Other components of the application must not be able to store or access the credit card numbers. Which solution will meet these requirements?",Use EC2 Dedicated Instances for the tokenization component of the application.,Place the EC2 instances that manage the tokenization process into a partition placement group.,Create a separate VPDeploy new EC2 instances into the separate VPC to support the data tokenization.,Deploy the tokenization code onto AWS Nitro Enclaves that are hosted on EC2 instances.,,,D,0,1,,,,0,0,,,,,5.1,Encryption at Rest,,178,SCS-C02,AWS Certified Security - Specialty,,"AWS Nitro Enclaves provide a hardware-isolated, networkless environment ideal for processing highly sensitive data like credit card numbers, with KMS integration and attestation to control key access. This ensures other application components cannot access or store the raw data. The other options (Dedicated Instances, placement groups, separate VPC) do not provide this level of isolation or protection from the host/application."
4.3,EBS & EFS Encryption,A company runs workloads in the us-east-1 Region. The company has never deployed resources to other AWS Regions and does not have any multi-Region resources. The company needs to replicate its workloads and infrastructure to the us-west-1 Region. A security engineer must implement a solution that uses AWS Secrets Manager to store secrets in both Regions. The solution must use AWS Key Management Service (AWS KMS) to encrypt the secrets. The solution must minimize latency and must be able to work if only one Region is available. The security engineer uses Secrets Manager to create the secrets in us-east-1. What should the security engineer do next to meet the requirements?,Encrypt the secrets in us-east-1 by using an AWS managed KMS key. Replicate the secrets to us-west-1. Encrypt the secrets in us-west-1 by using a new AWS managed KMS key in us-west-1.,Encrypt the secrets in us-east-1 by using an AWS managed KMS key. Conﬁgure resources in us-west-1 to call the Secrets Manager endpoint in us-east-1.,Encrypt the secrets in us-east-1 by using a customer managed KMS key. Conﬁgure resources in us-west-1 to call the Secrets Manager endpoint in us-east-1.,Encrypt the secrets in us-east-1 by using a customer managed KMS key. Replicate the secrets to us-west-1. Encrypt the secrets in us-west- 1 by using the customer managed KMS key from us-east-1.,,,D,0,1,,,,0,0,,,,,5.3,Key Management,,190,SCS-C02,AWS Certified Security - Specialty,,"Replicate the secrets and use a customer managed multi-Region KMS key so the replica secret in us-west-1 is encrypted with the replica of the same key, allowing local reads (lower latency) and continued operation if one Region is down. Calling the us-east-1 endpoint (B/C) adds latency and fails during a regional outage, and AWS managed keys (A) don’t support multi-Region key replication."
4.3,EBS & EFS Encryption,An ecommerce website was down for 1 hour following a DDoS attack. Users were unable to connect to the website during the attack period. The ecommerce company’s security team is worried about future potential attacks and wants to prepare for such events. The company needs to minimize downtime in its response to similar attacks in the future. Which steps would help achieve this? (Choose two.),Enable Amazon GuardDuty to automatically monitor for malicious activity and block unauthorized access.,Subscribe to AWS Shield Advanced and reach out to AWS Support in the event of an attack.,Use VPC Flow Logs to monitor network traﬃc and an AWS Lambda function to automatically block an attacker’s IP using security groups.,"Set up an Amazon EventBridge rule to monitor the AWS CloudTrail events in real time, use AWS Conﬁg rules to audit the conﬁguration, and use AWS Systems Manager for remediation.",Use AWS WAF to create rules to respond to such attacks.,,"B, E",1,0,1,0,0,0,2,,,,,1.1,Preparation & Runbooks,,241,SCS-C02,AWS Certified Security - Specialty,,"Subscribing to AWS Shield Advanced provides purpose-built DDoS detection/mitigation and access to the AWS DDoS Response Team, reducing time to mitigate during attacks. Using AWS WAF enables creation of targeted and rate-based rules to block malicious traffic at the edge, minimizing impact and downtime. Other options focus on monitoring or ad‑hoc remediation and are not designed for rapid, scalable DDoS mitigation."
4.3,EBS & EFS Encryption,"A security engineer has been asked to troubleshoot inbound connectivity to a web server. This single web server is not receiving inbound connections from the internet, whereas all other web servers are functioning properly. The architecture includes network ACLs, security groups, and a virtual security appliance. In addition, the development team has implemented Application Load Balancers (ALBs) to distribute the load across all web servers. It is a requirement that traﬃc between the web servers and the internet ﬂow through the virtual security appliance. The security engineer has veriﬁed the following: 1. The rule set in the security groups is correct. 2. The rule set in the network ACLs is correct. 3. The rule set in the virtual appliance is correct. Which of the following are other valid items to troubleshoot in this scenario? (Choose two.)",Verify that the 0.0.0.0/0 route in the route table for the web server subnet points to a NAT gateway.,Verify which security group is applied to the particular web server’s elastic network interface (ENI).,Verify that the 0.0.0.0/0 route in the route table for the web server subnet points to the virtual security appliance.,Verify the registered targets in the ALB.,Verify that the 0.0.0.0/0 route in the public subnet points to a NAT gateway.,,"B, D",1,1,,,,0,0,,,,,3.1,Network Architecture Security,,253,SCS-C02,AWS Certified Security - Specialty,,"Even if the rules are correct, the specific ENI might be attached to the wrong security group, blocking traffic to that one server. Additionally, if the instance is not registered or is unhealthy in the ALB target group, the ALB will not forward traffic to it. NAT gateway routes are for outbound traffic and the subnet routing is likely correct since other web servers are working."
4.3,EBS & EFS Encryption,A company must create annual snapshots of Amazon Elastic Block Store (Amazon EBS) volumes. The company must retain the snapshots for 10 years. The company will use AWS Key Management Service (AWS KMS) to encrypt the EBS volumes and snapshots. The encryption keys must be rotated automatically every year. Snapshots that were created in previous years must be readable after rotation of the encryption keys. Which type of KMS keys should the company use for encryption to meet these requirements?,Asymmetric AWS managed KMS keys with key material created by AWS KMS,Symmetric customer managed KMS keys with key material created by AWS KMS,Symmetric customer managed KMS keys with custom imported key material,Asymmetric AWS managed KMS keys with custom imported key material,,,B,0,1,,,,0,0,,,,,5.3,Key Management,,291,SCS-C02,AWS Certified Security - Specialty,,"Amazon EBS encryption requires symmetric KMS keys, and customer managed keys created by AWS KMS support automatic annual rotation. KMS retains previous key versions, ensuring older snapshots remain decryptable after rotation. Imported key material and asymmetric keys do not meet these requirements."
4.3,EBS & EFS Encryption,A company’s network security policy requires encryption for all data in transit. The company must encrypt data that is sent between Amazon EC2 instances and Amazon Elastic Block Store (Amazon EBS) volumes. Which solution will meet this requirement?,Conﬁgure Amazon EC2 to enable encryption in the EC2 network interface properties.,Conﬁgure Amazon EBS to enable volume encryption with AWS Key Management Service (AWS KMS) for data at rest.,Conﬁgure Amazon EBS to enable TLS encryption in the volume conﬁguration properties.,Conﬁgure Amazon EC2 to enable TLS encryption with certiﬁcates that are stored in AWS Certiﬁcate Manager (ACM).,,,B,0,1,,,,0,0,,,,,5.2,Encryption in Transit,,303,SCS-C02,AWS Certified Security - Specialty,,"Enabling EBS encryption with AWS KMS encrypts data at rest and automatically encrypts all data in transit between the EC2 instance and the EBS volume. There is no TLS setting for EBS volumes, and ACM/TLS or EC2 NIC properties do not apply to EBS I/O traffic."
4.4,DynamoDB & RDS Encryption,A company runs workloads in the us-east-1 Region. The company has never deployed resources to other AWS Regions and does not have any multi-Region resources. The company needs to replicate its workloads and infrastructure to the us-west-1 Region. A security engineer must implement a solution that uses AWS Secrets Manager to store secrets in both Regions. The solution must use AWS Key Management Service (AWS KMS) to encrypt the secrets. The solution must minimize latency and must be able to work if only one Region is available. The security engineer uses Secrets Manager to create the secrets in us-east-1. What should the security engineer do next to meet the requirements?,Encrypt the secrets in us-east-1 by using an AWS managed KMS key. Replicate the secrets to us-west-1. Encrypt the secrets in us-west-1 by using a new AWS managed KMS key in us-west-1.,Encrypt the secrets in us-east-1 by using an AWS managed KMS key. Conﬁgure resources in us-west-1 to call the Secrets Manager endpoint in us-east-1.,Encrypt the secrets in us-east-1 by using a customer managed KMS key. Conﬁgure resources in us-west-1 to call the Secrets Manager endpoint in us-east-1.,Encrypt the secrets in us-east-1 by using a customer managed KMS key. Replicate the secrets to us-west-1. Encrypt the secrets in us-west- 1 by using the customer managed KMS key from us-east-1.,,,D,0,1,,,,0,0,,,,,5.3,Key Management,,190,SCS-C02,AWS Certified Security - Specialty,,"Use Secrets Manager replication so each Region has a local copy of the secret, minimizing latency and allowing operation if one Region is down. This requires customer managed multi-Region KMS keys (primary in us-east-1 with a replica in us-west-1) to encrypt the secrets in each Region. Cross-Region access (B/C) increases latency and fails on Regional outage, and AWS managed keys (A) don’t support multi-Region replication control."
4.4,DynamoDB & RDS Encryption,"A security engineer is implementing a solution to allow users to seamlessly encrypt Amazon S3 objects without having to touch the keys directly. The solution must be highly scalable without requiring continual management. Additionally, the organization must be able to immediately delete the encryption keys. Which solution meets these requirements?",Use AWS KMS with AWS managed keys and the ScheduleKeyDeletion API with a PendingWindowInDays set to 0 to remove the keys if necessary.,Use KMS with AWS imported key material and then use the DeleteImportedKeyMaterial API to remove the key material if necessary.,Use AWS CloudHSM to store the keys and then use the CloudHSM API or the PKCS11 library to delete the keys if necessary.,Use the Systems Manager Parameter Store to store the keys and then use the service API operations to delete the keys if necessary.,,,B,0,1,,,,0,0,,,,,5.3,Key Management,,184,SCS-C02,AWS Certified Security - Specialty,,"KMS integrates natively with S3 for seamless, highly scalable encryption, and using customer-managed keys with imported key material allows immediate revocation by calling DeleteImportedKeyMaterial. AWS managed keys cannot be deleted immediately due to the mandatory waiting period, and CloudHSM or Parameter Store would add management overhead and lack the same seamless S3 integration."
4.4,DynamoDB & RDS Encryption,A company has an AWS Key Management Service (AWS KMS) customer managed key with imported key material. Company policy requires all encryption keys to be rotated every year. What should a security engineer do to meet this requirement for this customer managed key?,Enable automatic key rotation annually for the existing customer managed key.,Use the AWS CLI to create an AWS Lambda function to rotate the existing customer managed key annually.,Import new key material to the existing customer managed key. Manually rotate the key.,Create a new customer managed key. Import new key material to the new key. Point the key alias to the new key.,,,D,0,1,,,,0,0,,,,,5.3,Key Management,,205,SCS-C02,AWS Certified Security - Specialty,,"AWS KMS does not support automatic key rotation for CMKs with imported key material, and you cannot change the key material of an existing CMK (you can only reimport the same material). To rotate, you must create a new CMK, import new key material into it, and then repoint the alias to the new key."
4.4,DynamoDB & RDS Encryption,"A company needs to use HTTPS when connecting to its web applications to meet compliance requirements. These web applications run in Amazon VPC on Amazon EC2 instances behind an Application Load Balancer (ALB). A security engineer wants to ensure that the load balancer will only accept connections over port 443, even if the ALB is mistakenly conﬁgured with an HTTP listener. Which conﬁguration steps should the security engineer take to accomplish this task?",Create a security group with a rule that denies inbound connections from 0.0.0.0/0 on port 80. Attach this security group to the ALB to overwrite more permissive rules from the ALB’s default security group.,Create a network ACL that denies inbound connections from 0.0.0.0/0 on port 80. Associate the network ACL with the VPC’s internet gateway.,Create a network ACL that allows outbound connections to the VPC IP range on port 443 only. Associate the network ACL with the VPC’s internet gateway.,Create a security group with a single inbound rule that allows connections from 0.0.0.0/0 on port 443. Ensure this security group is the only one associated with the ALB.,,,D,0,1,,,,0,0,,,,,3.1,Network Architecture Security,,247,SCS-C02,AWS Certified Security - Specialty,,"Attach a security group to the ALB that only allows inbound traffic on port 443; security groups are stateful allow-lists, so the absence of a rule for port 80 automatically blocks HTTP even if an HTTP listener exists. Options A–C are incorrect because security groups can’t deny traffic, and network ACLs are applied to subnets (not internet gateways) and aren’t needed here."
4.4,DynamoDB & RDS Encryption,A company hosts a web-based application that captures and stores sensitive data in an Amazon DynamoDB table. The company needs to implement a solution that provides end-to-end data protection and the ability to detect unauthorized data changes. Which solution will meet these requirements?,Use an AWS Key Management Service (AWS KMS) customer managed key. Encrypt the data at rest.,Use AWS Private Certiﬁcate Authority. Encrypt the data in transit.,Use the DynamoDB Encryption Client. Use client-side encryption. Sign the table items.,Use the AWS Encryption SDK. Use client-side encryption. Sign the table items.,,,C,0,1,,,,0,0,,,,,5.1,Encryption at Rest,,297,SCS-C02,AWS Certified Security - Specialty,,"The DynamoDB Encryption Client provides client-side encryption and cryptographic signing of items, delivering end-to-end protection and integrity verification to detect unauthorized data changes. Options A and B only protect data at rest or in transit, not end-to-end with integrity checks. While the AWS Encryption SDK can encrypt and sign, the DynamoDB Encryption Client is purpose-built for DynamoDB item-level encryption and signing."
4.5,Certificate Manager & ACM PCA,A company is running an application on Amazon EC2 instances in an Auto Scaling group. The application stores logs locally. A security engineer noticed that logs were lost after a scale-in event. The security engineer needs to recommend a solution to ensure the durability and availability of log data. All logs must be kept for a minimum of 1 year for auditing purposes. What should the security engineer recommend?,"Within the Auto Scaling lifecycle, add a hook to create and attach an Amazon Elastic Block Store (Amazon EBS) log volume each time an EC2 instance is created. When the instance is terminated, the EBS volume can be reattached to another instance for log review.",Create an Amazon Elastic File System (Amazon EFS) ﬁle system and add a command in the user data section of the Auto Scaling launch template to mount the EFS ﬁle system during EC2 instance creation. Conﬁgure a process on the instance to copy the logs once a day from an instance Amazon Elastic Block Store (Amazon EBS) volume to a directory in the EFS ﬁle system.,Add an Amazon CloudWatch agent into the AMI used in the Auto Scaling group. Conﬁgure the CloudWatch agent to send the logs to Amazon CloudWatch Logs for review.,"Within the Auto Scaling lifecycle, add a lifecycle hook at the terminating state transition and alert the engineering team by using a lifecycle notiﬁcation to Amazon Simple Notiﬁcation Service (Amazon SNS). Conﬁgure the hook to remain in the Terminating:Wait state for 1 hour to allow manual review of the security logs prior to instance termination.",,,C,0,0,0,1,0,2,0,,,,,2.1,Logging Configuration,,195,SCS-C02,AWS Certified Security - Specialty,,"Sending logs to Amazon CloudWatch Logs via the CloudWatch agent decouples log storage from the EC2 instance lifecycle, ensuring durability and availability even after scale-in events. CloudWatch Logs supports centralized access and configurable retention (e.g., 1 year) to meet auditing requirements. Alternatives either remain tied to instance storage, risk data loss, or require manual processes and are not as reliable or scalable."
5.1,VPC Security,"A company hosts a web application on an Apache web server. The application runs on Amazon EC2 instances that are in an Auto Scaling group. The company conﬁgured the EC2 instances to send the Apache web server logs to an Amazon CloudWatch Logs group that the company has conﬁgured to expire after 1 year. Recently, the company discovered in the Apache web server logs that a speciﬁc IP address is sending suspicious requests to the web application. A security engineer wants to analyze the past week of Apache web server logs to determine how many requests that the IP address sent and the corresponding URLs that the IP address requested. What should the security engineer do to meet these requirements with the LEAST effort?",Export the CloudWatch Logs group data to Amazon S3. Use Amazon Macie to query the logs for the speciﬁc IP address and the requested URL.,Conﬁgure a CloudWatch Logs subscription to stream the log group to an Amazon OpenSearch Service cluster. Use OpenSearch Service to analyze the logs for the speciﬁc IP address and the requested URLs.,Use CloudWatch Logs Insights and a custom query syntax to analyze the CloudWatch logs for the speciﬁc IP address and the requested URLs.,Export the CloudWatch Logs group data to Amazon S3. Use AWS Glue to crawl the S3 bucket for only the log entries that contain the speciﬁc IP address. Use AWS Glue to view the results.,,,C,0,1,,,,0,0,,,,,2.1,Logging Configuration,,24,SCS-C02,AWS Certified Security - Specialty,,"CloudWatch Logs Insights lets you run ad-hoc queries directly against the existing log group, making it the fastest, least-effort way to filter by IP and list requested URLs. Options A, B, and D require exporting logs or provisioning additional services (Macie, OpenSearch, Glue), which adds unnecessary setup and complexity for a one-time analysis."
5.1,VPC Security,"A company has thousands of AWS Lambda functions. While reviewing the Lambda functions, a security engineer discovers that sensitive information is being stored in environment variables and is viewable as plaintext in the Lambda console. The values of the sensitive information are only a few characters long. What is the MOST cost-effective way to address this security issue?",Set up IAM policies from the Lambda console to hide access to the environment variables.,Use AWS Step Functions to store the environment variables. Access the environment variables at runtime. Use IAM permissions to restrict access to the environment variables to only the Lambda functions that require access.,"Store the environment variables in AWS Secrets Manager, and access them at runtime. Use IAM permissions to restrict access to the secrets to only the Lambda functions that require access.","Store the environment variables in AWS Systems Manager Parameter Store as secure string parameters, and access them at runtime. Use IAM permissions to restrict access to the parameters to only the Lambda functions that require access.",,,D,0,1,,,,0,0,,,,,5.1,Encryption at Rest,,42,SCS-C02,AWS Certified Security - Specialty,,"Store sensitive values in AWS Systems Manager Parameter Store as SecureString parameters encrypted with KMS, then retrieve them at runtime using IAM to restrict access. This removes plaintext exposure in the Lambda console and is more cost-effective than Secrets Manager, which is better suited for secret rotation and higher-cost use cases."
5.1,VPC Security,A company uses Amazon GuardDuty. The company's security team wants all High severity ﬁndings to automatically generate a ticket in a third-party ticketing system through email integration. Which solution will meet this requirement?,Create a veriﬁed identity for the third-party ticketing email system in Amazon Simple Email Service (Amazon SES). Create an Amazon EventBridge rule that includes an event pattern that matches High severity GuardDuty ﬁndings. Specify the SES identity as the target for the EventBridge rule.,Create an Amazon Simple Notiﬁcation Service (Amazon SNS) topic. Subscribe the third-party ticketing email system to the SNS topic. Create an Amazon EventBridge rule that includes an event pattern that matches High severity GuardDuty ﬁndings. Specify the SNS topic as the target for the EventBridge rule.,Use the GuardDuty CreateFilter API operation to build a ﬁlter in GuardDuty to monitor for High severity ﬁndings. Export the results of the ﬁlter to an Amazon Simple Notiﬁcation Service (Amazon SNS) topic. Subscribe the third-party ticketing email system to the SNS topic.,Use the GuardDuty CreateFilter API operation to build a ﬁlter in GuardDuty to monitor for High severity ﬁndings. Create an Amazon Simple Notiﬁcation Service (Amazon SNS) topic. Subscribe the third-party ticketing email system to the SNS topic. Create an Amazon EventBridge rule that includes an event pattern that matches GuardDuty ﬁndings that are selected by the ﬁlter. Specify the SNS topic as the target for the EventBridge rule.,,,B,0,1,,,,0,0,,,,,2.4,Alerting & Remediation,,74,SCS-C02,AWS Certified Security - Specialty,,"EventBridge can match High severity GuardDuty findings and send them to an SNS topic, where the third‑party ticketing system can be subscribed via email to create tickets automatically. SES is not a valid EventBridge target, and GuardDuty filters do not directly publish to SNS, making B the simplest supported integration."
5.1,VPC Security,"A company has deployed servers on Amazon EC2 instances in a VPC. External vendors access these servers over the internet. Recently, the company deployed a new application on EC2 instances in a new CIDR range. The company needs to make the application available to the vendors. A security engineer veriﬁed that the associated security groups and network ACLs are allowing the required ports in the inbound direction. However, the vendors cannot connect to the application. Which solution will provide the vendors access to the application?",Modify the security group that is associated with the EC2 instances to have the same outbound rules as inbound rules.,Modify the network ACL that is associated with the CIDR range to allow outbound traﬃc to ephemeral ports.,Modify the inbound rules on the internet gateway to allow the required ports.,Modify the network ACL that is associated with the CIDR range to have the same outbound rules as inbound rules.,,,B,0,1,,,,0,0,,,,,3.1,Network Architecture Security,,92,SCS-C02,AWS Certified Security - Specialty,,"Network ACLs are stateless, so even if inbound ports are allowed, the return traffic must be explicitly permitted. Application responses use ephemeral ports; therefore, the subnet’s NACL must allow outbound traffic to the ephemeral port range. Security groups are stateful and internet gateways have no rules, so A, C, and D won’t fix the issue."
5.1,VPC Security,A company uses SAML federation with AWS Identity and Access Management (IAM) to provide internal users with SSO for their AWS accounts. The company's identity provider certiﬁcate was rotated as part of its normal lifecycle Shortly after users started receiving the following error when attempting to log in: “Error: Response Signature Invalid (Service: AWSSecurityTokenService; Status Code: 400; Error Code: InvalidIdentityToken)” A security engineer needs to address the immediate issue and ensure that it will not occur again. Which combination of steps should the security engineer take to accomplish this? (Choose two.),Download a new copy of the SAML metadata ﬁle from the identity provider. Create a new IAM identity provider entity. Upload the new metadata ﬁle to the new IAM identity provider entity.,"During the next certiﬁcate rotation period and before the current certiﬁcate expires, add a new certiﬁcate as the secondary to the identity provider. Generate a new metadata ﬁle and upload it to the IAM identity provider entity. Perform automated or manual rotation of the certiﬁcate when required.",Download a new copy of the SAML metadata ﬁle from the identity provider. Upload the new metadata to the IAM identity provider entity conﬁgured for the SAML integration in question.,"During the next certiﬁcate rotation period and before the current certiﬁcate expires, add a new certiﬁcate as the secondary to the identity provider. Generate a new copy of the metadata ﬁle and create a new IAM identity provider entity. Upload the metadata ﬁle to the new IAM identity provider entity. Perform automated or manual rotation of the certiﬁcate when required.",Download a new copy of the SAML metadata ﬁle from the identity provider. Create a new IAM identity provider entity. Upload the new metadata ﬁle to the new IAM identity provider entity. Update the identity provider conﬁgurations to pass a new IAM identity provider entity name in the SAML assertion.,,"B, C",1,1,,,,0,0,,,,,4.1,Authentication & Authorization,,121,SCS-C02,AWS Certified Security - Specialty,,"The error indicates IAM is validating SAML responses with an outdated IdP signing certificate; immediately fix by uploading the refreshed IdP metadata (with the new cert) to the existing IAM SAML provider (C). To prevent future outages, adopt a dual-certificate rotation: add the new cert as secondary on the IdP, regenerate and upload metadata so IAM trusts both old and new certs, then rotate when ready (B). Creating new IAM identity providers is unnecessary and adds complexity."
5.1,VPC Security,"An Amazon EC2 Auto Scaling group launches Amazon Linux EC2 instances and installs the Amazon CloudWatch agent to publish logs to Amazon CloudWatch Logs. The EC2 instances launch with an IAM role that has an IAM policy attached. The policy provides access to publish custom metrics to CloudWatch. The EC2 instances run in a private subnet inside a VPC The VPC provides access to the internet for private subnets through a NAT gateway. A security engineer notices that no logs are being published to CloudWatch Logs for the EC2 instances that the Auto Scaling group launches. The security engineer validates that the CloudWatch Logs agent is running and is conﬁgured properly on the EC2 instances. In addition, the security engineer validates that network communications are working properly to AWS services. What can the security engineer do to ensure that the logs are published to CloudWatch Logs?",Conﬁgure the IAM policy in use by the IAM role to have access to the required cloudwatch: API actions that will publish logs.,Adjust the Amazon EC2 Auto Scaling service-linked role to have permissions to write to CloudWatch Logs.,Conﬁgure the IAM policy in use by the IAM role to have access to the required AWS logs: API actions that will publish logs.,Add an interface VPC endpoint to provide a route to CloudWatch Logs.,,,A,0,1,,,,0,0,,,,,2.1,Logging Configuration,,125,SCS-C02,AWS Certified Security - Specialty,,"Since the agent is running and network connectivity is verified, the most likely cause is insufficient IAM permissions on the instance role. Grant the instance role the required CloudWatch/CloudWatch Logs permissions (e.g., create log groups/streams and put log events) so the agent can publish logs to CloudWatch Logs."
5.1,VPC Security,"An AWS Lambda function was misused to alter data, and a security engineer must identify who invoked the function and what output was produced. The engineer cannot ﬁnd any logs created by the Lambda function in Amazon CloudWatch Logs. Which of the following explains why the logs are not available?",The execution role for the Lambda function did not grant permissions to write log data to CloudWatch Logs.,"The Lambda function was invoked by using Amazon API Gateway, so the logs are not stored in CloudWatch Logs.",The execution role for the Lambda function did not grant permissions to write to the Amazon S3 bucket where CloudWatch Logs stores the logs.,The version of the Lambda function that was invoked was not current.,,,A,0,1,,,,0,0,,,,,2.1,Logging Configuration,,155,SCS-C02,AWS Certified Security - Specialty,,"Lambda writes to CloudWatch Logs using the function’s execution role; without permissions like logs:CreateLogGroup, logs:CreateLogStream, and logs:PutLogEvents, no logs are created or visible. API Gateway invocation, S3 storage, or the function version do not prevent CloudWatch logging."
5.1,VPC Security,A company runs a global ecommerce website that is hosted on AWS. The company uses Amazon CloudFront to serve content to its user base. The company wants to block inbound traﬃc from a speciﬁc set of countries to comply with recent data regulation policies. Which solution will meet these requirements MOST cost-effectively?,Create an AWS WAF web ACL with an IP match condition to deny the countries' IP ranges. Associate the web ACL with the CloudFront distribution.,Create an AWS WAF web ACL with a geo match condition to deny the speciﬁc countries. Associate the web ACL with the CloudFront distribution.,Use the geo restriction feature in CloudFront to deny the speciﬁc countries.,Use geolocation headers in CloudFront to deny the speciﬁc countries.,,,C,0,1,,,,0,0,,,,,3.2,Edge & Web Protection,,168,SCS-C02,AWS Certified Security - Specialty,,"CloudFront’s built-in geo restriction lets you deny traffic from specific countries directly at the edge, providing the simplest and most cost-effective way to comply. While AWS WAF geo match could also block countries, it adds additional WAF costs and complexity; IP-based blocking and geolocation headers are either harder to maintain or not intended for outright blocking."
5.1,VPC Security,"A company runs an online game on AWS. When players sign up for the game, their username and password credentials are stored in an Amazon Aurora database. The number of users has grown to hundreds of thousands of players. The number of requests for password resets and login assistance has become a burden for the company's customer service team. The company needs to implement a solution to give players another way to log in to the game. The solution must remove the burden of password resets and login assistance while securely protecting each player's credentials. Which solution will meet these requirements?","When a new player signs up, use an AWS Lambda function to automatically create an IAM access key and a secret access key. Program the Lambda function to store the credentials on the player's device. Create IAM keys for existing players.","Migrate the player credentials from the Aurora database to AWS Secrets Manager. When a new player signs up, create a key-value pair in Secrets Manager for the player’s user ID and password.","Conﬁgure Amazon Cognito user pools to federate access to the game with third-party identity providers (IdPs), such as social IdPs. Migrate the game’s authentication mechanism to Cognito.","Instead of using usernames and passwords for authentication, issue API keys to new and existing players. Create an Amazon API Gateway API to give the game client access to the game’s functionality.",,,C,0,1,,,,0,0,,,,,4.1,Authentication & Authorization,,172,SCS-C02,AWS Certified Security - Specialty,,"Amazon Cognito user pools provide a managed user directory and can federate with social and enterprise identity providers, allowing players to log in with existing identities and offloading password management, resets, and MFA to Cognito/IdPs. This reduces customer support burden while securely managing credentials and scaling to hundreds of thousands of users."
5.1,VPC Security,A company has an application that runs on Amazon EC2 instances behind an Application Load Balancer (ALB). The instances are in an Amazon EC2 Auto Scaling group and are attached to Amazon Elastic Block Store (Amazon EBS) volumes. A security engineer needs to preserve all forensic evidence from one of the instances. Which order of steps should the security engineer use to meet this requirement?,Take an EBS volume snapshot of the instance and store the snapshot in an Amazon S3 bucket. Take a memory snapshot of the instance and store the snapshot in an S3 bucket Detach the instance from the Auto Scaling group. Deregister the instance from the ALB. Stop the instance.,Take a memory snapshot of the instance and store the snapshot in an Amazon S3 bucket. Stop the instance. Take an EBS volume snapshot of the instance and store the snapshot in an S3 bucket. Detach the instance from the Auto Scaling group. Deregister the instance from the ALB.,Detach the instance from the Auto Scaling group. Deregister the instance from the ALB. Take an EBS volume snapshot of the instance and store the snapshot in an Amazon S3 bucket. Take a memory snapshot of the instance and store the snapshot in an S3 bucket. Stop the instance.,Detach the instance from the Auto Scaling group. Deregister the instance from the ALB Stop the instance. Take a memory snapshot of the instance and store the snapshot in an Amazon S3 bucket. Take an EBS volume snapshot of the instance and store the snapshot in an S3 bucket.,,,C,0,1,,,,0,0,,,,,1.2,Detection & Investigation,,197,SCS-C02,AWS Certified Security - Specialty,,"First isolate the instance by detaching it from the Auto Scaling group and deregistering it from the ALB to prevent termination or further changes. Then acquire evidence while it’s still running by taking EBS and memory snapshots, preserving both disk and volatile data. Finally, stop the instance to maintain its state after evidence collection."
5.1,VPC Security,A security engineer discovers that a company’s user passwords have no required minimum length. The company is using the following two identity providers (IdPs): • AWS Identity and Access Management (IAM) federated with on-premises Active Directory • Amazon Cognito user pools that contain the user database for an AWS Cloud application that the company developed Which combination of actions should the security engineer take to implement a required minimum length for the passwords? (Choose two.),Update the password length policy in the IAM conﬁguration.,Update the password length policy in the Cognito conﬁguration.,Update the password length policy in the on-premises Active Directory conﬁguration,Create an SCP in AWS Organizations. Conﬁgure the SCP to enforce a minimum password length for IAM and Cognito.,Create an IAM policy that includes a condition for minimum password length. Enforce the policy for IAM and Cognito.,,"B, C",1,1,,,,0,0,,,,,4.1,Authentication & Authorization,,213,SCS-C02,AWS Certified Security - Specialty,,"Cognito user pools have their own password policy, so you set the minimum length in the Cognito configuration. For federated users via on-premises Active Directory, passwords are managed by AD, so the minimum length must be enforced in AD. IAM password policies and SCPs do not control passwords for Cognito or federated AD users."
5.1,VPC Security,A company is planning to create an organization by using AWS Organizations. The company needs to integrate user management with the company’s external identity provider (IdP). The company also needs to centrally manage access to all of its AWS accounts and applications from the organization’s management account. Which solution will meet these requirements?,Conﬁgure AWS Directory Service with the external IdP. Create IAM policies and associate them with users from the external IdP.,Enable AWS IAM Identity Center and use the external IdP as the identity source. Create permission sets and account assignments by using IAM Identity Center.,Conﬁgure AWS Identity and Access Management (IAM) to use the external IdP as an IdP. Create IAM policies and associate them with users from the external IdP.,Enable Amazon Cognito in the organization’s management account. Create an identity pool and associate it with the external IdP. Create IAM roles and associate them with the identity pool.,,,B,0,1,,,,0,0,,,,,4.2,Cross-Account & Federation,,222,SCS-C02,AWS Certified Security - Specialty,,"IAM Identity Center (formerly AWS SSO) is designed to integrate with an external IdP and centrally manage user access across all AWS Organizations accounts using permission sets and account assignments from the management account. IAM or Directory Service do not provide centralized, multi-account SSO management, and Amazon Cognito is intended for application authentication, not AWS account access management."
5.1,VPC Security,"A company has an application on Amazon EC2 instances that store conﬁdential customer data. The company must restrict access to customer data. A security engineer requires secure access to the instances that host the application. According to company policy, users must not open any inbound ports, maintain bastion hosts, or manage SSH keys for the EC2 instances. The security engineer wants to monitor, store, and access all session activity logs. The logs must be encrypted. Which solution will meet these requirements?",Use AWS Control Tower to connect to the EC2 instances. Conﬁgure Amazon CloudWatch logging for the sessions. Select the upload session logs option and allow only encrypted CloudWatch Logs log groups.,Use AWS Security Hub to connect to the EC2 instances. Conﬁgure Amazon CloudWatch logging for the sessions. Select the upload session logs option and allow only encrypted CloudWatch Logs log groups.,Use AWS Systems Manager Session Manager to connect to the EC2 instances. Conﬁgure Amazon CloudWatch monitoring to record the sessions. Select the store session logs option for the desired CloudWatch Logs log groups.,Use AWS Systems Manager Session Manager to connect to the EC2 instances. Conﬁgure Amazon CloudWatch logging. Select the upload session logs option and allow only encrypted CloudWatch Logs log groups.,,,D,0,1,,,,0,0,,,,,4.1,Authentication & Authorization,,231,SCS-C02,AWS Certified Security - Specialty,,"AWS Systems Manager Session Manager enables secure instance access without opening inbound ports, using bastion hosts, or managing SSH keys. It supports uploading session logs to CloudWatch Logs (and S3) with KMS encryption and allows enforcing the use of encrypted log groups, meeting the monitoring and encryption requirements. Control Tower and Security Hub do not provide interactive access to EC2 instances."
5.1,VPC Security,"A company hired an external consultant who needs to use a laptop to access the company’s VPCs. Speciﬁcally, the consultant needs access to two VPCs that are peered together in the same AWS Region. The company wants to provide the consultant with access to these VPCs without also providing any unnecessary access to other network resources. Which solution will meet these requirements?",Create an AWS Site-to-Site VPN endpoint in the same Region as the VPCs. Conﬁgure access through an appropriate subnet and authorization rule.,Create an AWS account. Use the VPC sharing feature through AWS Resource Access Manager to allow the consultant to access the VPCs.,Create an AWS Client VPN endpoint in the same Region as the VPCs. Conﬁgure access through an appropriate subnet and authorization rule.,Create a gateway VPC endpoint in the same Region as the VPCs. Conﬁgure access through an appropriate subnet and authorization rule.,,,C,0,1,,,,0,0,,,,,4.1,Authentication & Authorization,,235,SCS-C02,AWS Certified Security - Specialty,,"AWS Client VPN provides secure, client-based remote access from a laptop to specific VPC subnets and can be restricted with authorization rules and routes to only the two peered VPCs. Site-to-Site VPN is for network-to-network connections, VPC sharing is for sharing resources across accounts (not network access), and gateway VPC endpoints are for private access to AWS services, not VPC connectivity."
5.1,VPC Security,A company’s security policy requires all Amazon EC2 instances to use the Amazon Time Sync Service. AWS CloudTrail trails are enabled in all of the company’s AWS accounts. VPC ﬂow logs are enabled for all VPCs. A security engineer must identify any EC2 instances that attempt to use Network Time Protocol (NTP) servers on the internet. Which solution will meet these requirements?,Monitor CloudTrail logs for API calls to non-standard time servers.,Monitor CloudTrail logs for API calls to the Amazon Time Sync Service.,Monitor VPC ﬂow logs for traﬃc to non-standard time servers.,Monitor VPC ﬂow logs for traﬃc to the Amazon Time Sync Service.,,,C,0,1,,,,0,0,,,,,2.3,Monitoring & Detection,,270,SCS-C02,AWS Certified Security - Specialty,,"NTP usage is a network-level action, not an API call, so CloudTrail cannot detect it. VPC Flow Logs capture outbound flows; filtering for UDP/123 traffic to public IPs (excluding 169.254.169.123, the Amazon Time Sync Service) identifies instances attempting to use non-standard internet NTP servers."
5.1,VPC Security,A public subnet contains two Amazon EC2 instances. The subnet has a custom network ACL. A security engineer is designing a solution to improve the subnet security. The solution must allow outbound traﬃc to an internet service that uses TLS through port 443. The solution also must deny inbound traﬃc that is destined for MySQL port 3306. Which network ACL rule set meets these requirements?,Use inbound rule 100 to allow traﬃc on TCP port 443. Use inbound rule 200 to deny traﬃc on TCP port 3306. Use outbound rule 100 to allow traﬃc on TCP port 443.,Use inbound rule 100 to deny traﬃc on TCP port 3306. Use inbound rule 200 to allow traﬃc on TCP port range 1024-65535. Use outbound rule 100 to allow traﬃc on TCP port 443.,Use inbound rule 100 to allow traﬃc on TCP port range 1024-65535. Use inbound rule 200 to deny traﬃc on TCP port 3306. Use outbound rule 100 to allow traﬃc on TCP port 443.,Use inbound rule 100 to deny traﬃc on TCP port 3306. Use inbound rule 200 to allow traﬃc on TCP port 443. Use outbound rule 100 to allow traﬃc on TCP port 443.,,,D,0,1,,,,0,0,,,,,3.1,Network Architecture Security,,285,SCS-C02,AWS Certified Security - Specialty,,"Network ACLs are stateless, so an explicit outbound allow on TCP 443 is required to permit TLS egress. Option D also explicitly denies inbound TCP 3306 to block MySQL, satisfying the deny requirement. Allowing inbound 443 is acceptable and does not violate requirements, while other inbound ports remain implicitly denied."
5.2,VPC Endpoints & PrivateLink,A security engineer is working with a company to design an ecommerce application. The application will run on Amazon EC2 instances that run in an Auto Scaling group behind an Application Load Balancer (ALB). The application will use an Amazon RDS DB instance for its database. The only required connectivity from the internet is for HTTP and HTTPS traﬃc to the application. The application must communicate with an external payment provider that allows traﬃc only from a preconﬁgured allow list of IP addresses. The company must ensure that communications with the external payment provider are not interrupted as the environment scales. Which combination of actions should the security engineer recommend to meet these requirements? (Choose three.),Deploy a NAT gateway in each private subnet for every Availability Zone that is in use.,Place the DB instance in a public subnet.,Place the DB instance in a private subnet.,Conﬁgure the Auto Scaling group to place the EC2 instances in a public subnet.,Conﬁgure the Auto Scaling group to place the EC2 instances in a private subnet.,Deploy the ALB in a private subnet.,"A, C, E",1,1,,,,0,0,,,,,3.1,Network Architecture Security,,17,SCS-C02,AWS Certified Security - Specialty,,"Place the EC2 instances and the RDS DB instance in private subnets (E and C) so the only internet-facing component is the ALB, meeting the requirement that only HTTP/HTTPS from the internet is allowed. Use NAT gateways (A) with Elastic IPs for outbound traffic so the payment provider can allowlist fixed source IPs, and deploy one per AZ for resilience and uninterrupted communication as the environment scales."
5.2,VPC Endpoints & PrivateLink,"While securing the connection between a company’s VPC and its on-premises data center, a security engineer sent a ping command from an on-premises host (IP address 203.0.113.12) to an Amazon EC2 instance (IP address 172.31.16.139). The ping command did not return a response. The ﬂow log in the VPC showed the following: What action should be performed to allow the ping to work?","In the security group of the EC2 instance, allow inbound ICMP traﬃc.","In the security group of the EC2 instance, allow outbound ICMP traﬃc.","In the VPC’s NACL, allow inbound ICMP traﬃc.","In the VPC’s NACL, allow outbound ICMP traﬃc.",,,D,0,1,,,,0,0,,,,,3.1,Network Architecture Security,,25,SCS-C02,AWS Certified Security - Specialty,,"Ping uses ICMP Echo Request and Echo Reply; while the request may reach the instance, the reply must be allowed back out. Network ACLs are stateless, so you must explicitly allow outbound ICMP for the subnet or the Echo Reply will be dropped. Adjusting the instance’s security group alone won’t fix an outbound NACL block."
5.2,VPC Endpoints & PrivateLink,A company is hosting multiple applications within a single VPC in its AWS account. The applications are running behind an Application Load Balancer that is associated with an AWS WAF web ACL. The company's security team has identiﬁed that multiple port scans are originating from a speciﬁc range of IP addresses on the internet. A security engineer needs to deny access from the offending IP addresses. Which solution will meet these requirements?,Modify the AWS WAF web ACL with an IP set match rule statement to deny incoming requests from the IP address range.,Add a rule to all security groups to deny the incoming requests from the IP address range.,Modify the AWS WAF web ACL with a rate-based rule statement to deny the incoming requests from the IP address range.,Conﬁgure the AWS WAF web ACL with regex match conditions. Specify a pattern set to deny the incoming requests based on the match condition.,,,A,0,1,,,,0,0,,,,,3.2,Edge & Web Protection,,67,SCS-C02,AWS Certified Security - Specialty,,"An AWS WAF IP set match rule is purpose-built to block traffic from specific IP addresses or CIDR ranges and can be applied directly to the ALB via the web ACL. Security groups cannot explicitly deny traffic, rate-based rules are for throttling not targeted IP blocking, and regex match conditions inspect request content, not source IPs."
5.2,VPC Endpoints & PrivateLink,A company is using AWS Organizations to implement a multi-account strategy. The company does not have on-premises infrastructure. All workloads run on AWS. The company currently has eight member accounts. The company anticipates that it will have no more than 20 AWS accounts total at any time. The company issues a new security policy that contains the following requirements: • No AWS account should use a VPC within the AWS account for workloads. • The company should use a centrally managed VPC that all AWS accounts can access to launch workloads in subnets. • No AWS account should be able to modify another AWS account's application resources within the centrally managed VPC. • The centrally managed VPC should reside in an existing AWS account that is named Ac-count-A within an organization. The company uses an AWS CloudFormation template to create a VPC that contains multiple subnets in Account-A. This template exports the subnet IDs through the CloudFormation Outputs section. Which solution will complete the security setup to meet these requirements?,Use a CloudFormation template in the member accounts to launch workloads. Conﬁgure the template to use the Fn::ImportValue function to obtain the subnet ID values.,Use a transit gateway in the VPC within Account-A. Conﬁgure the member accounts to use the transit gateway to access the subnets in Account-A to launch workloads.,Use AWS Resource Access Manager (AWS RAM) to share Account-A's VPC subnets with the remaining member accounts. Conﬁgure the member accounts to use the shared subnets to launch workloads.,Create a peering connection between Account-A and the remaining member accounts. Conﬁgure the member accounts to use the subnets in Account-A through the VPC peering connection to launch workloads.,,,C,0,1,,,,0,0,,,,,4.1,Authentication & Authorization,,75,SCS-C02,AWS Certified Security - Specialty,,"VPC sharing via AWS RAM lets Account-A share specific subnets with other organization accounts so they can launch resources directly in those subnets, while Account-A retains control of VPC-level settings. Participant accounts cannot modify the VPC or each other’s resources, meeting the isolation requirement. ImportValue is not cross-account, and peering/transit gateways provide connectivity, not the ability to launch into another account’s subnets."
5.2,VPC Endpoints & PrivateLink,A security engineer needs to set up an Amazon CloudFront distribution for an Amazon S3 bucket that hosts a static website. The security engineer must allow only speciﬁed IP addresses to access the website. The security engineer also must prevent users from accessing the website directly by using S3 URLs. Which solution will meet these requirements?,Generate an S3 bucket policy. Specify cloudfront.amazonaws.com as the principal. Use the aws:SourceIp condition key to allow access only if the request comes from the speciﬁed IP addresses.,"Create a CloudFront origin access control (OAC). Create the S3 bucket policy so that only the OAC has access. Create an AWS WAF web ACL, and add an IP set rule. Associate the web ACL with the CloudFront distribution.",Implement security groups to allow only the speciﬁed IP addresses access and to restrict S3 bucket access by using the CloudFront distribution.,Create an S3 bucket access point to allow access from only the CloudFront distribution. Create an AWS WAF web ACL and add an IP set rule. Associate the web ACL with the CloudFront distribution.,,,B,0,1,,,,0,0,,,,,3.2,Edge & Web Protection,,139,SCS-C02,AWS Certified Security - Specialty,,Creating a CloudFront Origin Access Control (OAC) and restricting the S3 bucket policy to only allow the OAC prevents direct access via S3 URLs. Limiting access to specified IPs is best done at the CloudFront edge by associating an AWS WAF web ACL with an IP set. This combination enforces least privilege and meets both requirements.
5.2,VPC Endpoints & PrivateLink,"A company wants to remove all SSH keys permanently from a speciﬁc subset of its Amazon Linux 2 Amazon EC2 instances that are using the same IAM instance proﬁle. However, three individuals who have IAM user accounts will need to access these instances by using an SSH session to perform critical duties. How can a security engineer provide the access to meet these requirements?",Assign an IAM policy to the instance proﬁle to allow the EC2 instances to be managed by AWS Systems Manager. Provide the IAM user accounts with permission to use Systems Manager. Remove the SSH keys from the EC2 instances. Use Systems Manager Inventory to select the EC2 instance and connect.,Assign an IAM policy to the IAM user accounts to provide permission to use AWS Systems Manager Run Command. Remove the SSH keys from the EC2 instances. Use Run Command to open an SSH connection to the EC2 instance.,Assign an IAM policy to the instance proﬁle to allow the EC2 instances to be managed by AWS Systems Manager. Provide the IAM user accounts with permission to use Systems Manager. Remove the SSH keys from the EC2 instances. Use Systems Manager Session Manager to select the EC2 instance and connect.,Assign an IAM policy to the IAM user accounts to provide permission to use the EC2 service in the AWS Management Console. Remove the SSH keys from the EC2 instances. Connect to the EC2 instance as the ec2-user through the AWS Management Console’s EC2 SSH client method.,,,C,0,1,,,,0,0,,,,,4.1,Authentication & Authorization,,187,SCS-C02,AWS Certified Security - Specialty,,"AWS Systems Manager Session Manager provides interactive shell access to EC2 instances without needing SSH keys or opening inbound ports. Granting the instance profile SSM permissions and giving the users Session Manager permissions meets the requirement to remove SSH keys while still allowing access. Options A and B don’t provide interactive sessions (Inventory/Run Command), and D is not a valid or secure method."
5.2,VPC Endpoints & PrivateLink,A company uses Amazon Cognito as an OAuth 2.0 identity platform for its web and mobile applications. The company needs to capture successful and unsuccessful login attempts. The company also needs to query the data about the login attempts. Which solution will meet these requirements?,Conﬁgure Cognito to send logs of user activity to Amazon CloudWatch. Conﬁgure Amazon EventBridge to invoke an AWS Lambda function to export the logs to an Amazon S3 bucket. Use Amazon Athena to query the logs for event names of SignUp with event sources of cognito-idp.amazonaws.com.,Enable AWS CloudTrail to deliver logs to an Amazon S3 bucket. Use Amazon Athena to query the logs for event names of InitiateAuth with event sources of cognito-idp.amazonaws.com.,Conﬁgure AWS CloudTrail to send Cognito CloudTrail events to Amazon CloudWatch for monitoring. Query the event logs for event names of SignUp with event sources of cognito-idp.amazonaws.com.,Conﬁgure Amazon CloudWatch metrics to monitor and report Cognito events. Create a CloudWatch dashboard for the provided metrics. Display the Cognito user pools for event names of InitiateAuth with event sources of cognito-idp.amazonaws.com.,,,B,0,1,,,,0,0,,,,,4.1,Authentication & Authorization,,218,SCS-C02,AWS Certified Security - Specialty,,"AWS CloudTrail records Cognito user pool API calls, including both successful and failed sign-in attempts via the InitiateAuth (and AdminInitiateAuth) events, and can deliver logs to S3 for analysis with Athena. This meets both capture and query requirements. Other options either track the wrong event (SignUp), lack queryable storage, or add unnecessary complexity."
5.2,VPC Endpoints & PrivateLink,A company uses AWS Lambda functions to implement application logic. The company uses an organization in AWS Organizations to manage hundreds of AWS accounts. The company needs to implement a solution to continuously monitor the Lambda functions for vulnerabilities in all accounts. The solution must publish detected issues to a dashboard. Lambda functions that are being tested or are in development must not appear on the dashboard. Which combination of steps will meet these requirements? (Choose two.),Designate a delegated Amazon GuardDuty administrator account in the organization’s management account. Use the GuardDuty Summary dashboard to obtain an overview of Lambda functions that have vulnerabilities.,Designate a delegated Amazon Inspector administrator account in the organization’s management account. Use the Amazon Inspector dashboard to obtain an overview of Lambda functions that have vulnerabilities.,Apply tags of “test” or “development” to all Lambda functions that are in testing or development. Use a suppression ﬁlter that suppresses ﬁndings that contain these tags.,Enable AWS Shield Advanced in the organization’s management account. Use Amazon CloudWatch to build a dashboard for Lambda functions that have vulnerabilities.,Enable Lambda Protection in GuardDuty for all accounts. Auto-enable Lambda Protection for new accounts. Apply a tag to the Lambda functions that are in testing or development. Use GuardDutyExclusion as the tag key and LambdaStandardScanning as the tag value.,,"B, C",1,1,,,,0,0,,,,,2.3,Monitoring & Detection,,249,SCS-C02,AWS Certified Security - Specialty,,"Amazon Inspector provides continuous, organization-wide vulnerability scanning for Lambda functions and offers a centralized dashboard via a delegated administrator, satisfying the monitoring and visibility requirements. By tagging test/dev functions and creating suppression filters, their findings are excluded from the dashboard. GuardDuty and Shield Advanced focus on threat detection/DDoS, not vulnerability assessments, so they don’t meet the stated needs."
5.2,VPC Endpoints & PrivateLink,A company runs workloads on Amazon EC2 instances in VPCs. The EC2 instances make requests to Amazon S3 buckets through VPC endpoints. The company uses AWS Organizations to manage its AWS accounts. The company needs the requests from the EC2 instances to originate from the same VPC that the EC2 instance credentials were issued to. Which solution will meet this requirement?,Deploy an SCP that includes the S3:* action with the “aws:SourceVpc”: “${aws:Ec2InstanceSourceVpc}” condition.,Edit the VPC endpoints to include the S3:* action with the “aws:Ec2InstanceSourcePrivateIPv4”: “${aws:VpcSourceIp}” condition.,Limit all actions in the S3 bucket policies by using the aws:SourceVpce condition key with the value of the allowed VPC endpoint.,Limit all actions in the S3 bucket policies by using the aws:SourceVpc condition key with the value of the allowed VPC ID.,,,D,0,1,,,,0,0,,,,,4.1,Authentication & Authorization,,299,SCS-C02,AWS Certified Security - Specialty,,"Using an S3 bucket policy with the aws:SourceVpc condition restricts access to requests that come through a gateway VPC endpoint from the specified VPC, ensuring the instance role’s credentials can only be used from that same VPC. For S3, aws:SourceVpc is the supported condition (not aws:SourceVpce), and SCPs or endpoint policies cannot enforce this per-request VPC origin."
5.3,Network Firewall & Route 53 Resolver,A company needs to detect unauthenticated access to its Amazon Elastic Kubernetes Service (Amazon EKS) clusters. The company needs a solution that requires no additional conﬁguration of the existing EKS deployment. Which solution will meet these requirements with the LEAST operational effort?,Install an Amazon EKS add-on from a security vendor.,Enable AWS Security Hub. Monitor the Kubernetes ﬁndings.,Monitor Amazon CloudWatch Container Insights metrics for Amazon EKS.,Enable Amazon GuardDuty. Use EKS Audit Log Monitoring.,,,D,0,0,1,0,0,1,1,,,,,3.2,Edge & Web Protection,,207,SCS-C02,AWS Certified Security - Specialty,,"Amazon GuardDuty EKS Audit Log Monitoring natively analyzes Kubernetes audit logs to detect threats such as unauthenticated access without requiring agents or changes to the existing EKS deployment, delivering the least operational effort. Security Hub only aggregates findings, Container Insights focuses on performance metrics, and third-party add-ons require installation and configuration."
5.4,ELB & ALB Security,"A company has enabled Amazon GuardDuty in all AWS Regions as part of its security monitoring strategy. In one of its VPCs, the company hosts an Amazon EC2 instance that works as an FTP server. A high number of clients from multiple locations contact the FTP server. GuardDuty identiﬁes this activity as a brute force attack because of the high number of connections that happen every hour. The company has ﬂagged the ﬁnding as a false positive, but GuardDuty continues to raise the issue. A security engineer must improve the signal-to-noise ratio without compromising the company's visibility of potential anomalous behavior. Which solution will meet these requirements?",Disable the FTP rule in GuardDuty in the Region where the FTP server is deployed.,Add the FTP server to a trusted IP list. Deploy the list to GuardDuty to stop receiving the notiﬁcations.,Create a suppression rule in GuardDuty to ﬁlter ﬁndings by automatically archiving new ﬁndings that match the speciﬁed criteria.,Create an AWS Lambda function that has the appropriate permissions to delete the ﬁnding whenever a new occurrence is reported.,,,C,0,1,,,,0,0,,,,,2.3,Monitoring & Detection,,36,SCS-C02,AWS Certified Security - Specialty,,"GuardDuty suppression rules let you automatically archive findings that match specific criteria (e.g., finding type, instance ID), reducing noise while preserving visibility because the findings remain accessible if needed. Disabling the rule (A) removes important coverage, trusted IP lists (B) won’t help because the benign source IPs are many and variable and the server itself isn’t the source, and deleting findings via Lambda (D) is not the intended or supported workflow versus archival/suppression."
5.4,ELB & ALB Security,A company discovers a billing anomaly in its AWS account. A security consultant investigates the anomaly and discovers that an employee who left the company 30 days ago still has access to the account. The company has not monitored account activity in the past. The security consultant needs to determine which resources have been deployed or reconﬁgured by the employee as quickly as possible. Which solution will meet these requirements?,"In AWS Cost Explorer, ﬁlter chart data to display results from the past 30 days. Export the results to a data table. Group the data table by resource.","Use AWS Cost Anomaly Detection to create a cost monitor. Access the detection history. Set the time frame to Last 30 days. In the search area, choose the service category.","In AWS CloudTrail, ﬁlter the event history to display results from the past 30 days. Create an Amazon Athena table that contains the data. Partition the table by event source.",Use AWS Audit Manager to create an assessment for the past 30 days. Apply a usage-based framework to the assessment. Conﬁgure the assessment to assess by resource.,,,C,0,1,,,,0,0,,,,,2.3,Monitoring & Detection,,49,SCS-C02,AWS Certified Security - Specialty,,"CloudTrail records API activity (creates, updates, deletes) and the calling identity for the past 90 days, letting the consultant filter to the last 30 days and see exactly what the ex-employee changed. Querying the logs with Athena, partitioned by eventSource, enables fast, service-by-service identification of affected resources. Cost tools and Audit Manager do not provide per-API, per-identity change details needed for this task."
5.4,ELB & ALB Security,A company uses SAML federation to grant users access to AWS accounts. A company workload that is in an isolated AWS account runs on immutable infrastructure with no human access to Amazon EC2. The company requires a specialized user known as a break glass user to have access to the workload AWS account and instances in the case of SAML errors. A recent audit discovered that the company did not create the break glass user for the AWS account that contains the workload. The company must create the break glass user. The company must log any activities of the break glass user and send the logs to a security team. Which combination of solutions will meet these requirements? (Choose two.),Create a local individual break glass IAM user for the security team. Create a trail in AWS CloudTrail that has Amazon CloudWatch Logs turned on. Use Amazon EventBridge to monitor local user activities.,Create a break glass EC2 key pair for the AWS account. Provide the key pair to the security team. Use AWS CloudTrail to monitor key pair activity. Send notiﬁcations to the security team by using Amazon Simple Notiﬁcation Service (Amazon SNS).,Create a break glass IAM role for the account. Allow security team members to perform the AssumeRoleWithSAML operation. Create an AWS CloudTrail trail that has Amazon CloudWatch Logs turned on. Use Amazon EventBridge to monitor security team activities.,Create a local individual break glass IAM user on the operating system level of each workload instance. Conﬁgure unrestricted security groups on the instances to grant access to the break glass IAM users.,Conﬁgure AWS Systems Manager Session Manager for Amazon EC2. Conﬁgure an AWS CloudTrail ﬁlter based on Session Manager. Send the results to an Amazon Simple Notiﬁcation Service (Amazon SNS) topic.,,"A, E",1,1,,,,0,0,,,,,4.1,Authentication & Authorization,,86,SCS-C02,AWS Certified Security - Specialty,,"A creates a local IAM user independent of SAML, ensuring emergency account access, and uses CloudTrail with CloudWatch Logs/EventBridge to log and monitor all activities for the security team. E provides controlled, auditable access to EC2 via Systems Manager Session Manager (no SSH needed), with CloudTrail filtering and SNS notifications to alert the security team. Other options either rely on SAML, use insecure/unsuitable access (SSH key pairs), or propose unsafe instance-level users and security groups."
5.4,ELB & ALB Security,An IAM user receives an Access Denied message when the user attempts to access objects in an Amazon S3 bucket. The user and the S3 bucket are in the same AWS account. The S3 bucket is conﬁgured to use server-side encryption with AWS KMS keys (SSE-KMS) to encrypt all of its objects at rest by using a customer managed key from the same AWS account. The S3 bucket has no bucket policy deﬁned. The IAM user has been granted permissions through an IAM policy that allows the kms:Decrypt permission to the customer managed key. The IAM policy also allows the s3:List* and s3:Get* permissions for the S3 bucket and its objects. Which of the following is a possible reason that the IAM user cannot access the objects in the S3 bucket?,The IAM policy needs to allow the kms:DescribeKey permission.,The S3 bucket has been changed to use the AWS managed key to encrypt objects at rest.,An S3 bucket policy needs to be added to allow the IAM user to access the objects.,The KMS key policy has been edited to remove the ability for the AWS account to have full access to the key.,,,D,0,1,,,,0,0,,,,,4.1,Authentication & Authorization,,99,SCS-C02,AWS Certified Security - Specialty,,"With SSE-KMS, access requires both S3 permissions and KMS permissions allowed by the KMS key policy. Even if the IAM user has kms:Decrypt in an IAM policy, if the KMS key policy no longer grants the account (or user) permission to use the key, KMS will deny decryption and S3 object access will fail. Bucket policies aren’t required for same-account access, and kms:DescribeKey isn’t needed to decrypt."
5.4,ELB & ALB Security,A company uses AWS Organizations and has Amazon Elastic Kubernetes Service (Amazon EKS) clusters in many AWS accounts. A security engineer integrates Amazon EKS with AWS CloudTrail. The CloudTrail trails are stored in an Amazon S3 bucket in each account to monitor API calls. The security engineer observes that CloudTrail logs are not displaying Kubernetes pod creation events. What should the security engineer do to view the Kubernetes events from Amazon CloudWatch?,Conﬁgure the EKS clusters to use private S3 VPC endpoints. Conﬁgure the S3 buckets for logging.,Enable Kubernetes API server component logs for each cluster.,Enable cross-origin resource sharing (CORS) in the S3 bucket that is used for logging.,Conﬁgure CloudWatch. View the events in the CloudWatch console.,,,B,0,1,,,,0,0,,,,,2.3,Monitoring & Detection,,146,SCS-C02,AWS Certified Security - Specialty,,"CloudTrail records AWS API calls, not Kubernetes API server actions like pod creations. To view Kubernetes events in CloudWatch, you must enable EKS control plane (Kubernetes API server/audit) logging, which sends those logs (e.g., API, audit) to CloudWatch Logs. The other options don’t surface Kubernetes API events."
5.4,ELB & ALB Security,AWS CloudTrail is being used to monitor API calls in an organization. An audit revealed that CloudTrail is failing to deliver events to Amazon S3 as expected. What initial actions should be taken to allow delivery of CloudTrail events to S3? (Choose two.),Verify that the S3 bucket policy allows CloudTrail to write objects.,Verify that the IAM role used by CloudTrail has access to write to Amazon CloudWatch Logs.,Remove any lifecycle policies on the S3 bucket that are archiving objects to S3 Glacier Flexible Retrieval.,Verify that the S3 bucket deﬁned in CloudTrail exists.,Verify that the log ﬁle preﬁx deﬁned in CloudTrail exists in the S3 bucket.,,"A, D",1,1,,,,0,0,,,,,2.1,Logging Configuration,,192,SCS-C02,AWS Certified Security - Specialty,,"CloudTrail must have permission to write to the specified S3 bucket, so confirming the bucket policy allows the CloudTrail service to PutObject (often with the required ACL condition) is essential. You must also verify that the S3 bucket defined in the trail actually exists and is correctly referenced. CloudWatch Logs permissions, lifecycle policies, and pre-creating the prefix are not required for S3 delivery (CloudTrail creates the prefix automatically)."
5.4,ELB & ALB Security,A security engineer is designing security controls for a ﬂeet of Amazon EC2 instances that run sensitive workloads in a VPC. The security engineer needs to implement a solution to detect and mitigate software vulnerabilities on the EC2 instances. Which solution will meet this requirement?,Scan the EC2 instances by using Amazon Inspector. Apply security patches and updates by using AWS Systems Manager Patch Manager.,Install host-based ﬁrewall and antivirus software on each EC2 instance. Use AWS Systems Manager Run Command to update the ﬁrewall and antivirus software.,Install the Amazon CloudWatch agent on the EC2 instances. Enable detailed logging. Use Amazon EventBridge to review the software logs for anomalies.,Scan the EC2 instances by using Amazon GuardDuty Malware Protection. Apply security patches and updates by using AWS Systems Manager Patch Manager.,,,A,0,1,,,,0,0,,,,,3.3,Host & Instance Hardening,,256,SCS-C02,AWS Certified Security - Specialty,,"Amazon Inspector automatically scans EC2 instances for known software vulnerabilities (CVEs) and unintended network exposure, providing continuous, managed detection. AWS Systems Manager Patch Manager complements this by automating the application of security patches, addressing the discovered vulnerabilities. Other options either don’t detect CVEs (GuardDuty focuses on malware) or rely on manual/insufficient controls."
5.4,ELB & ALB Security,"A company runs an application on a ﬂeet of Amazon EC2 instances behind an Application Load Balancer (ALB). A security engineer needs to provide secure access to the application without requiring the use of a VPN. Users should be able to access the application only when they meet speciﬁc security conditions, including a deﬁned device posture. Which solution will meet these requirements?",Create an AWS WAF web ACL. Conﬁgure a custom response to block traﬃc that does not align with the deﬁned device posture.,Conﬁgure AWS Veriﬁed Access. Add the application by creating an endpoint for the ALB.,Conﬁgure Amazon Veriﬁed Permissions. Use a policy-based access control (PBAC) policy to perform authorization.,Conﬁgure Amazon Veriﬁed Permissions. Add the application by creating an endpoint for the ALB.,,,B,0,1,,,,0,0,,,,,4.1,Authentication & Authorization,,261,SCS-C02,AWS Certified Security - Specialty,,"AWS Verified Access enables VPN-less, zero-trust access to private applications and can enforce policies that include identity and device posture conditions via trust providers. You add the application by creating a Verified Access endpoint that targets the ALB. AWS WAF can’t evaluate device posture, and Amazon Verified Permissions is for authorization logic, not for securing network access or creating endpoints."
5.4,ELB & ALB Security,A security engineer is designing a solution that will provide end-to-end encryption between clients and Docker containers running in Amazon Elastic Container Service (Amazon ECS). This solution will also handle volatile traﬃc patterns. Which solution would have the MOST scalability and LOWEST latency?,Conﬁgure a Network Load Balancer to terminate the TLS traﬃc and then re-encrypt the traﬃc to the containers.,Conﬁgure an Application Load Balancer to terminate the TLS traﬃc and then re-encrypt the traﬃc to the containers.,Conﬁgure a Network Load Balancer with a TCP listener to pass through TLS traﬃc to the containers.,Conﬁgure Amazon Route 53 to use multivalue answer routing to send traﬃc to the containers.,,,C,0,1,,,,0,0,,,,,3.4,Container & Serverless Security,,306,SCS-C02,AWS Certified Security - Specialty,,"A Network Load Balancer with a TCP listener preserves TLS pass-through so the TLS session terminates at the containers, delivering true end-to-end encryption with minimal latency because NLB operates at Layer 4. NLBs also scale near-instantly to volatile traffic patterns, whereas terminating TLS at ALB/NLB adds overhead and breaks end-to-end, and Route 53 multivalue routing lacks load balancer capabilities."
5.5,Backup & Disaster Recovery,"A systems engineer is troubleshooting the connectivity of a test environment that includes a virtual security appliance deployed inline. In addition to using the virtual security appliance, the development team wants to use security groups and network ACLs to accomplish various security requirements in the environment. What conﬁguration is necessary to allow the virtual security appliance to route the traﬃc?",Disable network ACLs.,Conﬁgure the security appliance's elastic network interface for promiscuous mode.,Disable the Network Source/Destination check on the security appliance's elastic network interface.,Place the security appliance in the public subnet with the internet gateway.,,,C,0,0,1,0,0,0,1,,,,,3.1,Network Architecture Security,,107,SCS-C02,AWS Certified Security - Specialty,,"By default, an EC2 instance enforces source/destination checks and drops any traffic not explicitly to or from itself. A virtual security appliance must forward traffic between other endpoints, so disabling the source/destination check on its ENI allows it to route packets while still using security groups and NACLs."
5.5,Backup & Disaster Recovery,A company uses AWS Organizations to manage several AWS accounts. The company processes a large volume of sensitive data. The company uses a serverless approach to microservices. The company stores all the data in either Amazon S3 or Amazon DynamoDB. The company reads the data by using either AWS Lambda functions or container-based services that the company hosts on Amazon Elastic Kubernetes Service (Amazon EKS) on AWS Fargate. The company must implement a solution to encrypt all the data at rest and enforce least privilege data access controls. The company creates an AWS Key Management Service (AWS KMS) customer managed key. What should the company do next to meet these requirements?,Create a key policy that allows the kms:Decrypt action only for Amazon S3 and DynamoDB. Create an SCP that denies the creation of S3 buckets and DynamoDB tables that are not encrypted with the key.,Create an IAM policy that denies the kms:Decrypt action for the key. Create a Lambda function than runs on a schedule to attach the policy to any new roles. Create an AWS Conﬁg rule to send alerts for resources that are not encrypted with the key.,"Create a key policy that allows the kms:Decrypt action only for Amazon S3, DynamoDB, Lambda, and Amazon EKS. Create an SCP that denies the creation of S3 buckets and DynamoDB tables that are not encrypted with the key.","Create a key policy that allows the kms:Decrypt action only for Amazon S3, DynamoDB, Lambda, and Amazon EKS. Create an AWS Conﬁg rule to send alerts for resources that are not encrypted with the key.",,,C,0,0,1,0,0,0,1,,,,,5.1,Encryption at Rest,,154,SCS-C02,AWS Certified Security - Specialty,,"Option C both enforces encryption and least privilege. A KMS key policy scoped with kms:ViaService to S3, DynamoDB, Lambda, and EKS limits decrypt use to the exact services that need it, and an SCP that denies creating S3 buckets and DynamoDB tables without that key enforces encryption at rest org-wide. Other options either omit needed services or only alert rather than enforce."
5.5,Backup & Disaster Recovery,An Amazon API Gateway API invokes an AWS Lambda function that needs to interact with a software-as-a-service (SaaS) platform. A unique client token is generated in the SaaS platform to grant access to the Lambda function. A security engineer needs to design a solution to encrypt the access token at rest and pass the token to the Lambda function at runtime. Which solution will meet these requirements MOST cost-effectively?,Store the client token as a secret in AWS Secrets Manager. Use the AWS SDK to retrieve the secret in the Lambda function.,Conﬁgure a token-based Lambda authorizer in API Gateway.,Store the client token as a SecureString parameter in AWS Systems Manager Parameter Store. Use the AWS SDK to retrieve the value of the SecureString parameter in the Lambda function.,Use AWS Key Management Service (AWS KMS) to encrypt the client token. Pass the token to the Lambda function at runtime through an environment variable.,,,C,0,0,1,0,0,0,1,,,,,5.1,Encryption at Rest,,202,SCS-C02,AWS Certified Security - Specialty,,"Systems Manager Parameter Store SecureString encrypts the token at rest with KMS and allows the Lambda function to fetch it at runtime via the SDK, meeting the requirements with minimal cost. Secrets Manager also works but is more expensive for this simple use case. A Lambda authorizer doesn’t store secrets, and environment variables are not ideal for secure storage or rotation and require function updates to change the token."
5.5,Backup & Disaster Recovery,A company needs to implement data lifecycle management for Amazon RDS snapshots. The company will use AWS Backup to manage the snapshots. The company must retain RDS automated snapshots for 5 years and will use Amazon S3 for long-term archival storage. Which solution will meet these requirements?,Use AWS Backup to apply a 5-year retention tag to the RDS snapshots.,Enable versioning on the S3 bucket that AWS Backup uses for the RDS snapshots. Conﬁgure a 5-year retention period.,Create an S3 Lifecycle policy. Include a 5-year retention period for the S3 bucket that AWS Backup uses for the RDS snapshots.,Create a backup plan in AWS Backup. Conﬁgure a 5-year retention period.,,,D,0,0,1,0,0,0,1,,,,,5.1,Encryption at Rest,,269,SCS-C02,AWS Certified Security - Specialty,,"AWS Backup uses backup plans to define lifecycle and retention policies for supported resources, including RDS, allowing you to transition backups to cold storage and expire them after 5 years. Tags, S3 versioning, and S3 lifecycle policies do not control retention of RDS snapshots managed by AWS Backup, and RDS snapshots are not stored as customer-managed S3 objects."
,,A company makes forecasts each quarter to decide how to optimize operations to meet expected demand. The company uses ML models to make these forecasts. An AI practitioner is writing a report about the trained ML models to provide transparency and explainability to company stakeholders. What should the AI practitioner include in the report to meet the transparency and explainability requirements?,Code for model training,Partial dependence plots (PDPs),Sample data for training,Model convergence tables,,,B,,0,0,1,0,1,1,,,,,4.2,Recognize the importance of transparent and explainable models.,,1,AIF-C01,AWS Certified AI Practitioner,,"Partial dependence plots (PDPs) show how a model’s predictions change as one feature varies, holding others constant. This clearly explains feature influence on forecasts, improving transparency for stakeholders. Code, sample data, or convergence tables don’t reveal how features drive predictions."
,,A law ﬁrm wants to build an AI application by using large language models (LLMs). The application will read legal documents and extract key points from the documents. Which solution meets these requirements?,Build an automatic named entity recognition system.,Create a recommendation engine.,Develop a summarization chatbot.,Develop a multi-language translation system.,,,C,,0,0,0,1,1,,,,,,2.2,Understand the capabilities and limitations of generative AI for solving business problems.,,2,AIF-C01,AWS Certified AI Practitioner,,"Summarization is the LLM task designed to read long texts and produce concise key points, making a summarization chatbot the right fit for legal document analysis. Named entity recognition only finds entities (e.g., names, dates), and recommendation or translation systems do not address extracting key points."
,,A company wants to classify human genes into 20 categories based on gene characteristics. The company needs an ML algorithm to document how the inner mechanism of the model affects the output. Which ML algorithm meets these requirements?,Decision trees,Linear regression,Logistic regression,Neural networks,,,A,,0,0,1,0,1,1,,,,,4.2,Recognize the importance of transparent and explainable models.,,3,AIF-C01,AWS Certified AI Practitioner,,"Decision trees are inherently interpretable: each split shows which feature and threshold guided the decision, so you can trace how inputs lead to a class. They handle multi-class classification naturally, such as 20 gene categories. In contrast, neural networks are opaque, and regression models are not ideal for multi-class classification explainability."
,,A company has built an image classiﬁcation model to predict plant diseases from photos of plant leaves. The company wants to evaluate how many images the model classiﬁed correctly. Which evaluation metric should the company use to measure the model's performance?,R-squared score,Accuracy,Root mean squared error (RMSE),Learning rate,,,B,,0,0,1,0,2,,,,,,1.1,Explain basic AI concepts and terminologies.,,4,AIF-C01,AWS Certified AI Practitioner,,"Accuracy measures the proportion of images correctly classified out of all images, which directly answers how many predictions are correct. R-squared and RMSE are regression metrics, and learning rate is a training hyperparameter, not a classification performance metric."
,,A company is using a pre-trained large language model (LLM) to build a chatbot for product recommendations. The company needs the LLM outputs to be short and written in a speciﬁc language. Which solution will align the LLM response quality with the company's expectations?,Adjust the prompt.,Choose an LLM of a different size.,Increase the temperature.,Increase the Top K value.,,,A,,0,0,0,1,1,,,,,,3.2,Choose effective prompt engineering techniques.,,5,AIF-C01,AWS Certified AI Practitioner,,"Prompt engineering lets you specify constraints like language, tone, and response length (e.g., 'Answer in Spanish in two short sentences'). Model size and sampling settings (temperature/Top K) influence creativity and diversity, not strict adherence to format and language. Therefore, adjusting the prompt best aligns outputs with the company’s requirements."
,,A company uses Amazon SageMaker for its ML pipeline in a production environment. The company has large input data sizes up to 1 GB and processing times up to 1 hour. The company needs near real-time latency. Which SageMaker inference option meets these requirements?,Real-time inference,Serverless inference,Asynchronous inference,Batch transform,,,C,,0,0,0,1,2,1,,,,,1.3,Describe the ML development lifecycle.,,6,AIF-C01,AWS Certified AI Practitioner,,"Asynchronous inference is designed for large payloads (up to ~1 GB) and long-running processing (up to ~1 hour). It queues requests and returns results when ready (via S3/notifications), enabling near real-time behavior without synchronous timeouts. Real-time/serverless have stricter payload/time limits, and batch transform is for offline batch jobs."
,,"A company is using domain-speciﬁc models. The company wants to avoid creating new models from the beginning. The company instead wants to adapt pre-trained models to create models for new, related tasks. Which ML strategy meets these requirements?",Increase the number of epochs.,Use transfer learning.,Decrease the number of epochs.,Use unsupervised learning.,,,B,,0,0,0,1,1,,,,,,3.3,Describe the training and fine-tuning process for foundation models.,,7,AIF-C01,AWS Certified AI Practitioner,,"Transfer learning adapts a pre-trained model to a new, related task, avoiding the need to train from scratch. It reuses learned features and fine-tunes on domain-specific data, saving time and data. Changing epochs or using unsupervised learning does not address adapting pre-trained models."
,,A company is building a solution to generate images for protective eyewear. The solution must have high accuracy and must minimize the risk of incorrect annotations. Which solution will meet these requirements?,Human-in-the-loop validation by using Amazon SageMaker Ground Truth Plus,Data augmentation by using an Amazon Bedrock knowledge base,Image recognition by using Amazon Rekognition,Data summarization by using Amazon QuickSight Q,,,A,,0,0,1,0,1,,,,,,4.1,Explain the development of AI systems that are responsible.,,8,AIF-C01,AWS Certified AI Practitioner,,"SageMaker Ground Truth Plus uses human-in-the-loop workflows and rigorous quality controls to produce highly accurate labels, reducing the risk of incorrect annotations. Accurate, verified labels are critical for training reliable image-generation models. The other options do not provide managed human validation for labeling."
,,A company wants to create a chatbot by using a foundation model (FM) on Amazon Bedrock. The FM needs to access encrypted data that is stored in an Amazon S3 bucket. The data is encrypted with Amazon S3 managed keys (SSE-S3). The FM encounters a failure when attempting to access the S3 bucket data. Which solution will meet these requirements?,Ensure that the role that Amazon Bedrock assumes has permission to decrypt data with the correct encryption key.,Set the access permissions for the S3 buckets to allow public access to enable access over the internet.,Use prompt engineering techniques to tell the model to look for information in Amazon S3.,Ensure that the S3 data does not contain sensitive information.,,,A,,0,0,0,1,1,,,,,,5.1,Explain methods to secure AI systems.,,9,AIF-C01,AWS Certified AI Practitioner,,"The failure occurs because the Bedrock execution role lacks the necessary permissions to read and decrypt the S3 objects. Grant the role appropriate access to the bucket and decryption with the encryption key so S3 can transparently decrypt the data during GetObject calls. Public access, prompt engineering, or data sensitivity do not resolve access/permission issues."
,,A company wants to use language models to create an application for inference on edge devices. The inference must have the lowest latency possible. Which solution will meet these requirements?,Deploy optimized small language models (SLMs) on edge devices.,Deploy optimized large language models (LLMs) on edge devices.,Incorporate a centralized small language model (SLM) API for asynchronous communication with edge devices.,Incorporate a centralized large language model (LLM) API for asynchronous communication with edge devices.,,,A,,0,0,0,1,1,,,,,,3.1,Describe design considerations for applications that use foundation models.,,10,AIF-C01,AWS Certified AI Practitioner,,"To achieve the lowest latency, inference must run locally on the device to avoid network round trips. Optimized small language models are lightweight enough for edge hardware, providing fast responses with limited compute and memory. Large models or centralized APIs introduce compute or network latency, which increases response time."
,,A company wants to build an ML model by using Amazon SageMaker. The company needs to share and manage variables for model development across multiple teams. Which SageMaker feature meets these requirements?,Amazon SageMaker Feature Store,Amazon SageMaker Data Wrangler,Amazon SageMaker Clarify,Amazon SageMaker Model Cards,,,A,,0,0,0,1,2,1,,,,,1.3,Describe the ML development lifecycle.,,11,AIF-C01,AWS Certified AI Practitioner,,"SageMaker Feature Store is a centralized repository to store, manage, and share ML features (variables) across teams, ensuring consistency and reuse in training and inference. It supports governance with versioning and lineage, and offers both online and offline stores. Data Wrangler is for data prep, Clarify is for bias/explainability, and Model Cards are for documentation, not feature sharing."
,,A company wants to use generative AI to increase developer productivity and software development. The company wants to use Amazon Q Developer. What can Amazon Q Developer do to help the company meet these requirements?,"Create software snippets, reference tracking, and open source license tracking.",Run an application without provisioning or managing servers.,Enable voice commands for coding and providing natural language search.,Convert audio ﬁles to text documents by using ML models.,,,A,,0,0,1,0,2,,,,,,2.2,Understand the capabilities and limitations of generative AI for solving business problems.,,12,AIF-C01,AWS Certified AI Practitioner,,"Amazon Q Developer is a generative AI coding assistant that boosts developer productivity by generating code snippets, providing reference citations, and tracking open‑source license usage for compliance. The other options describe different AWS services or unrelated capabilities (like serverless compute or speech-to-text)."
,,"A ﬁnancial institution is using Amazon Bedrock to develop an AI application. The application is hosted in a VPC. To meet regulatory compliance standards, the VPC is not allowed access to any internet traﬃc. Which AWS service or feature will meet these requirements?",AWS PrivateLink,Amazon Macie,Amazon CloudFront,Internet gateway,,,A,,0,0,0,1,1,,,,,,2.3,Describe AWS infrastructure and technologies for building generative AI applications.,,13,AIF-C01,AWS Certified AI Practitioner,,"AWS PrivateLink lets your VPC privately connect to Amazon Bedrock using interface VPC endpoints, without public IPs or internet access. All traffic stays on the AWS network, meeting strict no-internet compliance. Other options either expose traffic to the internet (Internet gateway, CloudFront) or are unrelated (Macie)."
,,"A company wants to develop an educational game where users answer questions such as the following: ""A jar contains six red, four green, and three yellow marbles. What is the probability of choosing a green marble from the jar?"" Which solution meets these requirements with the LEAST operational overhead?",Use supervised learning to create a regression model that will predict probability.,Use reinforcement learning to train a model to return the probability.,Use code that will calculate probability by using simple rules and computations.,Use unsupervised learning to create a model that will estimate probability density.,,,C,,0,0,1,0,1,1,,,,,2.3,Describe AWS infrastructure and technologies for building generative AI applications.,,14,AIF-C01,AWS Certified AI Practitioner,,"This problem is a straightforward, deterministic calculation: favorable outcomes divided by total outcomes (4/13). Simple code can compute this directly, giving exact results with minimal infrastructure. Machine learning approaches would add training, data, and maintenance overhead without any benefit."
,,Which metric measures the runtime eﬃciency of operating AI models?,Customer satisfaction score (CSAT),Training time for each epoch,Average response time,Number of training instances,,,C,,0,0,0,1,1,,,,,,3.1,Describe design considerations for applications that use foundation models.,,15,AIF-C01,AWS Certified AI Practitioner,,"Average response time shows how quickly a deployed model returns predictions during inference, which reflects runtime efficiency. CSAT is a business metric, training time per epoch measures training efficiency, and number of training instances is about dataset size—not inference performance."
,,A company is building a contact center application and wants to gain insights from customer conversations. The company wants to analyze and extract key information from the audio of the customer calls. Which solution meets these requirements?,Build a conversational chatbot by using Amazon Lex.,Transcribe call recordings by using Amazon Transcribe.,Extract information from call recordings by using Amazon SageMaker Model Monitor.,Create classiﬁcation labels by using Amazon Comprehend.,,,B,,0,0,1,0,2,1,,,,,1.2,Identify practical use cases for AI.,,16,AIF-C01,AWS Certified AI Practitioner,,"Amazon Transcribe converts call audio into text and offers Call Analytics to extract insights like speaker separation, sentiment, and categories from conversations. Lex builds chatbots, Comprehend analyzes text (after transcription), and SageMaker Model Monitor is for monitoring models—not for processing audio."
,,A company has petabytes of unlabeled customer data to use for an advertisement campaign. The company wants to classify its customers into tiers to advertise and promote the company's products. Which methodology should the company use to meet these requirements?,Supervised learning,Unsupervised learning,Reinforcement learning,Reinforcement learning from human feedback (RLHF),,,B,,0,0,1,0,1,,,,,,1.1,Explain basic AI concepts and terminologies.,,17,AIF-C01,AWS Certified AI Practitioner,,"Unsupervised learning is used to find patterns and group similar items when data has no labels, which fits petabytes of unlabeled customer data. It can cluster customers into tiers for targeting, while supervised learning needs labeled examples and reinforcement methods focus on rewards, not segmentation."
,,An AI practitioner wants to use a foundation model (FM) to design a search application. The search application must handle queries that have text and images. Which type of FM should the AI practitioner use to power the search application?,Multi-modal embedding model,Text embedding model,Multi-modal generation model,Image generation model,,,A,,0,0,0,1,1,,,,,,3.1,Describe design considerations for applications that use foundation models.,,18,AIF-C01,AWS Certified AI Practitioner,,"Search relies on embeddings to compare queries and items by similarity. A multi-modal embedding model maps both text and images into the same vector space, enabling cross-modal search (text-to-image and image-to-text). Text-only embeddings can’t handle images, and generation models create content rather than provide comparable embeddings for retrieval."
,,A company uses a foundation model (FM) from Amazon Bedrock for an AI search tool. The company wants to ﬁne-tune the model to be more accurate by using the company's data. Which strategy will successfully ﬁne-tune the model?,Provide labeled data with the prompt ﬁeld and the completion ﬁeld.,Prepare the training dataset by creating a .txt ﬁle that contains multiple lines in .csv format.,Purchase Provisioned Throughput for Amazon Bedrock.,Train the model on journals and textbooks.,,,A,,0,0,1,0,1,1,,,,,3.3,Describe the training and fine-tuning process for foundation models.,,19,AIF-C01,AWS Certified AI Practitioner,,"Fine-tuning a Bedrock foundation model uses supervised training on labeled prompt–completion pairs, so the model learns how to respond given your company’s inputs. Provisioned Throughput only reserves capacity, and dumping data as CSV or generic textbooks is not the required labeled format for fine-tuning."
,,A company wants to use AI to protect its application from threats. The AI solution needs to check if an IP address is from a suspicious source. Which solution meets these requirements?,Build a speech recognition system.,Create a natural language processing (NLP) named entity recognition system.,Develop an anomaly detection system.,Create a fraud forecasting system.,,,C,,0,0,0,1,2,1,,,,,1.2,Identify practical use cases for AI.,,20,AIF-C01,AWS Certified AI Practitioner,,"Anomaly detection finds unusual patterns in network traffic, flagging IP addresses that behave differently from normal users. This directly addresses identifying suspicious sources to protect applications. Speech recognition, NLP entity extraction, and fraud forecasting target different problems (voice, text entities, transaction risk) and do not evaluate IP behavior."
,,Which feature of Amazon OpenSearch Service gives companies the ability to build vector database applications?,Integration with Amazon S3 for object storage,Support for geospatial indexing and queries,Scalable index management and nearest neighbor search capability,Ability to perform real-time analysis on streaming data,,,C,,0,0,0,1,1,,,,,,2.3,Describe AWS infrastructure and technologies for building generative AI applications.,,21,AIF-C01,AWS Certified AI Practitioner,,"Vector database applications rely on storing embeddings and retrieving similar items using k-nearest neighbor (k-NN) or approximate nearest neighbor (ANN) search. Amazon OpenSearch Service provides scalable vector indices and native k-NN search, enabling efficient similarity search at scale. The other options deal with storage, geospatial data, or streaming analytics, not vector search."
,,Which option is a use case for generative AI models?,Improving network security by using intrusion detection systems,Creating photorealistic images from text descriptions for digital marketing,Enhancing database performance by using optimized indexing,Analyzing ﬁnancial data to forecast stock market trends,,,B,,0,0,0,1,1,,,,,,2.2,Understand the capabilities and limitations of generative AI for solving business problems.,,22,AIF-C01,AWS Certified AI Practitioner,,"Generative AI creates new content, and models like diffusion or GANs can turn text prompts into photorealistic images—ideal for digital marketing. The other options involve detection, optimization, or forecasting, which are analytic/discriminative tasks rather than content generation."
,,A company wants to build a generative AI application by using Amazon Bedrock and needs to choose a foundation model (FM). The company wants to know how much information can ﬁt into one prompt. Which consideration will inform the company's decision?,Temperature,Context window,Batch size,Model size,,,B,,0,0,0,1,1,,,,,,3.1,Describe design considerations for applications that use foundation models.,,23,AIF-C01,AWS Certified AI Practitioner,,"The context window is the maximum number of tokens a model can process at once, which determines how much information (input plus expected output) can fit into a single prompt. Temperature controls randomness, batch size relates to training/inference throughput, and model size refers to parameters—not the prompt length limit."
,,A company wants to make a chatbot to help customers. The chatbot will help solve technical problems without human intervention. The company chose a foundation model (FM) for the chatbot. The chatbot needs to produce responses that adhere to company tone. Which solution meets these requirements?,Set a low limit on the number of tokens the FM can produce.,Use batch inferencing to process detailed responses.,Experiment and reﬁne the prompt until the FM produces the desired responses.,Deﬁne a higher number for the temperature parameter.,,,C,,0,0,0,1,1,,,,,,3.2,Choose effective prompt engineering techniques.,,24,AIF-C01,AWS Certified AI Practitioner,,"Prompt engineering is how you steer a foundation model’s tone, style, and behavior; iteratively refining the prompt aligns outputs with company voice. Token limits and batch inference affect length and throughput, not tone. Increasing temperature makes responses more random, which reduces consistency with a defined company tone."
,,A company wants to use a large language model (LLM) on Amazon Bedrock for sentiment analysis. The company wants to classify the sentiment of text passages as positive or negative. Which prompt engineering strategy meets these requirements?,Provide examples of text passages with corresponding positive or negative labels in the prompt followed by the new text passage to be classiﬁed.,Provide a detailed explanation of sentiment analysis and how LLMs work in the prompt.,Provide the new text passage to be classiﬁed without any additional context or examples.,"Provide the new text passage with a few examples of unrelated tasks, such as text summarization or question answering.",,,A,,0,0,0,1,1,,,,,,3.2,Choose effective prompt engineering techniques.,,25,AIF-C01,AWS Certified AI Practitioner,,Providing labeled examples is a few-shot prompting strategy that teaches the LLM the exact mapping from text to “positive” or “negative.” This guidance improves accuracy and consistency for sentiment classification on Bedrock. Explanations or unrelated examples don’t help the model perform the task and zero-shot prompts are less reliable.
,,A security company is using Amazon Bedrock to run foundation models (FMs). The company wants to ensure that only authorized users invoke the models. The company needs to identify any unauthorized access attempts to set appropriate AWS Identity and Access Management (IAM) policies and roles for future iterations of the FMs. Which AWS service should the company use to identify unauthorized users that are trying to access Amazon Bedrock?,AWS Audit Manager,AWS CloudTrail,Amazon Fraud Detector,AWS Trusted Advisor,,,B,,0,0,0,1,3,1,,,,,5.1,Explain methods to secure AI systems.,,26,AIF-C01,AWS Certified AI Practitioner,,"AWS CloudTrail records all API calls to Amazon Bedrock, including failed or unauthorized requests, along with user identity, time, and source IP. These logs let the company detect who attempted access and why it was denied, informing IAM policy and role adjustments. Other services listed do not capture per-request access attempts for Bedrock APIs."
,,A company has developed an ML model for image classiﬁcation. The company wants to deploy the model to production so that a web application can use the model. The company needs to implement a solution to host the model and serve predictions without managing any of the underlying infrastructure. Which solution will meet these requirements?,Use Amazon SageMaker Serverless Inference to deploy the model.,Use Amazon CloudFront to deploy the model.,Use Amazon API Gateway to host the model and serve predictions.,Use AWS Batch to host the model and serve predictions.,,,A,,0,0,0,1,1,,,,,,1.3,Describe the ML development lifecycle.,,27,AIF-C01,AWS Certified AI Practitioner,,"Amazon SageMaker Serverless Inference provides a fully managed way to host ML models behind an HTTPS endpoint without provisioning or managing servers. It automatically scales with request volume and charges per use, making it ideal for web app predictions."
,,An AI company periodically evaluates its systems and processes with the help of independent software vendors (ISVs). The company needs to receive email message notiﬁcations when an ISV's compliance reports become available. Which AWS service can the company use to meet this requirement?,AWS Audit Manager,AWS Artifact,AWS Trusted Advisor,AWS Data Exchange,,,B,,0,0,1,0,2,3,,,,,5.2,Recognize governance and compliance regulations for AI systems.,,28,AIF-C01,AWS Certified AI Practitioner,,"AWS Artifact provides on-demand access to security and compliance reports (including from select ISVs) and lets you subscribe to email notifications when new or updated reports are available. In contrast, Audit Manager helps you audit your own environment, Trusted Advisor gives best-practice checks, and Data Exchange is for data products, not compliance reports."
,,A company wants to use a large language model (LLM) to develop a conversational agent. The company needs to prevent the LLM from being manipulated with common prompt engineering techniques to perform undesirable actions or expose sensitive information. Which action will reduce these risks?,Create a prompt template that teaches the LLM to detect attack patterns.,Increase the temperature parameter on invocation requests to the LLM.,Avoid using LLMs that are not listed in Amazon SageMaker.,Decrease the number of input tokens on invocations of the LLM.,,,A,,0,0,1,0,2,1,,,,,5.1,Explain methods to secure AI systems.,,29,AIF-C01,AWS Certified AI Practitioner,,"Teaching the LLM, via a robust prompt template, to recognize and reject prompt injection and jailbreak patterns directly addresses manipulation risks. Clear system instructions and examples help the model refuse unsafe requests and avoid revealing sensitive data. Changing temperature, token count, or model source does not materially improve security against these attacks."
,,A company is using the Generative AI Security Scoping Matrix to assess security responsibilities for its solutions. The company has identiﬁed four different solution scopes based on the matrix. Which solution scope gives the company the MOST ownership of security responsibilities?,Using a third-party enterprise application that has embedded generative AI features.,Building an application by using an existing third-party generative AI foundation model (FM).,Reﬁning an existing third-party generative AI foundation model (FM) by ﬁne-tuning the model by using data speciﬁc to the business.,Building and training a generative AI model from scratch by using speciﬁc data that a customer owns.,,,D,,0,0,0,1,1,,,,,,5.1,Explain methods to secure AI systems.,,30,AIF-C01,AWS Certified AI Practitioner,,"Building and training your own model from scratch gives you full control over the data, model, training/inference pipelines, and deployment. Because you own every layer, you must implement and manage all security controls (data protection, access, monitoring, compliance). The other options rely on third-party models or apps, which shift more security responsibility to the provider."
,,An AI practitioner has a database of animal photos. The AI practitioner wants to automatically identify and categorize the animals in the photos without manual human effort. Which strategy meets these requirements?,Object detection,Anomaly detection,Named entity recognition,Inpainting,,,A,,0,0,1,0,1,,,,,,1.1,Explain basic AI concepts and terminologies.,,31,AIF-C01,AWS Certified AI Practitioner,,"Object detection automatically finds and labels objects (like animals) in images by locating them and assigning categories, enabling identification without manual effort. Anomaly detection finds outliers, named entity recognition works on text, and inpainting fills missing image areas—none of these match the need to identify and categorize animals in photos."
,,A company wants to create an application by using Amazon Bedrock. The company has a limited budget and prefers ﬂexibility without long-term commitment. Which Amazon Bedrock pricing model meets these requirements?,On-Demand,Model customization,Provisioned Throughput,Spot Instance,,,A,,0,0,0,1,1,,,,,,2.3,Describe AWS infrastructure and technologies for building generative AI applications.,,32,AIF-C01,AWS Certified AI Practitioner,,"On-Demand in Amazon Bedrock charges per request/token with no upfront fees or long-term commitment, offering maximum flexibility on a limited budget. Provisioned Throughput requires reserved capacity and a commitment period, and model customization adds extra training costs. Spot Instances apply to EC2, not Bedrock."
,,Which AWS service or feature can help an AI development team quickly deploy and consume a foundation model (FM) within the team's VPC?,Amazon Personalize,Amazon SageMaker JumpStart,"PartyRock, an Amazon Bedrock Playground",Amazon SageMaker endpoints,,,B,,0,0,1,0,1,1,,,,,2.3,Describe AWS infrastructure and technologies for building generative AI applications.,,33,AIF-C01,AWS Certified AI Practitioner,,"Amazon SageMaker JumpStart provides ready-to-deploy foundation models and automates creating secure, managed endpoints in your VPC. This lets teams quickly select, provision, and consume FMs with minimal setup while keeping traffic private within their environment."
,,How can companies use large language models (LLMs) securely on Amazon Bedrock?,Design clear and speciﬁc prompts. Conﬁgure AWS Identity and Access Management (IAM) roles and policies by using least privilege access.,Enable AWS Audit Manager for automatic model evaluation jobs.,Enable Amazon Bedrock automatic model evaluation jobs.,Use Amazon CloudWatch Logs to make models explainable and to monitor for bias.,,,A,,0,0,0,1,1,,,,,,5.1,Explain methods to secure AI systems.,,34,AIF-C01,AWS Certified AI Practitioner,,"Secure LLM use on Bedrock starts with strong access control—IAM roles and least-privilege policies limit who can invoke models and what data they can access. Clear, specific prompts also help prevent unintended data disclosure and guide safe outputs. The other options focus on evaluation or logging, not core security controls."
,,A company has terabytes of data in a database that the company can use for business analysis. The company wants to build an AI-based application that can build a SQL query from input text that employees provide. The employees have minimal experience with technology. Which solution meets these requirements?,Generative pre-trained transformers (GPT),Residual neural network,Support vector machine,WaveNet,,,A,,0,0,1,0,2,,,,,,2.2,Understand the capabilities and limitations of generative AI for solving business problems.,,35,AIF-C01,AWS Certified AI Practitioner,,"Generative pre-trained transformers (GPT) are large language models that can understand natural language and generate structured text like SQL, enabling text-to-SQL queries for nontechnical users. ResNets are for image recognition, SVMs for classification, and WaveNet for audio; they do not generate database queries."
,,A company built a deep learning model for object detection and deployed the model to production. Which AI process occurs when the model analyzes a new image to identify objects?,Training,Inference,Model deployment,Bias correction,,,B,,0,0,1,0,1,,,,,,1.1,Explain basic AI concepts and terminologies.,,36,AIF-C01,AWS Certified AI Practitioner,,"Inference is when a trained model applies what it learned to new data and produces predictions—here, detecting objects in a fresh image. Training happens earlier to learn from labeled data, while deployment just makes the model available; bias correction deals with fairness, not prediction."
,,An AI practitioner is building a model to generate images of humans in various professions. The AI practitioner discovered that the input data is biased and that speciﬁc attributes affect the image generation and create bias in the model. Which technique will solve the problem?,Data augmentation for imbalanced classes,Model monitoring for class distribution,Retrieval Augmented Generation (RAG),Watermark detection for images,,,A,,0,0,1,0,1,,,,,,4.1,Explain the development of AI systems that are responsible.,,37,AIF-C01,AWS Certified AI Practitioner,,"Data augmentation for imbalanced classes increases representation of underrepresented groups/attributes, helping the model learn balanced patterns and reduce bias. Monitoring only detects imbalance, RAG addresses knowledge grounding (not image bias), and watermark detection is unrelated to bias mitigation."
,,A company is implementing the Amazon Titan foundation model (FM) by using Amazon Bedrock. The company needs to supplement the model by using relevant data from the company's private data sources. Which solution will meet this requirement?,Use a different FM.,Choose a lower temperature value.,Create an Amazon Bedrock knowledge base.,Enable model invocation logging.,,,C,,0,0,0,1,1,,,,,,3.1,Describe design considerations for applications that use foundation models.,,38,AIF-C01,AWS Certified AI Practitioner,,"An Amazon Bedrock knowledge base enables retrieval-augmented generation by connecting the model to private data sources and grounding responses with relevant context. Changing temperature or the FM does not add private data, and model invocation logging only records requests and responses. Therefore, creating a knowledge base is the correct way to supplement Titan with company data."
,,A medical company is customizing a foundation model (FM) for diagnostic purposes. The company needs the model to be transparent and explainable to meet regulatory requirements. Which solution will meet these requirements?,Conﬁgure the security and compliance by using Amazon Inspector.,"Generate simple metrics, reports, and examples by using Amazon SageMaker Clarify.",Encrypt and secure training data by using Amazon Macie.,Gather more data. Use Amazon Rekognition to add custom labels to the data.,,,B,,0,0,1,0,1,,,,,,4.2,Recognize the importance of transparent and explainable models.,,39,AIF-C01,AWS Certified AI Practitioner,,"Amazon SageMaker Clarify provides explainability through feature attribution, bias detection, and reports that help meet transparency and regulatory needs. It generates metrics and visualizations to explain model predictions for both training and inference. The other options address security or data labeling, not explainability."
,,A company wants to deploy a conversational chatbot to answer customer questions. The chatbot is based on a ﬁne-tuned Amazon SageMaker JumpStart model. The application must comply with multiple regulatory frameworks. Which capabilities can the company show compliance for? (Choose two.),Auto scaling inference endpoints,Threat detection,Data protection,Cost optimization,Loosely coupled microservices,,"B, C",1,0,0,0,1,1,,,,,,5.1,Explain methods to secure AI systems.,,40,AIF-C01,AWS Certified AI Practitioner,,"Threat detection and data protection are core security controls required by regulatory frameworks and can be evidenced with AWS capabilities (e.g., GuardDuty, CloudTrail, encryption, IAM). Auto scaling, cost optimization, and loosely coupled microservices are architectural or cost considerations, not compliance controls. Therefore, B and C directly support demonstrating compliance for the chatbot workload."
,,A company is training a foundation model (FM). The company wants to increase the accuracy of the model up to a speciﬁc acceptance level. Which solution will meet these requirements?,Decrease the batch size.,Increase the epochs.,Decrease the epochs.,Increase the temperature parameter.,,,B,,0,0,1,0,1,,,,,,3.3,Describe the training and fine-tuning process for foundation models.,,41,AIF-C01,AWS Certified AI Practitioner,,"Increasing the number of epochs lets the model iterate over the training data more times, often improving accuracy until convergence or overfitting. Temperature affects inference randomness, not training accuracy, and batch size changes optimization dynamics but does not reliably increase accuracy. Decreasing epochs typically reduces learning and is unlikely to meet an accuracy target."
,,A company is building a large language model (LLM) question answering chatbot. The company wants to decrease the number of actions call center employees need to take to respond to customer questions. Which business objective should the company use to evaluate the effect of the LLM chatbot?,Website engagement rate,Average call duration,Corporate social responsibility,Regulatory compliance,,,B,,0,0,0,1,1,,,,,,2.2,Understand the capabilities and limitations of generative AI for solving business problems.,,42,AIF-C01,AWS Certified AI Practitioner,,"Average call duration directly reflects how efficiently agents resolve issues; fewer required actions typically shorten calls. This metric captures the chatbot’s impact on reducing agent workload and handling time. Website engagement, CSR, and regulatory compliance do not measure call center operational efficiency."
,,Which functionality does Amazon SageMaker Clarify provide?,Integrates a Retrieval Augmented Generation (RAG) workﬂow,Monitors the quality of ML models in production,Documents critical details about ML models,Identiﬁes potential bias during data preparation,,,D,,0,0,1,0,1,,,,,,4.1,Explain the development of AI systems that are responsible.,,43,AIF-C01,AWS Certified AI Practitioner,,"Amazon SageMaker Clarify detects and reports potential bias in datasets and models, supporting fairness during data preparation and evaluation. It computes bias metrics pre- and post-training and offers explainability via feature attributions. The other options pertain to different services: RAG (not Clarify), production monitoring (SageMaker Model Monitor), and documentation (Model Cards for SageMaker)."
,,"A company is developing a new model to predict the prices of speciﬁc items. The model performed well on the training dataset. When the company deployed the model to production, the model's performance decreased signiﬁcantly. What should the company do to mitigate this problem?",Reduce the volume of data that is used in training.,Add hyperparameters to the model.,Increase the volume of data that is used in training.,Increase the model training time.,,,C,,0,0,1,0,2,1,,,,,1.1,Explain basic AI concepts and terminologies.,,44,AIF-C01,AWS Certified AI Practitioner,,Strong training performance but weak production performance indicates overfitting or poor generalization. Increasing the training data volume helps the model learn broader patterns and reduces overfitting. The other options do not directly address generalization and may worsen the issue.
,,An ecommerce company wants to build a solution to determine customer sentiments based on written customer reviews of products. Which AWS services meet these requirements? (Choose two.),Amazon Lex,Amazon Comprehend,Amazon Polly,Amazon Bedrock,Amazon Rekognition,,"B, D",1,0,0,1,0,1,,,,,,2.3,Describe AWS infrastructure and technologies for building generative AI applications.,,45,AIF-C01,AWS Certified AI Practitioner,,"Amazon Comprehend provides native NLP sentiment analysis for text reviews. Amazon Bedrock can also perform sentiment classification using foundation models via prompting. Amazon Lex is for chatbots, Amazon Polly is text-to-speech, and Amazon Rekognition is for image/video analysis, so they do not meet the text sentiment requirement."
,,A company wants to use large language models (LLMs) with Amazon Bedrock to develop a chat interface for the company's product manuals. The manuals are stored as PDF ﬁles. Which solution meets these requirements MOST cost-effectively?,Use prompt engineering to add one PDF ﬁle as context to the user prompt when the prompt is submitted to Amazon Bedrock.,Use prompt engineering to add all the PDF ﬁles as context to the user prompt when the prompt is submitted to Amazon Bedrock.,Use all the PDF documents to ﬁne-tune a model with Amazon Bedrock. Use the ﬁne-tuned model to process user prompts.,Upload PDF documents to an Amazon Bedrock knowledge base. Use the knowledge base to provide context when users submit prompts to Amazon Bedrock.,,,D,,0,0,0,1,1,,,,,,3.1,Describe design considerations for applications that use foundation models.,,46,AIF-C01,AWS Certified AI Practitioner,,"Bedrock knowledge bases enable retrieval-augmented generation by indexing PDFs into embeddings and retrieving only relevant chunks at query time. This reduces tokens per request and avoids costly fine-tuning while improving relevance. Options A and B are inefficient due to context window limits and token costs, and C is unnecessary and more expensive."
,,A social media company wants to use a large language model (LLM) for content moderation. The company wants to evaluate the LLM outputs for bias and potential discrimination against speciﬁc groups or individuals. Which data source should the company use to evaluate the LLM outputs with the LEAST administrative effort?,User-generated content,Moderation logs,Content moderation guidelines,Benchmark datasets,,,D,,0,0,1,0,1,2,,,,,3.4,Describe methods to evaluate foundation model performance.,,47,AIF-C01,AWS Certified AI Practitioner,,"Benchmark datasets are pre-curated and standardized to test bias and fairness, enabling quick, repeatable evaluations with minimal setup. User content and moderation logs require significant labeling, privacy review, and policy alignment before use. Guidelines are not data and cannot directly measure bias."
,,A company wants to use a pre-trained generative AI model to generate content for its marketing campaigns. The company needs to ensure that the generated content aligns with the company's brand voice and messaging requirements. Which solution meets these requirements?,Optimize the model's architecture and hyperparameters to improve the model's overall performance.,Increase the model's complexity by adding more layers to the model's architecture.,Create effective prompts that provide clear instructions and context to guide the model's generation.,"Select a large, diverse dataset to pre-train a new generative model.",,,C,,0,0,0,1,1,,,,,,3.2,Choose effective prompt engineering techniques.,,48,AIF-C01,AWS Certified AI Practitioner,,"Effective prompts can encode brand voice, tone, style, and constraints to guide a pre-trained model's generation without retraining. Changing architecture or hyperparameters does not specifically ensure brand alignment and is unnecessary for this use case. Training a new model is costly and overkill compared to prompt-based control."
,,A loan company is building a generative AI-based solution to offer new applicants discounts based on speciﬁc business criteria. The company wants to build and use an AI model responsibly to minimize bias that could negatively affect some customers. Which actions should the company take to meet these requirements? (Choose two.),Detect imbalances or disparities in the data.,Ensure that the model runs frequently.,Evaluate the model's behavior so that the company can provide transparency to stakeholders.,Use the Recall-Oriented Understudy for Gisting Evaluation (ROUGE) technique to ensure that the model is 100% accurate.,Ensure that the model's inference time is within the accepted limits.,,"A, C",1,0,0,1,0,1,,,,,,4.1,Explain the development of AI systems that are responsible.,,49,AIF-C01,AWS Certified AI Practitioner,,"A addresses fairness by detecting data imbalances/disparities that can lead to biased outcomes. C focuses on evaluating model behavior and providing transparency to stakeholders, key tenets of responsible AI. Factors like run frequency, latency, or ROUGE scores do not mitigate bias or improve transparency."
,,A company is using an Amazon Bedrock base model to summarize documents for an internal use case. The company trained a custom model to improve the summarization quality. Which action must the company take to use the custom model through Amazon Bedrock?,Purchase Provisioned Throughput for the custom model.,Deploy the custom model in an Amazon SageMaker endpoint for real-time inference.,Register the model with the Amazon SageMaker Model Registry.,Grant access to the custom model in Amazon Bedrock.,,,A,,0,0,1,0,1,1,,,,,2.3,Describe AWS infrastructure and technologies for building generative AI applications.,,50,AIF-C01,AWS Certified AI Practitioner,,"In Amazon Bedrock, custom (fine-tuned) models must be hosted with Provisioned Throughput to be invoked via Bedrock APIs. Provisioned Throughput allocates dedicated capacity and exposes the custom model for inference. Deploying to SageMaker or registering in Model Registry is not required for Bedrock usage, and granting model access applies to provider base models, not your custom model."
,,A company needs to choose a model from Amazon Bedrock to use internally. The company must identify a model that generates responses in a style that the company's employees prefer. What should the company do to meet these requirements?,Evaluate the models by using built-in prompt datasets.,Evaluate the models by using a human workforce and custom prompt datasets.,Use public model leaderboards to identify the model.,Use the model InvocationLatency runtime metrics in Amazon CloudWatch when trying models.,,,B,,0,0,1,0,1,1,,,,,3.4,Describe methods to evaluate foundation model performance.,,51,AIF-C01,AWS Certified AI Practitioner,,"Human evaluation with custom prompts best measures whether a model’s outputs match an organization’s preferred style and use cases. Public leaderboards and built-in prompts may not reflect internal requirements, and latency metrics do not assess response quality or style. Therefore, using a human workforce and custom datasets is the appropriate evaluation method."
,,A student at a university is copying content from generative AI to write essays. Which challenge of responsible generative AI does this scenario represent?,Toxicity,Hallucinations,Plagiarism,Privacy,,,C,,0,0,0,1,1,,,,,,4.1,Explain the development of AI systems that are responsible.,,52,AIF-C01,AWS Certified AI Practitioner,,"Copying AI-generated content and submitting it as one's own constitutes plagiarism, a key responsible AI challenge in ethical use. It involves a lack of attribution and academic dishonesty. This differs from toxicity (harmful language), hallucinations (fabricated facts), and privacy (data exposure)."
,,A company needs to build its own large language model (LLM) based on only the company's private data. The company is concerned about the environmental effect of the training process. Which Amazon EC2 instance type has the LEAST environmental effect when training LLMs?,Amazon EC2 C series,Amazon EC2 G series,Amazon EC2 P series,Amazon EC2 Trn series,,,D,,0,0,1,0,1,1,,,,,2.3,Describe AWS infrastructure and technologies for building generative AI applications.,,53,AIF-C01,AWS Certified AI Practitioner,,"Amazon EC2 Trn (Trainium) instances are purpose-built accelerators for training foundation models and offer superior performance-per-watt, reducing energy use and carbon impact. They are more energy-efficient than general GPU families (P and G) and CPU-based C series for LLM training. Therefore, Trn instances have the least environmental effect for this workload."
,,A company wants to build an interactive application for children that generates new stories based on classic stories. The company wants to use Amazon Bedrock and needs to ensure that the results and topics are appropriate for children. Which AWS service or feature will meet these requirements?,Amazon Rekognition,Amazon Bedrock playgrounds,Guardrails for Amazon Bedrock,Agents for Amazon Bedrock,,,C,,0,0,0,1,2,,,,,,4.1,Explain the development of AI systems that are responsible.,,54,AIF-C01,AWS Certified AI Practitioner,,"Guardrails for Amazon Bedrock enable policy-based filtering and safety controls to block inappropriate topics and content in prompts and responses. This directly supports creating child-appropriate generative outputs. Rekognition is for image/video analysis, Bedrock playgrounds are for experimentation, and Agents add tool-use orchestration—not safety filtering."
,,A company is building an application that needs to generate synthetic data that is based on existing data. Which type of model can the company use to meet this requirement?,Generative adversarial network (GAN),XGBoost,Residual neural network,WaveNet,,,A,,0,0,1,0,5,,,,,,2.1,Explain the basic concepts of generative AI.,,55,AIF-C01,AWS Certified AI Practitioner,,"GANs learn the underlying data distribution via a generator–discriminator framework to produce realistic synthetic samples from existing data. XGBoost and residual networks are primarily discriminative models for prediction/classification, not data generation. WaveNet is generative but focused on audio, whereas GANs are widely used for general-purpose synthetic data creation."
,,A digital devices company wants to predict customer demand for memory hardware. The company does not have coding experience or knowledge of ML algorithms and needs to develop a data-driven predictive model. The company needs to perform analysis on internal data and external data. Which solution will meet these requirements?,Store the data in Amazon S3. Create ML models and demand forecast predictions by using Amazon SageMaker built-in algorithms that use the data from Amazon S3.,Import the data into Amazon SageMaker Data Wrangler. Create ML models and demand forecast predictions by using SageMaker built- in algorithms.,Import the data into Amazon SageMaker Data Wrangler. Build ML models and demand forecast predictions by using an Amazon Personalize Trending-Now recipe.,Import the data into Amazon SageMaker Canvas. Build ML models and demand forecast predictions by selecting the values in the data from SageMaker Canvas.,,,D,,0,0,1,0,1,2,,,,,1.2,Identify practical use cases for AI.,,56,AIF-C01,AWS Certified AI Practitioner,,"SageMaker Canvas is a no-code tool that lets business users build predictive models, including time-series demand forecasts, using internal and external data sources. Options A and B require ML expertise and coding to use built-in algorithms, and C (Amazon Personalize) is designed for recommendations, not demand forecasting."
,,A company has installed a security camera. The company uses an ML model to evaluate the security camera footage for potential thefts. The company has discovered that the model disproportionately ﬂags people who are members of a speciﬁc ethnic group. Which type of bias is affecting the model output?,Measurement bias,Sampling bias,Observer bias,Conﬁrmation bias,,,B,,0,0,1,0,1,,,,,,4.1,Explain the development of AI systems that are responsible.,,57,AIF-C01,AWS Certified AI Practitioner,,"Sampling bias occurs when the training data is not representative of the population, leading the model to over-flag a specific ethnic group. This imbalance skews learned patterns and results in disparate outcomes. It is distinct from measurement bias (faulty data collection), observer bias (annotator influence), and confirmation bias (seeking evidence that confirms preexisting beliefs)."
,,A company is building a customer service chatbot. The company wants the chatbot to improve its responses by learning from past interactions and online resources. Which AI learning strategy provides this self-improvement capability?,Supervised learning with a manually curated dataset of good responses and bad responses,Reinforcement learning with rewards for positive customer feedback,Unsupervised learning to ﬁnd clusters of similar customer inquiries,Supervised learning with a continuously updated FAQ database,,,B,,0,0,1,0,2,,,,,,1.1,Explain basic AI concepts and terminologies.,,58,AIF-C01,AWS Certified AI Practitioner,,"Reinforcement learning optimizes behavior through feedback signals (rewards), enabling a chatbot to improve its responses based on positive customer feedback. Supervised learning relies on labeled examples and does not inherently support interactive self-improvement, and unsupervised clustering doesn't optimize for response quality. Therefore, using rewards tied to customer satisfaction aligns with reinforcement learning."
,,An AI practitioner has built a deep learning model to classify the types of materials in images. The AI practitioner now wants to measure the model performance. Which metric will help the AI practitioner evaluate the performance of the model?,Confusion matrix,Correlation matrix,R2 score,Mean squared error (MSE),,,A,,0,0,1,0,1,,,,,,1.1,Explain basic AI concepts and terminologies.,,59,AIF-C01,AWS Certified AI Practitioner,,"A confusion matrix summarizes classification results with counts of true/false positives and negatives, enabling derived metrics like accuracy, precision, recall, and F1. A correlation matrix measures relationships between variables, not model performance. R2 and MSE are regression metrics, not appropriate for image classification tasks."
,,A company has built a chatbot that can respond to natural language questions with images. The company wants to ensure that the chatbot does not return inappropriate or unwanted images. Which solution will meet these requirements?,Implement moderation APIs.,Retrain the model with a general public dataset.,Perform model validation.,Automate user feedback integration.,,,A,,0,0,1,0,1,,,,,,4.1,Explain the development of AI systems that are responsible.,,60,AIF-C01,AWS Certified AI Practitioner,,"Moderation APIs provide real-time filtering and classification of generated content, blocking inappropriate images before they reach users. Retraining with a general dataset and model validation do not guarantee prevention at inference time, and automated feedback is reactive rather than a proactive safety control."
,,An AI practitioner is using an Amazon Bedrock base model to summarize session chats from the customer service department. The AI practitioner wants to store invocation logs to monitor model input and output data. Which strategy should the AI practitioner use?,Conﬁgure AWS CloudTrail as the logs destination for the model.,Enable invocation logging in Amazon Bedrock.,Conﬁgure AWS Audit Manager as the logs destination for the model.,Conﬁgure model invocation logging in Amazon EventBridge.,,,B,,0,0,1,0,1,,,,,,2.3,Describe AWS infrastructure and technologies for building generative AI applications.,,61,AIF-C01,AWS Certified AI Practitioner,,"Amazon Bedrock provides native model invocation logging to capture and store model inputs and outputs for monitoring and governance. CloudTrail records API calls, not full model payloads; Audit Manager manages audit evidence; and EventBridge routes events, not detailed inference logs. Therefore, enabling invocation logging in Amazon Bedrock is the correct approach."
,,A company is building an ML model to analyze archived data. The company must perform inference on large datasets that are multiple GBs in size. The company does not need to access the model predictions immediately. Which Amazon SageMaker inference option will meet these requirements?,Batch transform,Real-time inference,Serverless inference,Asynchronous inference,,,A,,0,0,0,1,1,,,,,,1.3,Describe the ML development lifecycle.,,62,AIF-C01,AWS Certified AI Practitioner,,"Batch transform is designed for offline, large-scale inference on datasets in Amazon S3 and does not require immediate predictions or a persistent endpoint. It efficiently processes multi-GB inputs and scales automatically. Real-time/serverless are for low-latency requests, and asynchronous inference still uses an endpoint and is less suitable for very large batch datasets."
,,Which term describes the numerical representations of real-world objects and concepts that AI and natural language processing (NLP) models use to improve understanding of textual information?,Embeddings,Tokens,Models,Binaries,,,A,,0,0,1,0,2,2,,,,,1.1,Explain basic AI concepts and terminologies.,,63,AIF-C01,AWS Certified AI Practitioner,,"Embeddings are dense numerical vectors that represent words, sentences, or concepts in a way that captures their semantic meaning for AI/NLP models. This enables similarity search and improved understanding of textual relationships. Tokens are text units, models are algorithms, and binaries are executable files, not semantic representations."
,,"A research company implemented a chatbot by using a foundation model (FM) from Amazon Bedrock. The chatbot searches for answers to questions from a large database of research papers. After multiple prompt engineering attempts, the company notices that the FM is performing poorly because of the complex scientiﬁc terms in the research papers. How can the company improve the performance of the chatbot?",Use few-shot prompting to deﬁne how the FM can answer the questions.,Use domain adaptation ﬁne-tuning to adapt the FM to complex scientiﬁc terms.,Change the FM inference parameters.,Clean the research paper data to remove complex scientiﬁc terms.,,,B,,0,0,1,0,1,1,,,,,3.3,Describe the training and fine-tuning process for foundation models.,,64,AIF-C01,AWS Certified AI Practitioner,,"Domain adaptation fine-tuning exposes the model to domain-specific vocabulary and patterns, improving comprehension of complex scientific terms. Few-shot prompting or tweaking inference parameters cannot reliably overcome knowledge gaps. Removing complex terms would discard essential information and degrade answer quality."
,,A company wants to use a large language model (LLM) on Amazon Bedrock for sentiment analysis. The company needs the LLM to produce more consistent responses to the same input prompt. Which adjustment to an inference parameter should the company make to meet these requirements?,Decrease the temperature value.,Increase the temperature value.,Decrease the length of output tokens.,Increase the maximum generation length.,,,A,,0,0,0,1,1,,,,,,2.1,Explain the basic concepts of generative AI.,,65,AIF-C01,AWS Certified AI Practitioner,,"Temperature controls randomness in token selection; lowering it makes outputs more deterministic and consistent. Increasing temperature increases variability, while output length parameters affect length, not consistency. Therefore, decreasing the temperature value yields more consistent responses."
,,A company wants to develop a large language model (LLM) application by using Amazon Bedrock and customer data that is uploaded to Amazon S3. The company's security policy states that each team can access data for only the team's own customers. Which solution will meet these requirements?,Create an Amazon Bedrock custom service role for each team that has access to only the team's customer data.,Create a custom service role that has Amazon S3 access. Ask teams to specify the customer name on each Amazon Bedrock request.,Redact personal data in Amazon S3. Update the S3 bucket policy to allow team access to customer data.,Create one Amazon Bedrock role that has full Amazon S3 access. Create IAM roles for each team that have access to only each team's customer folders.,,,A,,0,0,0,1,1,,,,,,5.1,Explain methods to secure AI systems.,,66,AIF-C01,AWS Certified AI Practitioner,,"Amazon Bedrock uses a service role to access downstream resources like S3; creating a separate least-privilege Bedrock service role per team restricts access to only that team's customer data. This enforces the security policy automatically on every Bedrock request. Alternatives either rely on manual input (B), do not enforce per-team isolation (C), or grant overly broad S3 access at the Bedrock role level (D), which breaks least privilege."
,,"A medical company deployed a disease detection model on Amazon Bedrock. To comply with privacy policies, the company wants to prevent the model from including personal patient information in its responses. The company also wants to receive notiﬁcation when policy violations occur. Which solution meets these requirements?",Use Amazon Macie to scan the model's output for sensitive data and set up alerts for potential violations.,Conﬁgure AWS CloudTrail to monitor the model's responses and create alerts for any detected personal information.,Use Guardrails for Amazon Bedrock to ﬁlter content. Set up Amazon CloudWatch alarms for notiﬁcation of policy violations.,Implement Amazon SageMaker Model Monitor to detect data drift and receive alerts when model quality degrades.,,,C,,0,0,0,1,1,,,,,,5.1,Explain methods to secure AI systems.,,67,AIF-C01,AWS Certified AI Practitioner,,"Guardrails for Amazon Bedrock can filter and block sensitive content (e.g., PII) in model responses to meet privacy requirements. It integrates with monitoring so violations can be surfaced via Amazon CloudWatch alarms for notifications. Macie, CloudTrail, and SageMaker Model Monitor do not provide real-time content filtering of model outputs."
,,"A company manually reviews all submitted resumes in PDF format. As the company grows, the company expects the volume of resumes to exceed the company's review capacity. The company needs an automated system to convert the PDF resumes into plain text format for additional processing. Which AWS service meets this requirement?",Amazon Textract,Amazon Personalize,Amazon Lex,Amazon Transcribe,,,A,,0,0,0,1,2,,,,,,1.2,Identify practical use cases for AI.,,68,AIF-C01,AWS Certified AI Practitioner,,"Amazon Textract extracts text from PDFs and scanned documents using OCR, converting resumes into machine-readable plain text. Amazon Personalize is for recommendations, Amazon Lex is for chatbots, and Amazon Transcribe converts speech to text, so they do not meet the document text extraction requirement."
,,An education provider is building a question and answer application that uses a generative AI model to explain complex concepts. The education provider wants to automatically change the style of the model response depending on who is asking the question. The education provider will give the model the age range of the user who has asked the question. Which solution meets these requirements with the LEAST implementation effort?,Fine-tune the model by using additional training data that is representative of the various age ranges that the application will support.,Add a role description to the prompt context that instructs the model of the age range that the response should target.,Use chain-of-thought reasoning to deduce the correct style and complexity for a response suitable for that user.,Summarize the response text depending on the age of the user so that younger users receive shorter responses.,,,B,,0,0,0,1,1,,,,,,3.2,Choose effective prompt engineering techniques.,,69,AIF-C01,AWS Certified AI Practitioner,,"Adding a role description or audience specification in the prompt is a standard prompt-engineering technique to control tone and complexity without changing the model. Fine-tuning is costly and unnecessary for simple stylistic adjustments. Chain-of-thought does not enforce audience style, and summarization only shortens text without controlling reading level."
,,Which strategy evaluates the accuracy of a foundation model (FM) that is used in image classiﬁcation tasks?,Calculate the total cost of resources used by the model.,Measure the model's accuracy against a predeﬁned benchmark dataset.,Count the number of layers in the neural network.,Assess the color accuracy of images processed by the model.,,,B,,0,0,0,1,1,,,,,,3.4,Describe methods to evaluate foundation model performance.,,70,AIF-C01,AWS Certified AI Practitioner,,"Evaluating an image classification FM involves measuring performance on a labeled, held-out or benchmark dataset to compute metrics like accuracy. This directly assesses how well predictions match ground truth. Cost, network depth, or image color fidelity do not measure classification accuracy."
,,An accounting ﬁrm wants to implement a large language model (LLM) to automate document processing. The ﬁrm must proceed responsibly to avoid potential harms. What should the ﬁrm do when developing and deploying the LLM? (Choose two.),Include fairness metrics for model evaluation.,Adjust the temperature parameter of the model.,Modify the training data to mitigate bias.,Avoid overﬁtting on the training data.,Apply prompt engineering techniques.,,"A, C",1,0,0,1,0,1,2,,,,,4.1,Explain the development of AI systems that are responsible.,,71,AIF-C01,AWS Certified AI Practitioner,,"Including fairness metrics (A) enables detection and monitoring of disparate impact, a core element of responsible AI. Modifying training data to mitigate bias (C) addresses harmful outcomes at the source by improving data quality and representativeness; this directly reduces risk in deployment."
,,"A company is building an ML model. The company collected new data and analyzed the data by creating a correlation matrix, calculating statistics, and visualizing the data. Which stage of the ML pipeline is the company currently in?",Data pre-processing,Feature engineering,Exploratory data analysis,Hyperparameter tuning,,,C,,0,0,0,1,1,,,,,,1.3,Describe the ML development lifecycle.,,72,AIF-C01,AWS Certified AI Practitioner,,"Computing correlation matrices, summary statistics, and visualizations are hallmark activities of exploratory data analysis (EDA). EDA helps understand distributions, relationships, and data quality before preprocessing or modeling. This differs from preprocessing, feature engineering, and hyperparameter tuning, which occur after initial exploration."
,,A company has documents that are missing some words because of a database error. The company wants to build an ML model that can suggest potential words to ﬁll in the missing text. Which type of model meets this requirement?,Topic modeling,Clustering models,Prescriptive ML models,BERT-based models,,,D,,0,0,1,0,1,1,,,,,1.1,Explain basic AI concepts and terminologies.,,73,AIF-C01,AWS Certified AI Practitioner,,"BERT-based models use masked language modeling to predict missing tokens in text, making them ideal for fill-in-the-blank tasks. Topic modeling and clustering group documents or data points rather than predicting specific words, and prescriptive models recommend actions, not token completions."
,,A company wants to display the total sales for its top-selling products across various retail locations in the past 12 months. Which AWS solution should the company use to automate the generation of graphs?,Amazon Q in Amazon EC2,Amazon Q Developer,Amazon Q in Amazon QuickSight,Amazon Q in AWS Chatbot,,,C,,0,0,1,0,1,,,,,,2.3,Describe AWS infrastructure and technologies for building generative AI applications.,,74,AIF-C01,AWS Certified AI Practitioner,,"Amazon Q in Amazon QuickSight enables natural language queries over business data and automatically generates charts and dashboards. It is purpose-built for BI use cases like displaying total sales across locations over time. The other options (Q Developer, Q in EC2, Q in AWS Chatbot) do not create BI visuals from business data."
,,A company is building a chatbot to improve user experience. The company is using a large language model (LLM) from Amazon Bedrock for intent detection. The company wants to use few-shot learning to improve intent detection accuracy. Which additional data does the company need to meet these requirements?,Pairs of chatbot responses and correct user intents,Pairs of user messages and correct chatbot responses,Pairs of user messages and correct user intents,Pairs of user intents and correct chatbot responses,,,C,,0,0,0,1,1,,,,,,3.2,Choose effective prompt engineering techniques.,,75,AIF-C01,AWS Certified AI Practitioner,,Few-shot learning for intent detection requires labeled examples that map user inputs to the correct intent labels. Providing pairs of user messages and their correct intents teaches the LLM the desired classification mapping. The other options do not directly provide the label mapping needed for intent classification.
,,A company is using few-shot prompting on a base model that is hosted on Amazon Bedrock. The model currently uses 10 examples in the prompt. The model is invoked once daily and is performing well. The company wants to lower the monthly cost. Which solution will meet these requirements?,Customize the model by using ﬁne-tuning.,Decrease the number of tokens in the prompt.,Increase the number of tokens in the prompt.,Use Provisioned Throughput.,,,B,,0,0,1,0,1,,,,,,3.2,Choose effective prompt engineering techniques.,,76,AIF-C01,AWS Certified AI Practitioner,,"Bedrock pricing is based largely on input/output tokens, so reducing examples in the prompt directly lowers token usage and cost. Provisioned Throughput targets consistent high-volume workloads, not a once-daily call. Fine-tuning adds complexity and cost, and increasing tokens would raise costs."
,,An AI practitioner is using a large language model (LLM) to create content for marketing campaigns. The generated content sounds plausible and factual but is incorrect. Which problem is the LLM having?,Data leakage,Hallucination,Overﬁtting,Underﬁtting,,,B,,0,0,1,0,2,,,,,,2.2,Understand the capabilities and limitations of generative AI for solving business problems.,,77,AIF-C01,AWS Certified AI Practitioner,,"Hallucination occurs when a generative model produces confident, fluent text that is factually incorrect or fabricated. This matches the scenario of plausible-sounding but wrong marketing content. It is not data leakage (exposing training data) nor classic ML issues like overfitting or underfitting."
,,An AI practitioner trained a custom model on Amazon Bedrock by using a training dataset that contains conﬁdential data. The AI practitioner wants to ensure that the custom model does not generate inference responses based on conﬁdential data. How should the AI practitioner prevent responses based on conﬁdential data?,Delete the custom model. Remove the conﬁdential data from the training dataset. Retrain the custom model.,Mask the conﬁdential data in the inference responses by using dynamic data masking.,Encrypt the conﬁdential data in the inference responses by using Amazon SageMaker.,Encrypt the conﬁdential data in the custom model by using AWS Key Management Service (AWS KMS).,,,A,,0,0,1,0,1,1,,,,,4.1,Explain the development of AI systems that are responsible.,,78,AIF-C01,AWS Certified AI Practitioner,,"If confidential data was used during training, the model may memorize and emit it; masking or encrypting outputs does not prevent learned leakage. Encryption (KMS/SageMaker) protects data at rest/in transit but not model behavior. The only reliable mitigation is to remove the sensitive data from the training set and retrain, deleting the prior model."
,,A company has built a solution by using generative AI. The solution uses large language models (LLMs) to translate training manuals from English into other languages. The company wants to evaluate the accuracy of the solution by examining the text generated for the manuals. Which model evaluation strategy meets these requirements?,Bilingual Evaluation Understudy (BLEU),Root mean squared error (RMSE),Recall-Oriented Understudy for Gisting Evaluation (ROUGE),F1 score,,,A,,0,0,1,0,1,,,,,,3.4,Describe methods to evaluate foundation model performance.,,79,AIF-C01,AWS Certified AI Practitioner,,"BLEU is a standard metric for evaluating machine translation by comparing n-gram overlaps between generated text and reference translations. ROUGE focuses on recall and is commonly used for summarization, RMSE is for regression, and F1 is for classification or token-level labeling tasks. Therefore, BLEU best measures translation accuracy for LLM-generated manuals."
,,A large retailer receives thousands of customer support inquiries about products every day. The customer support inquiries need to be processed and responded to quickly. The company wants to implement Agents for Amazon Bedrock. What are the key beneﬁts of using Amazon Bedrock agents that could help this retailer?,Generation of custom foundation models (FMs) to predict customer needs,Automation of repetitive tasks and orchestration of complex workﬂows,Automatically calling multiple foundation models (FMs) and consolidating the results,Selecting the foundation model (FM) based on predeﬁned criteria and metrics,,,B,,0,0,1,0,1,,,,,,2.3,Describe AWS infrastructure and technologies for building generative AI applications.,,80,AIF-C01,AWS Certified AI Practitioner,,"Agents for Amazon Bedrock can plan, invoke tools/APIs, and automate multi-step workflows to handle high volumes of repetitive support tasks end-to-end. This directly reduces manual effort and speeds response times. Other options describe capabilities not specific to agents, such as creating custom FMs or automatic multi-model consolidation."
,,Which option is a beneﬁt of ongoing pre-training when ﬁne-tuning a foundation model (FM)?,Helps decrease the model's complexity,Improves model performance over time,Decreases the training time requirement,Optimizes model inference time,,,B,,0,0,1,0,1,1,,,,,3.3,Describe the training and fine-tuning process for foundation models.,,81,AIF-C01,AWS Certified AI Practitioner,,"Continuing pre-training on additional or fresher data before/alongside fine-tuning improves the model’s representations, leading to better downstream task performance over time. It does not reduce model complexity, training time, or inference latency; continued training typically increases compute while yielding accuracy and generalization gains."
,,What are tokens in the context of generative AI models?,"Tokens are the basic units of input and output that a generative AI model operates on, representing words, subwords, or other linguistic units.",Tokens are the mathematical representations of words or concepts used in generative AI models.,Tokens are the pre-trained weights of a generative AI model that are ﬁne-tuned for speciﬁc tasks.,Tokens are the speciﬁc prompts or instructions given to a generative AI model to generate output.,,,A,,0,0,0,1,1,,,,,,2.1,Explain the basic concepts of generative AI.,,82,AIF-C01,AWS Certified AI Practitioner,,"Tokens are the minimal textual units (words, subwords, or characters) that generative models read and produce. Text is tokenized into these units for training and inference, and key constraints like context window and usage are measured in tokens."
,,A company wants to assess the costs that are associated with using a large language model (LLM) to generate inferences. The company wants to use Amazon Bedrock to build generative AI applications. Which factor will drive the inference costs?,Number of tokens consumed,Temperature value,Amount of data used to train the LLM,Total training time,,,A,,0,0,0,1,1,,,,,,2.3,Describe AWS infrastructure and technologies for building generative AI applications.,,83,AIF-C01,AWS Certified AI Practitioner,,"Amazon Bedrock inference is generally billed by tokens processed (input and output). Temperature affects style/creativity but not cost directly, except indirectly through output length. Training data size and training time apply to training costs, not inference on a managed foundation model."
,,A company is using Amazon SageMaker Studio notebooks to build and train ML models. The company stores the data in an Amazon S3 bucket. The company needs to manage the ﬂow of data from Amazon S3 to SageMaker Studio notebooks. Which solution will meet this requirement?,Use Amazon Inspector to monitor SageMaker Studio.,Use Amazon Macie to monitor SageMaker Studio.,Conﬁgure SageMaker to use a VPC with an S3 endpoint.,Conﬁgure SageMaker to use S3 Glacier Deep Archive.,,,C,,0,0,0,1,2,,,,,,5.1,Explain methods to secure AI systems.,,84,AIF-C01,AWS Certified AI Practitioner,,"Configuring SageMaker to run in a VPC with an S3 VPC endpoint routes traffic privately and enables fine-grained control via security groups and endpoint policies. This manages and secures the data flow between S3 and SageMaker Studio without traversing the public internet. Inspector and Macie are for security assessment and data discovery, and Glacier Deep Archive is a storage class, none of which manage data flow paths."
,,A company has a foundation model (FM) that was customized by using Amazon Bedrock to answer customer queries about products. The company wants to validate the model's responses to new types of queries. The company needs to upload a new dataset that Amazon Bedrock can use for validation. Which AWS service meets these requirements?,Amazon S3,Amazon Elastic Block Store (Amazon EBS),Amazon Elastic File System (Amazon EFS),AWS Snowcone,,,A,,0,0,0,1,1,,,,,,2.3,Describe AWS infrastructure and technologies for building generative AI applications.,,85,AIF-C01,AWS Certified AI Practitioner,,"Amazon Bedrock expects datasets for evaluation and customization to reside in Amazon S3. S3 provides durable, scalable object storage and native integrations with Bedrock workflows. EBS/EFS are attached storage for compute, and Snowcone is for edge data transfer, not for Bedrock dataset ingestion."
,,Which prompting attack directly exposes the conﬁgured behavior of a large language model (LLM)?,Prompted persona switches,Exploiting friendliness and trust,Ignoring the prompt template,Extracting the prompt template,,,D,,0,0,0,1,1,,,,,,5.1,Explain methods to secure AI systems.,,86,AIF-C01,AWS Certified AI Practitioner,,"Extracting the prompt template is a prompt-leakage attack that exposes the hidden system instructions and configuration, directly revealing how the LLM is set to behave. Knowing the template enables adversaries to bypass controls or craft targeted jailbreaks. Other options manipulate behavior but do not directly disclose the underlying configuration."
,,A company wants to use Amazon Bedrock. The company needs to review which security aspects the company is responsible for when using Amazon Bedrock. Which security aspect will the company be responsible for?,Patching and updating the versions of Amazon Bedrock,Protecting the infrastructure that hosts Amazon Bedrock,Securing the company's data in transit and at rest,Provisioning Amazon Bedrock within the company network,,,C,,0,0,0,1,1,,,,,,5.1,Explain methods to secure AI systems.,,87,AIF-C01,AWS Certified AI Practitioner,,"With Amazon Bedrock (a managed service), AWS secures and patches the underlying service and infrastructure. Customers are responsible for protecting their data, including encryption in transit and at rest and proper IAM controls. Thus C is correct, while A, B, and D fall under AWS’s responsibilities for the managed service."
,,A social media company wants to use a large language model (LLM) to summarize messages. The company has chosen a few LLMs that are available on Amazon SageMaker JumpStart. The company wants to compare the generated output toxicity of these models. Which strategy gives the company the ability to evaluate the LLMs with the LEAST operational overhead?,Crowd-sourced evaluation,Automatic model evaluation,Model evaluation with human workers,Reinforcement learning from human feedback (RLHF),,,B,,0,0,1,0,1,,,,,,3.4,Describe methods to evaluate foundation model performance.,,88,AIF-C01,AWS Certified AI Practitioner,,"Automatic model evaluation provides built-in, programmatic toxicity scoring with minimal setup, delivering comparable metrics across models. Human-based approaches (crowd-sourced or worker-led) and RLHF introduce significant operational overhead and costs. Automated evaluation tools on AWS (e.g., Bedrock Model Evaluation, SageMaker capabilities) streamline toxicity assessments at scale."
,,"A company is testing the security of a foundation model (FM). During testing, the company wants to get around the safety features and make harmful content. Which security technique is this an example of?",Fuzzing training data to ﬁnd vulnerabilities,Denial of service (DoS),Penetration testing with authorization,Jailbreak,,,D,,0,0,0,1,1,,,,,,5.1,Explain methods to secure AI systems.,,89,AIF-C01,AWS Certified AI Practitioner,,"Jailbreaking uses adversarial prompts to bypass a foundation model’s guardrails and elicit harmful content. This is different from DoS (availability attacks) or fuzzing training data, and while it can occur during authorized pen testing, the specific technique is a jailbreak."
,,A company needs to use Amazon SageMaker for model training and inference. The company must comply with regulatory requirements to run SageMaker jobs in an isolated environment without internet access. Which solution will meet these requirements?,Run SageMaker training and inference by using SageMaker Experiments.,Run SageMaker training and Inference by using network Isolation.,Encrypt the data at rest by using encryption for SageMaker geospatial capabilities.,Associate appropriate AWS Identity and Access Management (IAM) roles with the SageMaker jobs.,,,B,,0,0,0,1,1,,,,,,5.1,Explain methods to secure AI systems.,,90,AIF-C01,AWS Certified AI Practitioner,,"Enabling SageMaker network isolation prevents training and inference containers from making outbound network calls, ensuring jobs run without internet access and reducing data exfiltration risk. This satisfies regulatory requirements for isolated environments. The other options (Experiments, encryption for geospatial, IAM roles) do not ensure network isolation from the internet."
,,An ML research team develops custom ML models. The model artifacts are shared with other teams for integration into products and services. The ML team retains the model training code and data. The ML team wants to build a mechanism that the ML team can use to audit models. Which solution should the ML team use when publishing the custom ML models?,Create documents with the relevant information. Store the documents in Amazon S3.,Use AWS AI Service Cards for transparency and understanding models.,Create Amazon SageMaker Model Cards with intended uses and training and inference details.,Create model training scripts. Commit the model training scripts to a Git repository.,,,C,,0,0,1,0,1,,,,,,4.2,Recognize the importance of transparent and explainable models.,,91,AIF-C01,AWS Certified AI Practitioner,,"Amazon SageMaker Model Cards provide a standardized, centralized record of a model’s intended use, training data, evaluation metrics, lineage, and deployment details, enabling governance and auditability. Option A is unstructured and harder to audit, B applies to AWS-managed AI services (not custom models), and D only tracks code, not the broader documentation needed for audits."
,,A software company builds tools for customers. The company wants to use AI to increase software development productivity. Which solution will meet these requirements?,Use a binary classiﬁcation model to generate code reviews.,Install code recommendation software in the company's developer tools.,Install a code forecasting tool to predict potential code issues.,Use a natural language processing (NLP) tool to generate code.,,,B,,0,0,1,0,2,1,,,,,2.2,Understand the capabilities and limitations of generative AI for solving business problems.,,92,AIF-C01,AWS Certified AI Practitioner,,"Code recommendation tools (generative code completion) integrate into IDEs to suggest context-aware code, measurably boosting developer productivity. Binary classification and forecasting do not generate code or immediate coding assistance. A generic NLP tool is not specialized for programming languages and lacks the domain-optimized capabilities of dedicated code recommender systems."
,,A retail store wants to predict the demand for a speciﬁc product for the next few weeks by using the Amazon SageMaker DeepAR forecasting algorithm. Which type of data will meet this requirement?,Text data,Image data,Time series data,Binary data,,,C,,0,0,0,1,1,,,,,,1.2,Identify practical use cases for AI.,,93,AIF-C01,AWS Certified AI Practitioner,,"Amazon SageMaker DeepAR is designed for probabilistic forecasting on time series data, where observations are indexed over time. Predicting product demand over the next weeks requires historical, time-ordered sales data. Text, image, or binary data do not capture temporal patterns needed for forecasting."
,,A large retail bank wants to develop an ML system to help the risk management team decide on loan allocations for different demographics. What must the bank do to develop an unbiased ML model?,Reduce the size of the training dataset.,Ensure that the ML model predictions are consistent with historical results.,Create a different ML model for each demographic group.,Measure class imbalance on the training dataset. Adapt the training process accordingly.,,,D,,0,0,0,1,1,,,,,,4.1,Explain the development of AI systems that are responsible.,,94,AIF-C01,AWS Certified AI Practitioner,,"Measuring class imbalance and adapting training (e.g., reweighting, resampling, or cost-sensitive learning) helps prevent models from favoring majority groups and reduces bias. Option D directly addresses a common source of unfairness in credit risk models. The other options either entrench historical bias, reduce data utility, or introduce discriminatory modeling."
,,Which prompting technique can protect against prompt injection attacks?,Adversarial prompting,Zero-shot prompting,Least-to-most prompting,Chain-of-thought prompting,,,A,,0,0,0,1,3,1,,,,,5.1,Explain methods to secure AI systems.,,95,AIF-C01,AWS Certified AI Practitioner,,"Adversarial prompting intentionally probes models with malicious or deceptive inputs to uncover and mitigate prompt injection and jailbreak weaknesses. This red-teaming strengthens guardrails, validation, and system prompts to resist manipulation. Zero-shot, least-to-most, and chain-of-thought aim at task performance, not security hardening."
,,A company has ﬁne-tuned a large language model (LLM) to answer questions for a help desk. The company wants to determine if the ﬁne- tuning has enhanced the model's accuracy. Which metric should the company use for the evaluation?,Precision,Time to ﬁrst token,F1 score,Word error rate,,,C,,0,0,1,0,1,1,,,,,3.4,Describe methods to evaluate foundation model performance.,,96,AIF-C01,AWS Certified AI Practitioner,,"F1 score balances precision and recall, capturing both correctness and completeness of answers—ideal for assessing QA accuracy after fine-tuning. Precision alone ignores coverage, and recall alone ignores correctness. Time to first token measures latency, and word error rate applies to speech recognition, not text QA."
,,A company is using Retrieval Augmented Generation (RAG) with Amazon Bedrock and Stable Diffusion to generate product images based on text descriptions. The results are often random and lack speciﬁc details. The company wants to increase the speciﬁcity of the generated images. Which solution meets these requirements?,Increase the number of generation steps.,Use the MASK_IMAGE_BLACK mask source option.,Increase the classiﬁer-free guidance (CFG) scale.,Increase the prompt strength.,,,C,,0,0,0,1,3,2,,,,,2.1,Explain the basic concepts of generative AI.,,97,AIF-C01,AWS Certified AI Practitioner,,"Classifier-free guidance (CFG) increases how strictly Stable Diffusion follows the text prompt, improving specificity and reducing randomness. Increasing steps mainly affects image quality/denoising, not adherence to prompt. MASK_IMAGE_BLACK is for inpainting workflows, and prompt strength is relevant to image-to-image guidance, not text-to-image specificity."
,,A company wants to implement a large language model (LLM) based chatbot to provide customer service agents with real-time contextual responses to customers' inquiries. The company will use the company's policies as the knowledge base. Which solution will meet these requirements MOST cost-effectively?,Retrain the LLM on the company policy data.,Fine-tune the LLM on the company policy data.,Implement Retrieval Augmented Generation (RAG) for in-context responses.,Use pre-training and data augmentation on the company policy data.,,,C,,0,0,0,1,1,,,,,,3.1,Describe design considerations for applications that use foundation models.,,98,AIF-C01,AWS Certified AI Practitioner,,"RAG retrieves up-to-date company policies at query time and provides grounded, in-context responses without modifying the base model. This avoids the high cost and complexity of retraining or fine-tuning while enabling real-time, accurate answers. Pre-training or data augmentation is unnecessary and cost-prohibitive for this use case."
,,A company wants to create a new solution by using AWS Glue. The company has minimal programming experience with AWS Glue. Which AWS service can help the company use AWS Glue?,Amazon Q Developer,AWS Conﬁg,Amazon Personalize,Amazon Comprehend,,,A,,0,0,1,0,1,,,,,,2.3,Describe AWS infrastructure and technologies for building generative AI applications.,,99,AIF-C01,AWS Certified AI Practitioner,,"Amazon Q Developer is a generative AI coding assistant that can generate and explain AWS Glue ETL code and help author jobs, aiding users with limited programming experience. AWS Config handles configuration compliance, Amazon Personalize builds recommendation systems, and Amazon Comprehend provides NLP, none of which assist with creating Glue solutions."
,,"A company is developing a mobile ML app that uses a phone's camera to diagnose and treat insect bites. The company wants to train an image classiﬁcation model by using a diverse dataset of insect bite photos from different genders, ethnicities, and geographic locations around the world. Which principle of responsible AI does the company demonstrate in this scenario?",Fairness,Explainability,Governance,Transparency,,,A,,0,0,0,1,1,,,,,,4.1,Explain the development of AI systems that are responsible.,,100,AIF-C01,AWS Certified AI Practitioner,,"Collecting a diverse dataset across genders, ethnicities, and regions addresses bias and promotes equitable model performance, which aligns with the fairness principle. Fairness seeks to reduce disparate impact by ensuring the model generalizes well to all user groups. Explainability, governance, and transparency focus on different aspects like interpretability, policies, and disclosures, not dataset diversity."
,,A company is developing an ML model to make loan approvals. The company must implement a solution to detect bias in the model. The company must also be able to explain the model's predictions. Which solution will meet these requirements?,Amazon SageMaker Clarify,Amazon SageMaker Data Wrangler,Amazon SageMaker Model Cards,AWS AI Service Cards,,,A,,0,0,0,1,1,,,,,,4.1,Explain the development of AI systems that are responsible.,,101,AIF-C01,AWS Certified AI Practitioner,,"Amazon SageMaker Clarify detects bias in datasets and models and provides feature attribution (e.g., SHAP) to explain predictions. Data Wrangler is for data preparation, Model Cards document models, and AWS AI Service Cards describe managed services, none of which perform bias detection and explainability."
,,A company has developed a generative text summarization model by using Amazon Bedrock. The company will use Amazon Bedrock automatic model evaluation capabilities. Which metric should the company use to evaluate the accuracy of the model?,Area Under the ROC Curve (AUC) score,F1 score,BERTScore,Real world knowledge (RWK) score,,,C,,0,0,1,0,1,,,,,,3.4,Describe methods to evaluate foundation model performance.,,102,AIF-C01,AWS Certified AI Practitioner,,"BERTScore evaluates semantic similarity between generated and reference summaries using contextual embeddings, making it well-suited for summarization accuracy. AUC and F1 are primarily for classification tasks, and RWK is not a standard summarization metric. Amazon Bedrock’s automatic evaluations include metrics like BERTScore for generative tasks."
,,"An AI practitioner wants to predict the classiﬁcation of ﬂowers based on petal length, petal width, sepal length, and sepal width. Which algorithm meets these requirements?",K-nearest neighbors (k-NN),K-mean,Autoregressive Integrated Moving Average (ARIMA),Linear regression,,,A,,0,0,1,0,2,3,,,,,1.1,Explain basic AI concepts and terminologies.,,103,AIF-C01,AWS Certified AI Practitioner,,"k-NN is a supervised classification algorithm that predicts a label based on the closest labeled examples in the feature space (e.g., petal/sepal measurements). K-means is unsupervised clustering, ARIMA is for time-series forecasting, and linear regression predicts continuous values rather than classes."
,,A company is using custom models in Amazon Bedrock for a generative AI application. The company wants to use a company managed encryption key to encrypt the model artifacts that the model customization jobs create. Which AWS service meets these requirements?,AWS Key Management Service (AWS KMS),Amazon Inspector,Amazon Macie,AWS Secrets Manager,,,A,,0,0,0,1,2,,,,,,5.1,Explain methods to secure AI systems.,,104,AIF-C01,AWS Certified AI Practitioner,,"AWS KMS is the AWS service for creating and managing customer managed encryption keys used to encrypt data at rest across AWS services, including Amazon Bedrock customization job artifacts. By specifying a KMS CMK, Bedrock encrypts model artifacts it writes (e.g., to S3). Inspector, Macie, and Secrets Manager do not manage encryption keys for such artifacts."
,,A company wants to use large language models (LLMs) to produce code from natural language code comments. Which LLM feature meets these requirements?,Text summarization,Text generation,Text completion,Text classiﬁcation,,,B,,0,0,0,1,1,,,,,,2.2,Understand the capabilities and limitations of generative AI for solving business problems.,,105,AIF-C01,AWS Certified AI Practitioner,,"Generating code from natural language comments is a generative task, which LLMs perform via text generation. Text summarization and classification do not create new code, and text completion is narrower, typically finishing partial text rather than producing full code from instructions. Therefore, text generation best matches the requirement."
,,A company is introducing a mobile app that helps users learn foreign languages. The app makes text more coherent by calling a large language model (LLM). The company collected a diverse dataset of text and supplemented the dataset with examples of more readable versions. The company wants the LLM output to resemble the provided examples. Which metric should the company use to assess whether the LLM meets these requirements?,Value of the loss function,Semantic robustness,Recall-Oriented Understudy for Gisting Evaluation (ROUGE) score,Latency of the text generation,,,C,,0,0,1,0,1,,,,,,3.4,Describe methods to evaluate foundation model performance.,,106,AIF-C01,AWS Certified AI Practitioner,,"ROUGE measures n-gram overlap between generated text and reference texts, making it suitable for assessing similarity to provided readable examples. Loss function is a training signal, not an external quality metric; semantic robustness is not a standard similarity metric; and latency measures speed, not output quality."
,,A company notices that its foundation model (FM) generates images that are unrelated to the prompts. The company wants to modify the prompt techniques to decrease unrelated images. Which solution meets these requirements?,Use zero-shot prompts.,Use negative prompts.,Use positive prompts.,Use ambiguous prompts.,,,B,,0,0,1,0,1,2,,,,,3.2,Choose effective prompt engineering techniques.,,107,AIF-C01,AWS Certified AI Practitioner,,"Negative prompts specify what the model should avoid generating, reducing irrelevant or unwanted elements in the output. This helps constrain image generation to be more aligned with the intended prompt. Zero-shot, generic positive, or ambiguous prompts do not provide explicit constraints and can lead to off-target results."
,,"A company wants to use a large language model (LLM) to generate concise, feature-speciﬁc descriptions for the company’s products. Which prompt engineering technique meets these requirements?","Create one prompt that covers all products. Edit the responses to make the responses more speciﬁc, concise, and tailored to each product.",Create prompts for each product category that highlight the key features. Include the desired output format and length for each prompt response.,Include a diverse range of product features in each prompt to generate creative and unique descriptions.,"Provide detailed, product-speciﬁc prompts to ensure precise and customized descriptions.",,,B,,0,0,1,0,1,,,,,,3.2,Choose effective prompt engineering techniques.,,108,AIF-C01,AWS Certified AI Practitioner,,"Option B uses category-specific prompts that emphasize key features and specify output format and length, which are core prompt-engineering best practices for consistency and conciseness. It scales better than per-product prompts (D), avoids unnecessary creativity that can reduce conciseness (C), and eliminates manual post-editing (A)."
,,A company is developing an ML model to predict customer churn. The model performs well on the training dataset but does not accurately predict churn for new data. Which solution will resolve this issue?,Decrease the regularization parameter to increase model complexity.,Increase the regularization parameter to decrease model complexity.,Add more features to the input data.,Train the model for more epochs.,,,B,,0,0,1,0,2,1,,,,,1.1,Explain basic AI concepts and terminologies.,,109,AIF-C01,AWS Certified AI Practitioner,,"Good training performance but poor performance on new data indicates overfitting. Increasing the regularization parameter penalizes large weights, reduces model complexity, and improves generalization. Decreasing regularization, adding features, or training longer often exacerbates overfitting."
,,A company is implementing intelligent agents to provide conversational search experiences for its customers. The company needs a database service that will support storage and queries of embeddings from a generative AI model as vectors in the database. Which AWS service will meet these requirements?,Amazon Athena,Amazon Aurora PostgreSQL,Amazon Redshift,Amazon EMR,,,B,,0,0,1,0,1,1,,,,,2.3,Describe AWS infrastructure and technologies for building generative AI applications.,,110,AIF-C01,AWS Certified AI Practitioner,,"Amazon Aurora PostgreSQL supports the pgvector extension, enabling storage of embeddings and efficient similarity search (kNN/cosine distance) directly in the database. Amazon Athena and EMR are not vector databases, and Amazon Redshift is a data warehouse not primarily used for operational vector search in this context. Therefore, Aurora PostgreSQL best meets the requirement."
,,"A ﬁnancial institution is building an AI solution to make loan approval decisions by using a foundation model (FM). For security and audit purposes, the company needs the AI solution's decisions to be explainable. Which factor relates to the explainability of the AI solution's decisions?",Model complexity,Training time,Number of hyperparameters,Deployment time,,,A,,0,0,1,0,1,1,,,,,4.2,Recognize the importance of transparent and explainable models.,,111,AIF-C01,AWS Certified AI Practitioner,,"Explainability typically decreases as model complexity increases; simpler models are more interpretable, while complex FMs are often black boxes. Training time and deployment time do not affect interpretability, and the number of hyperparameters is not a direct proxy for explainability. Therefore, model complexity is the key factor related to explainability."
,,A pharmaceutical company wants to analyze user reviews of new medications and provide a concise overview for each medication. Which solution meets these requirements?,Create a time-series forecasting model to analyze the medication reviews by using Amazon Personalize.,Create medication review summaries by using Amazon Bedrock large language models (LLMs).,Create a classiﬁcation model that categorizes medications into different groups by using Amazon SageMaker.,Create medication review summaries by using Amazon Rekognition.,,,B,,0,0,1,0,2,,,,,,2.2,Understand the capabilities and limitations of generative AI for solving business problems.,,112,AIF-C01,AWS Certified AI Practitioner,,"Amazon Bedrock LLMs can perform text summarization to produce concise overviews of medication reviews. Amazon Personalize focuses on recommendations (not summarization), SageMaker classification groups items rather than summarizing text, and Rekognition is for image/video analysis, not text."
,,A company wants to build a lead prioritization application for its employees to contact potential customers. The application must give employees the ability to view and adjust the weights assigned to different variables in the model based on domain knowledge and expertise. Which ML model type meets these requirements?,Logistic regression model,Deep learning model built on principal components,K-nearest neighbors (k-NN) model,Neural network,,,A,,0,0,1,0,1,,,,,,1.1,Explain basic AI concepts and terminologies.,,113,AIF-C01,AWS Certified AI Practitioner,,"Logistic regression provides explicit, interpretable feature coefficients (weights) that can be viewed and adjusted to reflect domain expertise. Neural networks and deep learning models are less interpretable and not designed for manual weight tuning by end users. k-NN does not learn feature weights in the same way and lacks model parameters to inspect."
,,HOTSPOT - A company wants to build an ML application. Select and order the correct steps from the following list to develop a well-architected ML workload. Each step should be selected one time. Answer : Question 115 ( Exam A) Which strategy will determine if a foundation model (FM) effectively meets business objectives?,Evaluate the model's performance on benchmark datasets.,Analyze the model's architecture and hyperparameters.,Assess the model's alignment with speciﬁc use cases.,Measure the computational resources required for model deployment.,,,C,,0,,,1,,,,,,,3.4,Describe methods to evaluate foundation model performance.,,114,AIF-C01,AWS Certified AI Practitioner,,"Assessing alignment with specific use cases directly measures whether the model delivers value for the intended business outcomes. Benchmarks, architecture analysis, and resource measurements are informative, but they do not guarantee that the model solves the target problem effectively in context."
,,A company needs to train an ML model to classify images of different types of animals. The company has a large dataset of labeled images and will not label more data. Which type of learning should the company use to train the model?,Supervised learning,Unsupervised learning,Reinforcement learning,Active learning,,,A,,0,0,1,0,1,2,,,,,1.1,Explain basic AI concepts and terminologies.,,116,AIF-C01,AWS Certified AI Practitioner,,"Supervised learning uses labeled datasets to train classification models that map inputs (images) to known categories (animal types). This scenario fits supervised learning because the data is already labeled and no additional labeling will occur; unsupervised learning lacks labels, reinforcement learning uses reward signals, and active learning involves acquiring new labels iteratively."
,,Which phase of the ML lifecycle determines compliance and regulatory requirements?,Feature engineering,Model training,Data collection,Business goal identiﬁcation,,,D,,0,0,0,1,2,1,,,,,1.3,Describe the ML development lifecycle.,,117,AIF-C01,AWS Certified AI Practitioner,,"Compliance and regulatory requirements are identified during business goal identification to ensure the solution aligns with legal and organizational constraints from the outset. Establishing these requirements early guides data handling, model choices, and deployment practices. Later phases like feature engineering or training implement within those constraints rather than defining them."
,,A food service company wants to develop an ML model to help decrease daily food waste and increase sales revenue. The company needs to continuously improve the model's accuracy. Which solution meets these requirements?,Use Amazon SageMaker and iterate with newer data.,Use Amazon Personalize and iterate with historical data.,Use Amazon CloudWatch to analyze customer orders.,Use Amazon Rekognition to optimize the model.,,,A,,0,0,0,1,1,,,,,,1.3,Describe the ML development lifecycle.,,118,AIF-C01,AWS Certified AI Practitioner,,"Amazon SageMaker supports building, training, deploying, monitoring, and retraining models, enabling continuous accuracy improvement by iterating with new data. Amazon Personalize targets recommendation use cases and focuses on historical interaction data, while CloudWatch is for monitoring, not model training, and Rekognition is for image/video analysis, not this forecasting problem."
,,A company has developed an ML model to predict real estate sale prices. The company wants to deploy the model to make predictions without managing servers or infrastructure. Which solution meets these requirements?,Deploy the model on an Amazon EC2 instance.,Deploy the model on an Amazon Elastic Kubernetes Service (Amazon EKS) cluster.,Deploy the model by using Amazon CloudFront with an Amazon S3 integration.,Deploy the model by using an Amazon SageMaker endpoint.,,,D,,0,0,0,1,1,,,,,,1.3,Describe the ML development lifecycle.,,119,AIF-C01,AWS Certified AI Practitioner,,"Amazon SageMaker endpoints provide fully managed, scalable real-time inference without managing servers or infrastructure. EC2 and EKS require managing instances or clusters, and CloudFront with S3 serves static content rather than hosting model inference. Therefore, SageMaker endpoints best meet the serverless deployment requirement."
,,"A company wants to develop an AI application to help its employees check open customer claims, identify details for a speciﬁc claim, and access documents for a claim. Which solution meets these requirements?",Use Agents for Amazon Bedrock with Amazon Fraud Detector to build the application.,Use Agents for Amazon Bedrock with Amazon Bedrock knowledge bases to build the application.,Use Amazon Personalize with Amazon Bedrock knowledge bases to build the application.,Use Amazon SageMaker to build the application by training a new ML model.,,,B,,0,0,1,0,1,,,,,,2.3,Describe AWS infrastructure and technologies for building generative AI applications.,,120,AIF-C01,AWS Certified AI Practitioner,,"Agents for Amazon Bedrock can orchestrate multi-step tasks (e.g., checking claim status, fetching details) and call tools/APIs, while Amazon Bedrock knowledge bases provide RAG to index and retrieve claim documents. Amazon Fraud Detector is unrelated to claim lookup, Amazon Personalize is for recommendations, and training a new model in SageMaker is unnecessary for this retrieval-and-orchestration use case."
,,A manufacturing company uses AI to inspect products and ﬁnd any damages or defects. Which type of AI application is the company using?,Recommendation system,Natural language processing (NLP),Computer vision,Image processing,,,C,,0,0,0,1,3,,,,,,1.2,Identify practical use cases for AI.,,121,AIF-C01,AWS Certified AI Practitioner,,"Inspecting products for defects from images is a perception task that falls under computer vision. Computer vision models classify, detect, and segment objects or anomalies in images and video. Image processing mainly transforms images (e.g., filtering), while recommendation systems and NLP are unrelated to visual inspection."
,,A company wants to create an ML model to predict customer satisfaction. The company needs fully automated model tuning. Which AWS service meets these requirements?,Amazon Personalize,Amazon SageMaker,Amazon Athena,Amazon Comprehend,,,B,,0,0,1,0,3,,,,,,1.3,Describe the ML development lifecycle.,,122,AIF-C01,AWS Certified AI Practitioner,,"Amazon SageMaker offers automated model building and tuning via Autopilot and Hyperparameter Tuning Jobs, meeting the need for fully automated model tuning. Amazon Personalize is specialized for recommendations, Athena is for querying data, and Comprehend is for NLP tasks, none of which provide general-purpose automated model tuning."
,,Which technique can a company use to lower bias and toxicity in generative AI applications during the post-processing ML lifecycle?,Human-in-the-loop,Data augmentation,Feature engineering,Adversarial training,,,A,,0,0,1,0,1,2,,,,,4.1,Explain the development of AI systems that are responsible.,,123,AIF-C01,AWS Certified AI Practitioner,,"Human-in-the-loop enables human review and moderation of model outputs before they reach end users, reducing bias and toxic content at the post-processing stage. It provides governance controls and safety checks that complement automated filters. This aligns with responsible AI practices for generative applications."
,,"A bank has ﬁne-tuned a large language model (LLM) to expedite the loan approval process. During an external audit of the model, the company discovered that the model was approving loans at a faster pace for a speciﬁc demographic than for other demographics. How should the bank ﬁx this issue MOST cost-effectively?",Include more diverse training data. Fine-tune the model again by using the new data.,Use Retrieval Augmented Generation (RAG) with the ﬁne-tuned model.,Use AWS Trusted Advisor checks to eliminate bias.,Pre-train a new LLM with more diverse training data.,,,A,,0,0,1,0,1,,,,,,4.1,Explain the development of AI systems that are responsible.,,124,AIF-C01,AWS Certified AI Practitioner,,"The observed disparity indicates bias likely introduced by unrepresentative fine-tuning data; augmenting with diverse data and re-fine-tuning directly targets the issue cost-effectively. Pre-training a new LLM is far more expensive, RAG does not change the model’s decision policy, and AWS Trusted Advisor does not assess or correct model fairness."
,,HOTSPOT - A company has developed a large language model (LLM) and wants to make the LLM available to multiple internal teams. The company needs to select the appropriate inference mode for each team. Select the correct inference mode from the following list for each use case. Each inference mode should be selected one or more times. Answer : Question 126 ( Exam A) A company needs to log all requests made to its Amazon Bedrock API. The company must retain the logs securely for 5 years at the lowest possible cost. Which combination of AWS service and storage class meets these requirements? (Choose two.),AWS CloudTrail,Amazon CloudWatch,AWS Audit Manager,Amazon S3 Intelligent-Tiering,Amazon S3 Standard,,"A, D",1,0,,,1,,,,,,,5.1,Explain methods to secure AI systems.,,125,AIF-C01,AWS Certified AI Practitioner,,"AWS CloudTrail records all Amazon Bedrock API calls, providing the required audit logs. Storing those logs in Amazon S3 Intelligent-Tiering minimizes long-term storage costs while maintaining durability and secure retention for 5 years; CloudWatch and Audit Manager don’t serve as primary API call logs, and S3 Standard is more expensive for long-term storage."
,,An ecommerce company wants to improve search engine recommendations by customizing the results for each user of the company’s ecommerce platform. Which AWS service meets these requirements?,Amazon Personalize,Amazon Kendra,Amazon Rekognition,Amazon Transcribe,,,A,,0,0,0,1,1,,,,,,1.2,Identify practical use cases for AI.,,127,AIF-C01,AWS Certified AI Practitioner,,"Amazon Personalize is designed to deliver real-time, individualized recommendations and personalized search ranking for users. Amazon Kendra is for enterprise document search, Rekognition is for image and video analysis, and Transcribe is for speech-to-text, none of which address personalized ecommerce recommendations."
,,"A hospital is developing an AI system to assist doctors in diagnosing diseases based on patient records and medical images. To comply with regulations, the sensitive patient data must not leave the country the data is located in. Which data governance strategy will ensure compliance and protect patient privacy?",Data residency,Data quality,Data discoverability,Data enrichment,,,A,,0,0,0,1,2,,,,,,5.2,Recognize governance and compliance regulations for AI systems.,,128,AIF-C01,AWS Certified AI Practitioner,,"Data residency mandates that data remains within specified geographic boundaries to meet legal and regulatory requirements. For healthcare patient data, keeping storage and processing in-country helps comply with local laws and health privacy regulations and reduces risk of unlawful cross-border transfer. The other options address quality, cataloging, or enhancement, not geographic compliance."
,,A company needs to monitor the performance of its ML systems by using a highly scalable AWS service. Which AWS service meets these requirements?,Amazon CloudWatch,AWS CloudTrail,AWS Trusted Advisor,AWS Conﬁg,,,A,,0,0,0,1,3,,,,,,1.3,Describe the ML development lifecycle.,,129,AIF-C01,AWS Certified AI Practitioner,,"Amazon CloudWatch is the scalable AWS service for monitoring metrics, logs, dashboards, and setting alarms for ML system performance. CloudTrail focuses on API auditing, Trusted Advisor gives best-practice recommendations, and AWS Config tracks configuration changes—none of which provide primary performance monitoring."
,,An AI practitioner is developing a prompt for an Amazon Titan model. The model is hosted on Amazon Bedrock. The AI practitioner is using the model to solve numerical reasoning challenges. The AI practitioner adds the following phrase to the end of the prompt: “Ask the model to show its work by explaining its reasoning step by step.” Which prompt engineering technique is the AI practitioner using?,Chain-of-thought prompting,Prompt injection,Few-shot prompting,Prompt templating,,,A,,0,0,0,1,2,,,,,,3.2,Choose effective prompt engineering techniques.,,130,AIF-C01,AWS Certified AI Practitioner,,"Requesting the model to explain its reasoning step by step is chain-of-thought prompting, which elicits intermediate reasoning steps to improve numerical and logical problem solving. It is not few-shot prompting (no examples are provided), not prompt templating (no variable placeholders), and not prompt injection (which is a security attack pattern)."
,,Which AWS service makes foundation models (FMs) available to help users build and scale generative AI applications?,Amazon Q Developer,Amazon Bedrock,Amazon Kendra,Amazon Comprehend,,,B,,0,0,0,1,1,,,,,,2.3,Describe AWS infrastructure and technologies for building generative AI applications.,,131,AIF-C01,AWS Certified AI Practitioner,,"Amazon Bedrock is a fully managed service that provides API access to multiple foundation models to build and scale generative AI applications. Amazon Q Developer is a coding assistant, Amazon Kendra is enterprise search, and Amazon Comprehend is NLP for text analytics—not services for hosting or accessing FMs."
,,A company is building a mobile app for users who have a visual impairment. The app must be able to hear what users say and provide voice responses. Which solution will meet these requirements?,Use a deep learning neural network to perform speech recognition.,Build ML models to search for patterns in numeric data.,Use generative AI summarization to generate human-like text.,Build custom models for image classiﬁcation and recognition.,,,A,,0,0,0,1,2,,,,,,1.2,Identify practical use cases for AI.,,132,AIF-C01,AWS Certified AI Practitioner,,"Voice-driven apps use automatic speech recognition, typically powered by deep learning neural networks, to convert spoken input into text. Combined with text-to-speech for replies, this meets the app’s need to hear users and respond by voice. The other options do not address speech processing."
,,A company wants to enhance response quality for a large language model (LLM) for complex problem-solving tasks. The tasks require detailed reasoning and a step-by-step explanation process. Which prompt engineering technique meets these requirements?,Few-shot prompting,Zero-shot prompting,Directional stimulus prompting,Chain-of-thought prompting,,,D,,0,0,0,1,1,,,,,,3.2,Choose effective prompt engineering techniques.,,133,AIF-C01,AWS Certified AI Practitioner,,"Chain-of-thought prompting explicitly instructs the model to reason through problems step by step, improving performance on complex tasks. Few-shot and zero-shot prompting provide examples or no examples but don't inherently elicit detailed reasoning. Directional stimulus prompting guides style or direction, but chain-of-thought is specifically designed for structured, stepwise explanations."
,,A company wants to keep its foundation model (FM) relevant by using the most recent data. The company wants to implement a model training strategy that includes regular updates to the FM. Which solution meets these requirements?,Batch learning,Continuous pre-training,Static training,Latent training,,,B,,0,0,0,1,1,,,,,,3.3,Describe the training and fine-tuning process for foundation models.,,134,AIF-C01,AWS Certified AI Practitioner,,"Continuous pre-training incrementally updates a foundation model with new data to keep it current without restarting from scratch. This approach maintains model relevance and adapts to distribution shifts. Batch or static training are periodic or one-time processes, and 'latent training' is not a standard approach."
,,HOTSPOT - A company wants to develop ML applications to improve business operations and eﬃciency. Select the correct ML paradigm from the following list for each use case. Each ML paradigm should be selected one or more times. Answer : Question 136 ( Exam A) Which option is a characteristic of AI governance frameworks for building trust and deploying human-centered AI technologies?,Expanding initiatives across business units to create long-term business value,"Ensuring alignment with business standards, revenue goals, and stakeholder expectations",Overcoming challenges to drive business transformation and growth,"Developing policies and guidelines for data, transparency, responsible AI, and compliance",,,D,,0,,,1,,,,,,,5.2,Recognize governance and compliance regulations for AI systems.,,135,AIF-C01,AWS Certified AI Practitioner,,"AI governance frameworks center on establishing policies and guidelines for data management, transparency, responsible AI practices, and regulatory compliance. These measures build trust and ensure human-centered AI deployment. Option D best reflects these core elements of governance frameworks."
,,An ecommerce company is using a generative AI chatbot to respond to customer inquiries. The company wants to measure the ﬁnancial effect of the chatbot on the company’s operations. Which metric should the company use?,Number of customer inquiries handled,Cost of training AI models,Cost for each customer conversation,Average handled time (AHT),,,C,,0,0,1,0,1,1,,,,,3.4,Describe methods to evaluate foundation model performance.,,137,AIF-C01,AWS Certified AI Practitioner,,"Cost for each customer conversation directly quantifies operational expense per interaction, enabling clear measurement of financial impact and ROI of the chatbot. Number of inquiries handled and AHT are efficiency metrics, not direct cost measures. Cost of training is a one-time or periodic expense and does not reflect ongoing operational impact per conversation."
,,A company wants to ﬁnd groups for its customers based on the customers’ demographics and buying patterns. Which algorithm should the company use to meet this requirement?,K-nearest neighbors (k-NN),K-means,Decision tree,Support vector machine,,,B,,0,0,1,0,1,4,,,,,1.1,Explain basic AI concepts and terminologies.,,138,AIF-C01,AWS Certified AI Practitioner,,"K-means is an unsupervised clustering algorithm that partitions unlabeled data into k groups based on feature similarity, ideal for finding customer segments. k-NN, decision trees, and SVM are supervised methods requiring labeled outcomes, so they are not suited for discovering inherent groups in data without labels."
,,A company’s large language model (LLM) is experiencing hallucinations. How can the company decrease hallucinations?,Set up Agents for Amazon Bedrock to supervise the model training.,Use data pre-processing and remove any data that causes hallucinations.,Decrease the temperature inference parameter for the model.,Use a foundation model (FM) that is trained to not hallucinate.,,,C,,0,0,0,1,1,,,,,,2.1,Explain the basic concepts of generative AI.,,139,AIF-C01,AWS Certified AI Practitioner,,"Lowering temperature reduces randomness in token selection, making outputs more conservative and less prone to hallucinations. Hallucinations often arise at inference, not purely from training data, so preprocessing is insufficient and 'non-hallucinating' FMs don’t exist. Agents for Bedrock don’t supervise model training and won’t directly reduce hallucinations from the base generation process."
,,"A company is using a large language model (LLM) on Amazon Bedrock to build a chatbot. The chatbot processes customer support requests. To resolve a request, the customer and the chatbot must interact a few times. Which solution gives the LLM the ability to use content from previous customer messages?",Turn on model invocation logging to collect messages.,Add messages to the model prompt.,Use Amazon Personalize to save conversation history.,Use Provisioned Throughput for the LLM.,,,B,,0,0,1,0,1,2,,,,,3.2,Choose effective prompt engineering techniques.,,140,AIF-C01,AWS Certified AI Practitioner,,"LLM calls are stateless, so past messages must be added to the current prompt for the model to use them. Including prior user and assistant messages in the prompt provides conversational context within the model’s context window. Logging, Amazon Personalize, or Provisioned Throughput do not supply past content to the model at inference time."
,,A company’s employees provide product descriptions and recommendations to customers when customers call the customer service center. These recommendations are based on where the customers are located. The company wants to use foundation models (FMs) to automate this process. Which AWS service meets these requirements?,Amazon Macie,Amazon Transcribe,Amazon Bedrock,Amazon Textract,,,C,,0,0,1,0,1,,,,,,2.3,Describe AWS infrastructure and technologies for building generative AI applications.,,141,AIF-C01,AWS Certified AI Practitioner,,"Amazon Bedrock provides managed access to multiple foundation models for text generation, enabling automated product descriptions and recommendations with contextual inputs like customer location. Amazon Macie is for data security, Amazon Transcribe is for speech-to-text, and Amazon Textract is for document OCR, none of which deliver FM-driven text generation."
,,A company wants to upload customer service email messages to Amazon S3 to develop a business analysis application. The messages sometimes contain sensitive data. The company wants to receive an alert every time sensitive information is found. Which solution fully automates the sensitive information detection process with the LEAST development effort?,Conﬁgure Amazon Macie to detect sensitive information in the documents that are uploaded to Amazon S3.,Use Amazon SageMaker endpoints to deploy a large language model (LLM) to redact sensitive data.,Develop multiple regex patterns to detect sensitive data. Expose the regex patterns on an Amazon SageMaker notebook.,Ask the customers to avoid sharing sensitive information in their email messages.,,,A,,0,0,1,0,2,,,,,,5.1,Explain methods to secure AI systems.,,142,AIF-C01,AWS Certified AI Practitioner,,"Amazon Macie is a fully managed service that automatically discovers and classifies sensitive data in Amazon S3 and can generate alerts with minimal setup, satisfying the least development effort requirement. Using an LLM or custom regex rules requires significant development and ongoing maintenance and is less reliable. Asking customers not to include sensitive data is neither enforceable nor automated."
,,HOTSPOT - A company is training its employees on how to structure prompts for foundation models. Select the correct prompt engineering technique from the following list for each prompt template. Each prompt engineering technique should be selected one time. Answer : Question 144 ( Exam A) HOTSPOT - A company is using a generative AI model to develop a digital assistant. The model’s responses occasionally include undesirable and potentially harmful content. Select the correct Amazon Bedrock ﬁlter policy from the following list for each mitigation action. Each ﬁlter policy should be selected one time. Answer : Question 145 ( Exam A) Which option is a beneﬁt of using Amazon SageMaker Model Cards to document AI models?,Providing a visually appealing summary of a mode’s capabilities.,"Standardizing information about a model’s purpose, performance, and limitations.",Reducing the overall computational requirements of a model.,Physically storing models for archival purposes.,,,B,,0,,,1,,,,,,,4.2,Recognize the importance of transparent and explainable models.,,143,AIF-C01,AWS Certified AI Practitioner,,"Amazon SageMaker Model Cards provide a standardized, structured way to document a model’s purpose, performance metrics, datasets, and known limitations. This improves transparency and governance across teams and stakeholders. They do not optimize computation or serve as physical storage for models."
,,What does an F1 score measure in the context of foundation model (FM) performance?,Model precision and recall,Model speed in generating responses,Financial cost of operating the model,Energy eﬃciency of the model’s computations,,,A,,0,0,0,1,1,,,,,,3.4,Describe methods to evaluate foundation model performance.,,146,AIF-C01,AWS Certified AI Practitioner,,"The F1 score is the harmonic mean of precision and recall, combining both into a single metric. It balances false positives and false negatives, which is valuable especially with class imbalance. Therefore, it measures model precision and recall together."
,,A company deployed an AI/ML solution to help customer service agents respond to frequently asked questions. The questions can change over time. The company wants to give customer service agents the ability to ask questions and receive automatically generated answers to common customer questions. Which strategy will meet these requirements MOST cost-effectively?,Fine-tune the model regularly.,Train the model by using context data.,Pre-train and benchmark the model by using context data.,Use Retrieval Augmented Generation (RAG) with prompt engineering techniques.,,,D,,0,0,0,1,1,,,,,,3.1,Describe design considerations for applications that use foundation models.,,147,AIF-C01,AWS Certified AI Practitioner,,"RAG allows the model to fetch up-to-date answers from a knowledge base at inference time, avoiding repeated fine-tuning as FAQs change. This is more cost-effective and faster to maintain than fine-tuning or pre-training. Prompt engineering further improves answer quality by structuring queries and grounding responses to retrieved context."
,,A company built an AI-powered resume screening system. The company used a large dataset to train the model. The dataset contained resumes that were not representative of all demographics. Which core dimension of responsible AI does this scenario present?,Fairness,Explainability,Privacy and security,Transparency,,,A,,0,0,1,0,1,,,,,,4.1,Explain the development of AI systems that are responsible.,,148,AIF-C01,AWS Certified AI Practitioner,,"A non-representative training dataset can introduce bias, leading to unequal model performance across demographic groups. Fairness focuses on reducing such biases and ensuring equitable outcomes. Explainability, privacy/security, and transparency are important but do not directly address demographic representativeness."
,,A global ﬁnancial company has developed an ML application to analyze stock market data and provide stock market trends. The company wants to continuously monitor the application development phases and to ensure that company policies and industry regulations are followed. Which AWS services will help the company assess compliance requirements? (Choose two.),AWS Audit Manager,AWS Conﬁg,Amazon Inspector,Amazon CloudWatch,AWS CloudTrail,,"A, B",1,0,0,1,0,1,6,,,,,5.2,Recognize governance and compliance regulations for AI systems.,,149,AIF-C01,AWS Certified AI Practitioner,,"AWS Audit Manager helps map controls to regulatory frameworks and automates evidence collection for audits, directly supporting compliance assessments. AWS Config continuously evaluates resource configurations against rules and conformance packs to check compliance with policies. Inspector, CloudWatch, and CloudTrail provide security scanning, monitoring, and logging but do not directly assess compliance against policies/regulations."
,,A company wants to improve the accuracy of the responses from a generative AI application. The application uses a foundation model (FM) on Amazon Bedrock. Which solution meets these requirements MOST cost-effectively?,Fine-tune the FM.,Retrain the FM.,Train a new FM.,Use prompt engineering.,,,D,,0,0,0,1,1,,,,,,3.2,Choose effective prompt engineering techniques.,,150,AIF-C01,AWS Certified AI Practitioner,,"Prompt engineering is the most cost-effective way to improve response accuracy by refining instructions, adding few-shot examples, and constraining output format. Fine-tuning or retraining requires significant labeled data, compute, and time, increasing cost. For many accuracy issues, better prompts on Amazon Bedrock can yield immediate improvements without model training."
,,A company wants to identify harmful language in the comments section of social media posts by using an ML model. The company will not use labeled data to train the model. Which strategy should the company use to identify harmful language?,Use Amazon Rekognition moderation.,Use Amazon Comprehend toxicity detection.,Use Amazon SageMaker built-in algorithms to train the model.,Use Amazon Polly to monitor comments.,,,B,,0,0,1,0,2,1,,,,0,1.2,Identify practical use cases for AI.,,151,AIF-C01,AWS Certified AI Practitioner,,"Amazon Comprehend toxicity detection is a pre-trained NLP capability that identifies harmful language without needing labeled training data. Amazon Rekognition is for image/video moderation, SageMaker built-in algorithms would require labeled data to train, and Amazon Polly is for text-to-speech, not content moderation."
,,A media company wants to analyze viewer behavior and demographics to recommend personalized content. The company wants to deploy a customized ML model in its production environment. The company also wants to observe if the model quality drifts over time. Which AWS service or feature meets these requirements?,Amazon Rekognition,Amazon SageMaker Clarify,Amazon Comprehend,Amazon SageMaker Model Monitor,,,D,,0,0,1,0,2,1,,,,,1.3,Describe the ML development lifecycle.,,152,AIF-C01,AWS Certified AI Practitioner,,"Amazon SageMaker Model Monitor continuously captures inference data from deployed models and compares it to a baseline to detect data and model quality drift. It generates metrics and alerts to track degradation over time in production. Other options are managed AI APIs or bias/explainability tools, not end-to-end drift monitoring for custom models."
,,A company is deploying AI/ML models by using AWS services. The company wants to offer transparency into the models’ decision-making processes and provide explanations for the model outputs. Which AWS service or feature meets these requirements?,Amazon SageMaker Model Cards,Amazon Rekognition,Amazon Comprehend,Amazon Lex,,,A,,0,0,0,1,1,,,,,,4.2,Recognize the importance of transparent and explainable models.,,153,AIF-C01,AWS Certified AI Practitioner,,"Amazon SageMaker Model Cards provide standardized, shareable documentation that captures model purpose, training data, evaluation metrics, risks, and governance details to increase transparency. They help stakeholders understand how the model was developed and how to interpret its outputs. Rekognition, Comprehend, and Lex are application services and do not provide model explainability documentation."
,,A manufacturing company wants to create product descriptions in multiple languages. Which AWS service will automate this task?,Amazon Translate,Amazon Transcribe,Amazon Kendra,Amazon Polly,,,A,,0,0,0,1,1,,,,,,1.2,Identify practical use cases for AI.,,154,AIF-C01,AWS Certified AI Practitioner,,"Amazon Translate provides neural machine translation to automatically convert product descriptions into multiple languages at scale. Amazon Transcribe converts speech to text, Kendra is enterprise search, and Polly turns text into lifelike speech—none perform text translation."
,,HOTSPOT - A company wants more customized responses to its generative AI models’ prompts. Select the correct customization methodology from the following list for each use case. Each use case should be selected one time. Answer : Question 156 ( Exam A) Which AWS feature records details about ML instance data for governance and reporting?,Amazon SageMaker Model Cards,Amazon SageMaker Debugger,Amazon SageMaker Model Monitor,Amazon SageMaker JumpStart,,,A,,0,,,1,,,,,,,5.2,Recognize governance and compliance regulations for AI systems.,,155,AIF-C01,AWS Certified AI Practitioner,,"Amazon SageMaker Model Cards document model details, datasets, environment/instance information, metrics, and intended use to support governance and reporting. SageMaker Debugger is for training instrumentation, and Model Monitor is for production data/quality drift, while JumpStart provides prebuilt models and solutions—not governance documentation."
,,A ﬁnancial company is using ML to help with some of the company’s tasks. Which option is a use of generative AI models?,Summarizing customer complaints,Classifying customers based on product usage,Segmenting customers based on type of investments,Forecasting revenue for certain products,,,A,,0,0,0,1,1,,,,,,2.2,Understand the capabilities and limitations of generative AI for solving business problems.,,157,AIF-C01,AWS Certified AI Practitioner,,"LLMs perform generative tasks like abstractive text summarization, making them well-suited to summarize customer complaints. Classification, segmentation, and forecasting are discriminative or predictive tasks typically handled by traditional ML techniques rather than generative models. Therefore, summarization is the correct generative AI use case."
,,"A medical company wants to develop an AI application that can access structured patient records, extract relevant information, and generate concise summaries. Which solution will meet these requirements?",Use Amazon Comprehend Medical to extract relevant medical entities and relationships. Apply rule-based logic to structure and format summaries.,Use Amazon Personalize to analyze patient engagement patterns. Integrate the output with a general purpose text summarization tool.,Use Amazon Textract to convert scanned documents into digital text. Design a keyword extraction system to generate summaries.,Implement Amazon Kendra to provide a searchable index for medical records. Use a template-based system to format summaries.,,,A,,0,0,0,1,3,1,,,,,1.2,Identify practical use cases for AI.,,158,AIF-C01,AWS Certified AI Practitioner,,"Amazon Comprehend Medical is purpose-built to extract medical entities and relationships from clinical text, enabling structured summaries with simple rule-based formatting. Amazon Personalize is for recommendations, not text extraction; Amazon Textract performs OCR but doesn't identify medical entities; and Amazon Kendra provides search, not entity extraction or summarization."
,,Which option describes embeddings in the context of AI?,A method for compressing large datasets,An encryption method for securing sensitive data,A method for visualizing high-dimensional data,A numerical method for data representation in a reduced dimensionality space,,,D,,0,0,1,0,1,,,,,,1.1,Explain basic AI concepts and terminologies.,,159,AIF-C01,AWS Certified AI Practitioner,,"Embeddings map items (e.g., words, documents, images) into numerical vectors in a lower-dimensional continuous space. This reduced representation preserves semantic relationships, enabling similarity search and downstream ML tasks. Therefore, D accurately describes embeddings."
,,"A company is building an AI application to summarize books of varying lengths. During testing, the application fails to summarize some books. Why does the application fail to summarize some books?",The temperature is set too high.,The selected model does not support ﬁne-tuning.,The Top P value is too high.,The input tokens exceed the model’s context size.,,,D,,0,0,0,1,1,,,,,,3.1,Describe design considerations for applications that use foundation models.,,160,AIF-C01,AWS Certified AI Practitioner,,"Foundation models have a fixed context window (maximum input tokens). If the book text plus prompt exceeds that limit, the model cannot process all tokens, leading to truncation or errors and failed summarization. Temperature and top-p only affect output randomness, and fine-tuning support does not change input length constraints."
,,"An airline company wants to build a conversational AI assistant to answer customer questions about ﬂight schedules, booking, and payments. The company wants to use large language models (LLMs) and a knowledge base to create a text-based chatbot interface. Which solution will meet these requirements with the LEAST development effort?",Train models on Amazon SageMaker Autopilot.,Develop a Retrieval Augmented Generation (RAG) agent by using Amazon Bedrock.,Create a Python application by using Amazon Q Developer.,Fine-tune models on Amazon SageMaker Jumpstart.,,,B,,0,0,1,0,1,1,,,,,2.3,Describe AWS infrastructure and technologies for building generative AI applications.,,161,AIF-C01,AWS Certified AI Practitioner,,"Amazon Bedrock provides managed RAG capabilities (Agents and Knowledge Bases) to connect LLMs to enterprise data with minimal code and no model training. Options A and D involve training/fine-tuning, which increases effort and is unnecessary for Q&A over existing knowledge. Option C (Amazon Q Developer) targets developer assistance, not a customer-facing flight chatbot."
,,What is tokenization used for in natural language processing (NLP)?,To encrypt text data,To compress text ﬁles,To break text into smaller units for processing,To translate text between languages,,,C,,0,0,0,1,2,,,,,,1.1,Explain basic AI concepts and terminologies.,,162,AIF-C01,AWS Certified AI Practitioner,,"Tokenization segments raw text into discrete units (words, subwords, or characters) that models can index and process. This enables building vocabularies, creating embeddings, and feeding sequences to algorithms. It is not related to encryption, compression, or translation."
,,Which option is a characteristic of transformer-based language models?,Transformer-based language models use convolutional layers to apply ﬁlters across an input to capture local patterns through ﬁltered views.,Transformer-based language models can process only text data.,Transformer-based language models use self-attention mechanisms to capture contextual relationships.,Transformer-based language models process data sequences one element at a time in cyclic iterations.,,,C,,0,0,0,1,3,2,,,,,2.1,Explain the basic concepts of generative AI.,,163,AIF-C01,AWS Certified AI Practitioner,,"Transformers rely on self-attention to model contextual relationships across all tokens in a sequence. Option C is correct because self-attention enables parallel processing and long-range dependency capture. The other options describe CNNs (A), incorrectly limit modality (B), or conflate with RNN sequential processing (D)."
,,A ﬁnancial company is using AI systems to obtain customer credit scores as part of the loan application process. The company wants to expand to a new market in a different geographic area. The company must ensure that it can operate in that geographic area. Which compliance laws should the company review?,Local health data protection laws,Local payment card data protection laws,Local education privacy laws,Local algorithm accountability laws,,,D,,0,0,1,0,2,3,,,,,5.2,Recognize governance and compliance regulations for AI systems.,,164,AIF-C01,AWS Certified AI Practitioner,,"Credit scoring involves automated decision-making, which is governed by algorithm accountability and transparency laws in many regions. These laws often require auditability, bias mitigation, explainability, and documentation of AI models. Health, payment card, and education privacy laws are sector-specific and less directly applicable to credit scoring AI."
,,A company uses Amazon Bedrock for its generative AI application. The company wants to use Amazon Bedrock Guardrails to detect and ﬁlter harmful user inputs and model-generated outputs. Which content categories can the guardrails ﬁlter? (Choose two.),Hate,Politics,Violence,Gambling,Religion,,"A, C",1,0,0,0,1,1,,,,,,4.1,Explain the development of AI systems that are responsible.,,165,AIF-C01,AWS Certified AI Practitioner,,"Bedrock Guardrails provide built-in safety filters for categories such as hate and violence to reduce harmful inputs and outputs. Categories like politics, gambling, and religion are not default safety categories. Therefore, the correct filters are Hate (A) and Violence (C)."
,,Which scenario describes a potential risk and limitation of prompt engineering in the context of a generative AI model?,"Prompt engineering does not ensure that the model always produces consistent and deterministic outputs, eliminating the need for validation.",Prompt engineering could expose the model to vulnerabilities such as prompt injection attacks.,Properly designed prompts reduce but do not eliminate the risk of data poisoning or model hijacking.,Prompt engineering does not ensure that the model will consistently generate highly reliable outputs when working with real-world data.,,,B,,0,0,1,0,2,2,,,,,5.1,Explain methods to secure AI systems.,,166,AIF-C01,AWS Certified AI Practitioner,,"Prompt injection is a security vulnerability where malicious inputs manipulate the model to ignore instructions or leak sensitive data. This risk persists regardless of how well a prompt is crafted, so additional security controls (e.g., input/output filtering, context isolation, and least-privilege data access) are required."
,,A publishing company built a Retrieval Augmented Generation (RAG) based solution to give its users the ability to interact with published content. New content is published daily. The company wants to provide a near real-time experience to users. Which steps in the RAG pipeline should the company implement by using oﬄine batch processing to meet these requirements? (Choose two.),Generation of content embeddings,Generation of embeddings for user queries,Creation of the search index,Retrieval of relevant content,Response generation for the user,,"A, C",1,0,0,1,0,1,1,,,,,3.1,Describe design considerations for applications that use foundation models.,,167,AIF-C01,AWS Certified AI Practitioner,,"Content embeddings are relatively static and computationally expensive, so precomputing them offline ensures low-latency, near real-time user interactions. Query embeddings, retrieval, and response generation must occur online to reflect the user’s immediate input. Precomputing content embeddings avoids recomputation on each query and speeds up the RAG pipeline."
,,Which technique breaks a complex task into smaller subtasks that are sent sequentially to a large language model (LLM)?,One-shot prompting,Prompt chaining,Tree of thoughts,Retrieval Augmented Generation (RAG),,,B,,0,0,0,1,1,,,,,,3.2,Choose effective prompt engineering techniques.,,168,AIF-C01,AWS Certified AI Practitioner,,"Prompt chaining decomposes a complex problem into sequential prompts where each subtask feeds into the next. This contrasts with one-shot prompting (single example), tree of thoughts (branching exploration), and RAG (retrieving external context), making prompt chaining the correct choice."
,,An AI practitioner needs to improve the accuracy of a natural language generation model. The model uses rapidly changing inventory data. Which technique will improve the model's accuracy?,Transfer learning,Federated learning,Retrieval Augmented Generation (RAG),One-shot prompting,,,C,,0,0,0,1,1,,,,,,3.1,Describe design considerations for applications that use foundation models.,,169,AIF-C01,AWS Certified AI Practitioner,,"RAG retrieves the latest inventory data at inference time and injects it into the prompt, grounding the model's generation in up-to-date facts. This reduces hallucinations and counters stale model knowledge, improving accuracy on rapidly changing information. Transfer/federated learning or one-shot prompting do not reliably reflect fast-changing data in real time."
,,A company wants to collaborate with several research institutes to develop an AI model. The company needs standardized documentation of model version tracking and a record of model development. Which solution meets these requirements?,Track the model changes by using Git.,Track the model changes by using Amazon Fraud Detector.,Track the model changes by using Amazon SageMaker Model Cards.,Track the model changes by using Amazon Comprehend.,,,C,,0,0,0,1,1,,,,,,4.2,Recognize the importance of transparent and explainable models.,,170,AIF-C01,AWS Certified AI Practitioner,,"Amazon SageMaker Model Cards provide standardized, versioned documentation of model development, lineage, and performance for collaboration and audits. Git primarily tracks code changes, not structured model documentation for AI governance. Amazon Fraud Detector and Amazon Comprehend are domain-specific services and do not offer model documentation/version tracking for general AI models."
,,A company that uses multiple ML models wants to identify changes in original model quality so that the company can resolve any issues. Which AWS service or feature meets these requirements?,Amazon SageMaker JumpStart,Amazon SageMaker HyperPod,Amazon SageMaker Data Wrangler,Amazon SageMaker Model Monitor,,,D,,0,0,0,1,1,,,,,,1.3,Describe the ML development lifecycle.,,171,AIF-C01,AWS Certified AI Practitioner,,"Amazon SageMaker Model Monitor tracks model quality, data drift, and concept drift for deployed endpoints. It compares live data and predictions to a baseline, generates alerts via CloudWatch, and helps identify degradation so teams can take corrective action. Other options focus on model templates (JumpStart), distributed training infrastructure (HyperPod), or data prep (Data Wrangler), not production monitoring."
,,What is the purpose of chunking in Retrieval Augmented Generation (RAG)?,To avoid database storage limitations for large text documents by storing parts or chunks of the text,To improve eﬃciency by avoiding the need to convert large text into vector embeddings,To improve the contextual relevancy of results retrieved from the vector index,To decrease the cost of storage by storing parts or chunks of the text,,,C,,0,0,0,1,1,,,,,,3.1,Describe design considerations for applications that use foundation models.,,172,AIF-C01,AWS Certified AI Practitioner,,"Chunking breaks documents into semantically coherent segments so each piece can be embedded and retrieved accurately, increasing the chance that vector search returns the most relevant context. This reduces noise from overly long passages and improves contextual relevancy for the model’s final response."
,,"A company is developing an editorial assistant application that uses generative AI. During the pilot phase, usage is low and application performance is not a concern. The company cannot predict application usage after the application is fully deployed and wants to minimize application costs. Which solution will meet these requirements?",Use GPU-powered Amazon EC2 instances.,Use Amazon Bedrock with Provisioned Throughput.,Use Amazon Bedrock with On-Demand Throughput.,Use Amazon SageMaker JumpStart.,,,C,,0,0,1,0,1,,,,,,2.3,Describe AWS infrastructure and technologies for building generative AI applications.,,173,AIF-C01,AWS Certified AI Practitioner,,"Amazon Bedrock On-Demand Throughput is pay-per-request and scales automatically, minimizing costs when usage is low or unpredictable. Provisioned Throughput requires capacity reservations and fixed commitments, increasing cost risk. EC2 GPUs and SageMaker JumpStart require managing infrastructure/endpoints and are less cost-efficient for sporadic demand."
,,A company deployed a Retrieval Augmented Generation (RAG) application on Amazon Bedrock that gathers ﬁnancial news to distribute in daily newsletters. Users have recently reported politically inﬂuenced ideas in the newsletters. Which Amazon Bedrock guardrail can identify and ﬁlter this content?,Word ﬁlters,Denied topics,Sensitive information ﬁlters,Content ﬁlters,,,B,,0,0,1,0,1,,,,,,4.1,Explain the development of AI systems that are responsible.,,174,AIF-C01,AWS Certified AI Practitioner,,"Denied topics let you define disallowed subject areas (e.g., politics) and block or filter responses related to them. Content filters target safety categories (sexual, hate, violence, self-harm), and sensitive information filters target PII, while word filters require explicit keywords and are less reliable for broad political themes. Therefore, 'Denied topics' is the correct guardrail to identify and filter politically influenced content."
,,A ﬁnancial company is developing a fraud detection system that ﬂags potential fraud cases in credit card transactions. Employees will evaluate the ﬂagged fraud cases. The company wants to minimize the amount of time the employees spend reviewing ﬂagged fraud cases that are not actually fraudulent. Which evaluation metric meets these requirements?,Recall,Accuracy,Precision,Lift chart,,,C,,0,0,1,0,1,,,,,,1.1,Explain basic AI concepts and terminologies.,,175,AIF-C01,AWS Certified AI Practitioner,,"Precision measures the proportion of predicted fraud cases that are actually fraudulent, minimizing false positives and thus reviewer time. Recall focuses on catching all frauds, accuracy can be misleading with imbalanced data, and a lift chart is a diagnostic visualization rather than the specific metric needed here."
,,A company designed an AI-powered agent to answer customer inquiries based on product manuals. Which strategy can improve customer conﬁdence levels in the AI-powered agent's responses?,Writing the conﬁdence level in the response,Including referenced product manual links in the response,Designing an agent avatar that looks like a computer,Training the agent to respond in the company's language style,,,B,,0,0,0,1,1,,,,,,4.2,Recognize the importance of transparent and explainable models.,,176,AIF-C01,AWS Certified AI Practitioner,,"Including links to the exact product manual passages provides verifiable sources, increasing transparency and user trust. It enables customers to validate the AI’s claims and understand the basis of the answer. Confidence scores, avatars, or brand tone do not inherently improve verifiability or trust in correctness."
,,A hospital developed an AI system to provide personalized treatment recommendations for patients. The AI system must provide the rationale behind the recommendations and make the insights accessible to doctors and patients. Which human-centered design principle does this scenario present?,Explainability,Privacy and security,Fairness,Data governance,,,A,,0,0,0,1,1,,,,,,4.2,Recognize the importance of transparent and explainable models.,,177,AIF-C01,AWS Certified AI Practitioner,,"Explainability ensures that AI outputs are interpretable and the reasoning behind recommendations is clear to users. The scenario explicitly requires the system to provide rationale and make insights accessible to doctors and patients, which is the essence of explainability. Privacy/security, fairness, and data governance address different concerns not centered on understanding model decisions."
,,Which statement presents an advantage of using Retrieval Augmented Generation (RAG) for natural language processing (NLP) tasks?,RAG can use external knowledge sources to generate more accurate and informative responses.,RAG is designed to improve the speed of language model training.,RAG is primarily used for speech recognition tasks.,RAG is a technique for data augmentation in computer vision tasks.,,,A,,0,0,0,1,1,,,,,,3.1,Describe design considerations for applications that use foundation models.,,178,AIF-C01,AWS Certified AI Practitioner,,"RAG retrieves relevant documents from external knowledge sources and injects them into the model’s context, grounding generation in factual data. This improves accuracy and relevance while reducing hallucinations. The other options describe unrelated objectives (training speed, speech recognition, computer vision augmentation)."
,,A company has created a custom model by ﬁne-tuning an existing large language model (LLM) from Amazon Bedrock. The company wants to deploy the model to production and use the model to handle a steady rate of requests each minute. Which solution meets these requirements MOST cost-effectively?,Deploy the model by using an Amazon EC2 compute optimized instance.,Use the model with on-demand throughput on Amazon Bedrock.,Store the model in Amazon S3 and host the model by using AWS Lambda.,Purchase Provisioned Throughput for the model on Amazon Bedrock.,,,D,,0,0,1,0,1,,,,,,2.3,Describe AWS infrastructure and technologies for building generative AI applications.,,179,AIF-C01,AWS Certified AI Practitioner,,"Provisioned Throughput on Amazon Bedrock offers dedicated, predictable capacity and latency, making it more cost-effective for steady, predictable request rates. On-demand throughput is better for spiky or low-volume workloads and may cost more at steady utilization. Hosting on EC2 or Lambda is not applicable for Bedrock-hosted fine-tuned models and would add complexity and inappropriate infrastructure choices."
,,Which technique involves training AI models on labeled datasets to adapt the models to speciﬁc industry terminology and requirements?,Data augmentation,Fine-tuning,Model quantization,Continuous pre-training,,,B,,0,0,1,0,1,1,,,,,3.3,Describe the training and fine-tuning process for foundation models.,,180,AIF-C01,AWS Certified AI Practitioner,,"Fine-tuning updates a pre-trained model’s weights using labeled, domain-specific data so it learns industry terminology and requirements. By contrast, data augmentation just expands training data, model quantization reduces precision for efficiency, and continuous pre-training focuses on broad pretraining (often self-supervised) rather than targeted labeled adaptation."
,,"A company is creating an agent for its application by using Amazon Bedrock Agents. The agent is performing well, but the company wants to improve the agent’s accuracy by providing some speciﬁc examples. Which solution meets these requirements?",Modify the advanced prompts for the agent to include the examples.,Create a guardrail for the agent that includes the examples.,Use Amazon SageMaker Ground Truth to label the examples.,Run a script in AWS Lambda that adds the examples to the training dataset.,,,A,,0,0,0,1,1,,,,,,3.2,Choose effective prompt engineering techniques.,,181,AIF-C01,AWS Certified AI Practitioner,,"Adding specific examples to the agent’s advanced prompts is few-shot prompting, which guides the model to produce more accurate outputs without retraining. Guardrails target safety/compliance, and labeling or updating a training dataset does not directly enhance a Bedrock Agent’s runtime behavior."
,,Which option is a beneﬁt of using infrastructure as code (IaC) in machine learning operations (MLOps)?,IaC eliminates the need for hyperparameter tuning.,"IaC always provisions powerful compute instances, contributing to the training of more accurate models.",IaC streamlines the deployment of scalable and consistent ML workloads in cloud environments.,IaC minimizes overall expenses by deploying only low-cost instances.,,,C,,0,0,0,1,1,,,,,,1.3,Describe the ML development lifecycle.,,182,AIF-C01,AWS Certified AI Practitioner,,"IaC lets teams define and version-control infrastructure, enabling consistent, automated, and scalable deployments of ML workloads across environments. This improves reproducibility and reduces configuration drift, which is critical for MLOps. Options A, B, and D are unrelated to IaC’s purpose of standardizing and automating infrastructure rather than tuning models or selecting instance types based on cost or power."
,,A company wants to ﬁne-tune a foundation model (FM) to answer questions for a speciﬁc domain. The company wants to use instruction- based ﬁne-tuning. How should the company prepare the training data?,Gather company internal documents and industry-speciﬁc materials. Merge the documents and materials into a single ﬁle.,Collect external company reviews from various online sources. Manually label each review as either positive or negative.,Create pairs of questions and answers that speciﬁcally address topics related to the company's industry domain.,Create few-shot prompts to instruct the model to answer only domain knowledge.,,,C,,0,0,0,1,1,,,,,,3.3,Describe the training and fine-tuning process for foundation models.,,183,AIF-C01,AWS Certified AI Practitioner,,"Instruction-based fine-tuning uses supervised pairs of instructions/questions and ideal responses to teach the model desired behaviors in a domain. Creating domain-specific Q&A pairs provides the correct input-output format for aligning the FM to the company’s topics. The other options involve unlabeled corpora, sentiment classification, or prompting rather than fine-tuning."
,,Which ML technique ensures data compliance and privacy when training AI models on AWS?,Reinforcement learning,Transfer learning,Federated learning,Unsupervised learning,,,C,,0,0,1,0,2,3,,,,,5.1,Explain methods to secure AI systems.,,184,AIF-C01,AWS Certified AI Practitioner,,"Federated learning trains models locally on distributed data sources and only shares model updates, not raw data, preserving privacy and aiding compliance requirements like GDPR. This reduces data movement and centralization risks on AWS. In contrast, reinforcement, transfer, and unsupervised learning do not inherently address data privacy or compliance."
,,HOTSPOT - A company needs to customize a base model that is hosted on Amazon Bedrock. Select the correct model customization method from the following list of company requirements. Each model customization method should be selected one or more times. Answer : Question 186 ( Exam A) A manufacturing company has an application that ingests consumer complaints from publicly available sources. The application uses complex hard-coded logic to process the complaints. The company wants to scale this logic across markets and product lines. Which advantage do generative AI models offer for this scenario?,Predictability of outputs,Adaptability,Less sensitivity to changes in inputs,Explainability,,,B,,0,,,1,,,,,,,2.2,Understand the capabilities and limitations of generative AI for solving business problems.,,185,AIF-C01,AWS Certified AI Practitioner,,"Generative AI models can generalize from examples and adapt to varied, unstructured complaint text across markets and product lines without hard-coded rules. This adaptability enables scaling to new categories, formats, or languages with minimal rule maintenance. Predictability and explainability are weaker with generative models, making adaptability the key advantage here."
,,A ﬁnancial company wants to ﬂag all credit card activity as possibly fraudulent or non-fraudulent based on transaction data. Which type of ML model meets these requirements?,Regression,Diffusion,Binary classiﬁcation,Multi-class classiﬁcation,,,C,,0,0,1,0,2,,,,,,1.1,Explain basic AI concepts and terminologies.,,187,AIF-C01,AWS Certified AI Practitioner,,"Fraud detection with two outcomes (fraudulent vs non-fraudulent) is a binary classification problem. Regression predicts continuous values, and multi-class classification is for more than two discrete classes. Diffusion models are generative and not suited for this discriminative task."
 ,,HOTSPOT - A company is designing a customer service chatbot by using a ﬁne-tuned large language model (LLM). The company wants to ensure that the chatbot uses responsible AI characteristics. Select the correct responsible AI characteristic from the following list for each application design action. Each responsible AI characteristic should be selected one time or not at all. Answer : Question 189 ( Exam A) A hospital wants to use a generative AI solution with speech-to-text functionality to help improve employee skills in dictating clinical notes. Which AWS service meets these requirements?,Amazon Q Developer,Amazon Polly,Amazon Rekognition,AWS HealthScribe,,,D,,0,,,1,,,,,,,2.3,Describe AWS infrastructure and technologies for building generative AI applications.,,188,AIF-C01,AWS Certified AI Practitioner,,"AWS HealthScribe is a healthcare-focused, HIPAA-eligible service that uses generative AI to convert clinician-patient conversations into transcripts and draft clinical notes. It provides speech-to-text and generates summaries tailored for clinical documentation. The other options are not suited: Polly is text-to-speech, Rekognition is for image/video analysis, and Amazon Q Developer is a coding assistant."
,,Which type of AI model makes numeric predictions?,Diffusion,Regression,Transformer,Multi-modal,,,B,,0,0,1,0,2,,,,,,1.1,Explain basic AI concepts and terminologies.,,190,AIF-C01,AWS Certified AI Practitioner,,"Regression models predict continuous numeric values, such as prices or temperatures. Diffusion and transformers are generative/architectural approaches, not specifically for numeric prediction. Multi-modal refers to handling multiple data types, not a prediction type."
,,HOTSPOT - A company wants to use Amazon SageMaker features for various use cases. Select the correct SageMaker feature from the following list for each use case. Each SageMaker feature should be selected one time or not at all. Answer : Question 192 ( Exam A) What is the purpose of vector embeddings in a large language model (LLM)?,Splitting text into manageable pieces of data,Grouping a set of characters to be treated as a single unit,Providing the ability to mathematically compare texts,Providing the count of every word in the input,,,C,,0,,,1,,,,,,,2.1,Explain the basic concepts of generative AI.,,191,AIF-C01,AWS Certified AI Practitioner,,"Vector embeddings encode text into numerical vectors that capture semantic meaning. This allows texts to be compared mathematically (e.g., via cosine similarity) to assess how similar their meanings are. Such comparisons enable tasks like semantic search, clustering, and retrieval-augmented generation."
,,"A company wants to ﬁne-tune a foundation model (FM) by using AWS services. The company needs to ensure that its data stays private, safe, and secure in the source AWS Region where the data is stored. Which combination of steps will meet these requirements MOST cost-effectively? (Choose two.)",Host the model on premises by using AWS Outposts.,Use the Amazon Bedrock API.,Use AWS PrivateLink and a VPC.,Host the Amazon Bedrock API on premises.,Use Amazon CloudWatch logs and metrics.,,"B, C",1,0,0,1,0,1,,,,,,2.3,Describe AWS infrastructure and technologies for building generative AI applications.,,193,AIF-C01,AWS Certified AI Practitioner,,"Using the Amazon Bedrock API (B) provides managed fine-tuning of foundation models with regional data residency and without sharing customer data for model training. Pairing it with AWS PrivateLink and a VPC (C) ensures private, in-Region connectivity that keeps traffic off the public internet, enhancing security and cost-effectiveness. Outposts and hosting Bedrock on premises are unnecessary and costly, and CloudWatch does not address privacy requirements."
,,A ﬁnancial company uses AWS to host its generative AI models. The company must generate reports to show adherence to international regulations for handling sensitive customer data. Which AWS service meets these requirements?,Amazon Macie,AWS Artifact,AWS Secrets Manager,AWS Conﬁg,,,B,,0,0,1,0,4,,,,,,5.2,Recognize governance and compliance regulations for AI systems.,,194,AIF-C01,AWS Certified AI Practitioner,,"AWS Artifact provides on-demand access to AWS compliance reports and certifications (e.g., SOC, ISO, PCI) and agreements needed to demonstrate regulatory adherence. This enables generating documentation for audits and regulatory reporting. Amazon Macie detects sensitive data, Secrets Manager manages credentials, and AWS Config monitors resource configuration compliance, but none provide official compliance reports."
,,A medical company wants to modernize its onsite information processing application. The company wants to use generative AI to respond to medical questions from patients. Which AWS service should the company use to ensure responsible AI for the application?,Guardrails for Amazon Bedrock,Amazon Inspector,Amazon Rekognition,AWS Trusted Advisor,,,A,,0,0,1,0,1,,,,,,4.1,Explain the development of AI systems that are responsible.,,195,AIF-C01,AWS Certified AI Practitioner,,"Guardrails for Amazon Bedrock provides configurable content filters, safety policies, and PII controls to enforce responsible AI behavior in generative applications. Amazon Inspector is for vulnerability management, Amazon Rekognition is for image/video analysis, and AWS Trusted Advisor provides cost and best-practice checks, not LLM safety controls."
,,Which metric is used to evaluate the performance of foundation models (FMs) for text summarization tasks?,F1 score,Bilingual Evaluation Understudy (BLEU) score,Accuracy,Mean squared error (MSE),,,B,,0,0,1,0,1,,,,,,3.4,Describe methods to evaluate foundation model performance.,,196,AIF-C01,AWS Certified AI Practitioner,,"BLEU is a common metric for evaluating text generation quality by measuring n-gram overlap precision between a model’s output and reference summaries. It is used for tasks like translation and summarization to assess fluency and adequacy. F1 and accuracy suit classification, while MSE is for regression, not summarization quality."
,,What is the beneﬁt of ﬁne-tuning a foundation model (FM)?,Fine-tuning reduces the FM's size and complexity and enables slower inference.,Fine-tuning uses speciﬁc training data to retrain the FM from scratch to adapt to a speciﬁc use case.,Fine-tuning keeps the FM's knowledge up to date by pre-training the FM on more recent data.,Fine-tuning improves the performance of the FM on a speciﬁc task by further training the FM on new labeled data.,,,D,,0,0,0,1,1,,,,,,3.3,Describe the training and fine-tuning process for foundation models.,,197,AIF-C01,AWS Certified AI Practitioner,,"Fine-tuning adapts a pre-trained foundation model to a specific task by further training it on new, labeled data, improving task-specific performance. It does not retrain the model from scratch, reduce its size, or primarily serve to update general knowledge. Therefore, option D correctly reflects the benefit of fine-tuning."
,,A company wants to improve its chatbot's responses to match the company's desired tone. The company has 100 examples of high-quality conversations between customer service agents and customers. The company wants to use this data to incorporate company tone into the chatbot's responses. Which solution meets these requirements?,Use Amazon Personalize to generate responses.,Create an Amazon SageMaker HyperPod pre-training job.,Host the model by using Amazon SageMaker. Use TensorRT for large language model (LLM) deployment.,Create an Amazon Bedrock ﬁne-tuning job.,,,D,,0,0,1,0,1,1,,,,,3.3,Describe the training and fine-tuning process for foundation models.,,198,AIF-C01,AWS Certified AI Practitioner,,"Amazon Bedrock fine-tuning lets you customize a foundation model with your own examples to align responses with a specific company tone. Personalize is for recommendations, not chat generation; HyperPod targets large-scale pretraining; and SageMaker hosting with TensorRT optimizes inference but does not adjust the model’s tone. Fine-tuning on Bedrock directly addresses the need to incorporate the company’s conversational style."
,,"An ecommerce company is using a chatbot to automate the customer order submission process. The chatbot is powered by AI and is available to customers directly from the company's website 24 hours a day, 7 days a week. Which option is an AI system input vulnerability that the company needs to resolve before the chatbot is made available?",Data leakage,Prompt injection,Large language model (LLM) hallucinations,Concept drift,,,B,,0,0,0,1,1,,,,,,5.1,Explain methods to secure AI systems.,,199,AIF-C01,AWS Certified AI Practitioner,,"Prompt injection is an input manipulation attack where users craft prompts to override system instructions or trigger unintended actions, making it a key vulnerability to address before deployment. Data leakage is an outcome risk, hallucinations are a model limitation, and concept drift is a long-term distribution shift issue, not an immediate input vulnerability. Therefore, prompt injection is the correct security concern for a public-facing chatbot."
,,A social media company wants to prevent users from posting discriminatory content on the company's application. The company wants to use Amazon Bedrock as part of the solution. How can the company use Amazon Bedrock to meet these requirements?,Give users the ability to interact based on user preferences.,Block interactions related to predeﬁned topics.,Restrict user conversations to predeﬁned topics.,Provide a variety of responses to select from for user engagement.,,,B,,0,0,1,0,1,,,,,,4.1,Explain the development of AI systems that are responsible.,,200,AIF-C01,AWS Certified AI Practitioner,,"Amazon Bedrock Guardrails let you define and block interactions related to predefined harmful topics (e.g., hate or discrimination). By configuring topic and content filters, the system can prevent users from generating or posting discriminatory content. The other options do not provide a direct mechanism to block harmful content."
,,An education company waftion. The application will give users the ability to enter text or provide a picture of a question. The application will respond with a written answer and an explanation of the written answer. Which model type meets these requirements?,Computer vision model,Large multi-modal language model,Diffusion model,Text-to-speech model,,,B,,0,0,0,1,1,,,,,,2.1,Explain the basic concepts of generative AI.,,201,AIF-C01,AWS Certified AI Practitioner,,"A large multi-modal language model can accept both text and images and generate text responses with explanations. A computer vision model focuses on visual tasks and does not produce rich textual answers, a diffusion model generates images, and a text-to-speech model outputs audio rather than written text."
,,In which stage of the generative AI model lifecycle are tests performed to examine the model's accuracy?,Deployment,Data selection,Fine-tuning,Evaluation,,,D,,0,0,1,0,1,,,,,,3.4,Describe methods to evaluate foundation model performance.,,202,AIF-C01,AWS Certified AI Practitioner,,"Evaluation is the stage where a model’s accuracy and other metrics are measured using validation datasets and benchmarks. It verifies performance before deployment and after training or fine-tuning. Therefore, the correct stage is Evaluation."
,,Which statement correctly describes embeddings in generative AI?,Embeddings represent data as high-dimensional vectors that capture semantic relationships.,Embeddings is a technique that searches data to ﬁnd the most helpful information to answer natural language questions.,Embeddings reduce the hardware requirements of a model by using a less precise data type for the weights and activations.,Embeddings provide the ability to store and retrieve data for generative AI applications.,,,A,,0,0,1,0,2,,,,,,1.1,Explain basic AI concepts and terminologies.,,203,AIF-C01,AWS Certified AI Practitioner,,"Embeddings map data (text, images, etc.) into high-dimensional vectors where distances reflect semantic similarity. Option B describes the retrieval/search process, Option C refers to quantization, and Option D refers to vector databases/storage—not embeddings themselves."
,,A company wants to add generative AI functionality to its application by integrating a large language model (LLM). The responses from the LLM must be as deterministic and as stable as possible. Which solution meets these requirements?,Conﬁgure the application to automatically set the temperature parameter to 0 when submitting the prompt to the LLM.,"Conﬁgure the application to automatically add ""make your response deterministic"" at the end of the prompt before submitting the prompt to the LLM.","Conﬁgure the application to automatically add ""make your response deterministic"" at the beginning of the prompt before submitting the prompt to the LLM.",Conﬁgure the application to automatically set the temperature parameter to 1 when submitting the prompt to the LLM.,,,A,,0,0,0,1,4,1,,,,,2.1,Explain the basic concepts of generative AI.,,204,AIF-C01,AWS Certified AI Practitioner,,"Setting temperature to 0 minimizes randomness, making outputs as deterministic and stable as possible for the same prompt and context. Adding text like 'make your response deterministic' does not control the sampling process, and temperature 1 increases variability. Therefore, configuring temperature to 0 best meets the requirement."
,,A company needs to select a generative AI model to build an application. The application must provide responses to users in real time. Which model characteristic should the company consider to meet these requirements?,Model complexity,Innovation speed,Inference speed,Training time,,,C,,0,0,0,1,1,,,,,,3.1,Describe design considerations for applications that use foundation models.,,205,AIF-C01,AWS Certified AI Practitioner,,"Real-time responses depend on low latency, which is driven by the model’s inference speed. Training time and innovation speed do not affect request-time responsiveness, and greater model complexity can increase latency. Therefore, choosing a model with fast inference best meets the requirement."
,,Which term refers to the instructions given to foundation models (FMs) so that the FMs provide a more accurate response to a question?,Prompt,Direction,Dialog,Translation,,,A,,0,0,0,1,1,,,,,,3.2,Choose effective prompt engineering techniques.,,206,AIF-C01,AWS Certified AI Practitioner,,"A prompt is the instruction or input provided to a foundation model to guide its response. Effective prompts improve accuracy and relevance by specifying context, intent, and desired format. The other options are not standard terms for directing model behavior."
,,A retail company wants to build an ML model to recommend products to customers. The company wants to build the model based on responsible practices. Which practice should the company apply when collecting data to decrease model bias?,Use data from only customers who match the demographics of the company's overall customer base.,Collect data from customers who have a past purchase history.,Ensure that the data is balanced and collected from a diverse group.,Ensure that the data is from a publicly available dataset.,,,C,,0,0,0,1,1,,,,,,4.1,Explain the development of AI systems that are responsible.,,207,AIF-C01,AWS Certified AI Practitioner,,"Collecting balanced, diverse data mitigates sampling bias and improves fairness in recommendations. Limiting to matching demographics or only past purchasers introduces selection bias, and public datasets are not inherently representative or appropriate for the specific context."
,,A company is developing an ML model to predict customer churn. Which evaluation metric will assess the model's performance on a binary classiﬁcation task such as predicting churn?,F1 score,Mean squared error (MSE),R-squared,Time used to train the model,,,A,,0,0,1,0,1,3,,,,0,1.1,Explain basic AI concepts and terminologies.,,208,AIF-C01,AWS Certified AI Practitioner,,"F1 score is appropriate for binary classification because it balances precision and recall, which is important for tasks like churn prediction that may be imbalanced. MSE and R-squared are regression metrics, not classification metrics. Time used to train the model is an efficiency metric, not a performance evaluation metric."
,,An AI practitioner is evaluating the performance of an Amazon SageMaker model. The AI practitioner must choose a performance metric. The metric must show the ratio of the number of correctly classiﬁed items to the total number of correctly and incorrectly classiﬁed items. Which metric meets these requirements?,Accuracy,Precision,F1 score,Recall,,,A,,0,0,1,0,1,2,,,,,1.1,Explain basic AI concepts and terminologies.,,209,AIF-C01,AWS Certified AI Practitioner,,"Accuracy measures the proportion of correctly classified instances out of all predictions: (TP + TN) / (TP + TN + FP + FN). Precision and recall consider only subsets of outcomes (positive predictions or actual positives), and F1 is the harmonic mean of precision and recall. Therefore, the metric describing the ratio of correct classifications to all classifications is accuracy."
,,An ecommerce company receives multiple gigabytes of customer data daily. The company uses the data to train an ML model to forecast future product demand. The company needs a solution to perform inferences once each day. Which inference type meets these requirements?,Batch inference,Asynchronous inference,Real-time inference,Serverless inference,,,A,,0,0,0,1,1,,,,,,1.3,Describe the ML development lifecycle.,,210,AIF-C01,AWS Certified AI Practitioner,,"Batch inference is designed to run predictions over large datasets on a scheduled basis, making it ideal for daily demand forecasting. Real-time and serverless inference target low-latency, per-request predictions, while asynchronous inference handles long-running single requests rather than scheduled bulk processing."
,,A company has developed a generative AI model for customer segmentation. The model has been deployed in the company's production environment for a long time. The company recently noticed some inconsistency in the model's responses. The company wants to evaluate model bias and drift. Which AWS service or feature meets these requirements?,Amazon SageMaker Model Monitor,Amazon SageMaker Clarify,Amazon SageMaker Model Cards,Amazon SageMaker Feature Store,,,A,,0,0,1,0,1,1,,,,,3.4,Describe methods to evaluate foundation model performance.,,211,AIF-C01,AWS Certified AI Practitioner,,"Amazon SageMaker Model Monitor tracks data quality, model quality, bias drift, and feature attribution drift for deployed models and can alert on changes over time. SageMaker Clarify focuses on bias and explainability analyses primarily at training/evaluation time, not continuous production monitoring. Model Cards document models, and Feature Store manages features, neither providing drift monitoring."
,,A company has signed up for Amazon Bedrock access to build applications. The company wants to restrict employee access to speciﬁc models available on Amazon Bedrock. Which solution meets these requirements?,Use AWS Identity and Access Management (IAM) policies to restrict model access.,Use AWS Security Token Service (AWS STS) to generate temporary credentials for model use.,Use AWS Identity and Access Management (IAM) service roles to restrict model subscription.,Use Amazon Inspector to monitor model access.,,,A,,0,0,1,0,2,2,,,,,5.1,Explain methods to secure AI systems.,,212,AIF-C01,AWS Certified AI Practitioner,,"IAM policies provide fine-grained, model-level permissions for Amazon Bedrock (for example, restricting bedrock:InvokeModel to specific model ARNs). STS only issues temporary credentials and does not define access scope, and service roles are for AWS services, not user access control to models. Amazon Inspector is for vulnerability assessment, not authorization or model access restriction."
,,Which ML technique uses training data that is labeled with the correct output values?,Supervised learning,Unsupervised learning,Reinforcement learning,Transfer learning,,,A,,0,0,1,0,1,1,,,,,1.1,Explain basic AI concepts and terminologies.,,213,AIF-C01,AWS Certified AI Practitioner,,"Supervised learning trains models on labeled input–output pairs to learn a mapping from features to the correct target values. In contrast, unsupervised learning uses unlabeled data to find patterns, and reinforcement learning optimizes actions via rewards. Transfer learning reuses knowledge from a pre-trained model rather than defining how labels are used during initial training."
,,Which large language model (LLM) parameter controls the number of possible next words or tokens considered at each step of the text generation process?,Maximum tokens,Top K,Temperature,Batch size,,,B,,0,0,0,1,4,1,,,,,2.1,Explain the basic concepts of generative AI.,,214,AIF-C01,AWS Certified AI Practitioner,,"Top K limits the candidate set to the K highest-probability tokens at each generation step, directly controlling how many possible next tokens are considered. Temperature adjusts randomness among candidates, maximum tokens caps output length, and batch size affects parallelism—not token selection breadth."
,,A company is making a chatbot. The chatbot uses Amazon Lex and Amazon OpenSearch Service. The chatbot uses the company's private data to answer questions. The company needs to convert the data into a vector representation before storing the data in a database. Which type of foundation model (FM) meets these requirements?,Text completion model,Instruction following model,Text embeddings model,Image generation model,,,C,,0,0,0,1,1,,,,,,2.1,Explain the basic concepts of generative AI.,,215,AIF-C01,AWS Certified AI Practitioner,,"Text embedding models convert text into numerical vector representations suitable for storage in vector databases and semantic search with services like OpenSearch. Text completion and instruction-following models focus on generating text, not producing embeddings. Image generation models are unrelated to text vectorization."
,,A company wants to use a large language model (LLM) to generate product descriptions. The company wants to give the model example descriptions that follow a format. Which prompt engineering technique will generate descriptions that match the format?,Zero-shot prompting,Chain-of-thought prompting,One-shot prompting,Few-shot prompting,,,D,,0,0,0,1,1,,,,,,3.2,Choose effective prompt engineering techniques.,,216,AIF-C01,AWS Certified AI Practitioner,,"Few-shot prompting provides multiple examples that demonstrate the desired structure, guiding the LLM to match the format. Zero-shot lacks examples, and one-shot provides only a single example which may be less robust. Chain-of-thought is aimed at reasoning steps, not enforcing output format."
,,A bank is ﬁne-tuning a large language model (LLM) on Amazon Bedrock to assist customers with questions about their loans. The bank wants to ensure that the model does not reveal any private customer data. Which solution meets these requirements?,Use Amazon Bedrock Guardrails.,Remove personally identiﬁable information (PII) from the customer data before ﬁne-tuning the LLM.,Increase the Top-K parameter of the LLM.,Store customer data in Amazon S3. Encrypt the data before ﬁne-tuning the LLM.,,,B,,0,0,1,0,1,,,,,,4.1,Explain the development of AI systems that are responsible.,,217,AIF-C01,AWS Certified AI Practitioner,,"Removing PII before fine-tuning prevents the model from memorizing and potentially revealing sensitive customer information. Guardrails primarily moderate outputs and do not eliminate memorized private data, while encryption protects data at rest/in transit but not from being learned during training. Adjusting Top-K is a decoding parameter and does not address privacy risks."
,,A grocery store wants to create a chatbot to help customers ﬁnd products in the store. The chatbot must check the inventory in real time and provide the product location in the store. Which prompt engineering technique should the store use to build the chatbot?,Zero-shot prompting,Few-shot prompting,Least-to-most prompting,Reasoning and acting (ReAct) prompting,,,D,,0,0,0,1,1,,,,,,3.2,Choose effective prompt engineering techniques.,,218,AIF-C01,AWS Certified AI Practitioner,,"ReAct prompting combines reasoning with actions/tool calls, enabling the model to query external systems like inventory databases and then ground its answer. This fits the need for real-time checks and providing store locations. Other techniques (zero-/few-shot, least-to-most) do not inherently support tool use or real-time data access."
,,A company uses a third-party model on Amazon Bedrock to analyze conﬁdential documents. The company is concerned about data privacy. Which statement describes how Amazon Bedrock protects data privacy?,User inputs and model outputs are anonymized and shared with third-party model providers.,User inputs and model outputs are not shared with any third-party model providers.,"User inputs are kept conﬁdential, but model outputs are shared with third-party model providers.",User inputs and model outputs are redacted before the inputs and outputs are shared with third-party model providers.,,,B,,0,0,0,1,2,,,,,,5.1,Explain methods to secure AI systems.,,219,AIF-C01,AWS Certified AI Practitioner,,"Amazon Bedrock states that customer prompts and model outputs are not shared with third-party model providers and are not used to train models. Data is processed securely and kept private to the customer’s account, addressing confidentiality concerns. Therefore, B is correct and options suggesting any form of sharing are incorrect."
,,An animation company wants to provide subtitles for its content. Which AWS service meets this requirement?,Amazon Comprehend,Amazon Polly,Amazon Transcribe,Amazon Translate,,,C,,0,0,0,1,2,1,,,,,1.2,Identify practical use cases for AI.,,220,AIF-C01,AWS Certified AI Practitioner,,"Amazon Transcribe performs automatic speech recognition to convert audio to text with timestamps, which is ideal for generating subtitles or captions. Amazon Comprehend analyzes text, Amazon Polly converts text to speech, and Amazon Translate translates text between languages but does not produce transcripts from audio."
,,An ecommerce company wants to group customers based on their purchase history and preferences to personalize the user experience of the company's application. Which ML technique should the company use?,Classiﬁcation,Clustering,Regression,Content generation,,,B,,0,0,1,0,3,2,,,,,1.1,Explain basic AI concepts and terminologies.,,221,AIF-C01,AWS Certified AI Practitioner,,"Clustering is an unsupervised learning technique that groups customers with similar behaviors without requiring predefined labels, ideal for customer segmentation and personalization. Classification needs labeled classes, regression predicts numeric values, and content generation is for creating new content, not grouping users."
,,A company wants to control employee access to publicly available foundation models (FMs). Which solution meets these requirements?,Analyze cost and usage reports in AWS Cost Explorer.,Download AWS security and compliance documents from AWS Artifact.,Conﬁgure Amazon SageMaker JumpStart to restrict discoverable FMs.,Build a hybrid search solution by using Amazon OpenSearch Service.,,,C,,0,0,0,1,2,,,,,,5.1,Explain methods to secure AI systems.,,222,AIF-C01,AWS Certified AI Practitioner,,"Amazon SageMaker JumpStart lets administrators control which foundation models are discoverable and accessible by users, enabling restriction of publicly available FMs. AWS Cost Explorer and AWS Artifact do not manage access; they handle cost analysis and compliance documents, respectively. Amazon OpenSearch Service is unrelated to controlling FM access."
,,A company has set up a translation tool to help its customer service team handle issues from customers around the world. The company wants to evaluate the performance of the translation tool. The company sets up a parallel data process that compares the responses from the tool to responses from actual humans. Both sets of responses are generated on the same set of documents. Which strategy should the company use to evaluate the translation tool?,Use the Bilingual Evaluation Understudy (BLEU) score to estimate the absolute translation quality of the two methods.,Use the Bilingual Evaluation Understudy (BLEU) score to estimate the relative translation quality of the two methods.,Use the BERTScore to estimate the absolute translation quality of the two methods.,Use the BERTScore to estimate the relative translation quality of the two methods.,,,B,,0,0,1,0,1,,,,,,3.4,Describe methods to evaluate foundation model performance.,,223,AIF-C01,AWS Certified AI Practitioner,,"BLEU measures n-gram overlap between machine translations and human references and is best used to compare systems on the same dataset. Its absolute value is not directly meaningful across tasks or datasets, making it more suitable for relative quality assessment. Therefore, using BLEU to compare the tool against human responses on the same documents is appropriate."
,,An AI practitioner wants to generate more diverse and more creative outputs from a large language model (LLM). How should the AI practitioner adjust the inference parameter?,Increase the temperature value.,Decrease the Top K value.,Increase the response length.,Decrease the prompt length.,,,A,,0,0,0,1,1,,,,,,2.1,Explain the basic concepts of generative AI.,,224,AIF-C01,AWS Certified AI Practitioner,,"Temperature controls randomness in token sampling; increasing it flattens the probability distribution so the model is more likely to choose less probable tokens, producing more diverse and creative outputs. Decreasing Top K restricts options and reduces diversity, and changing response or prompt length does not inherently make outputs more creative."
,,"A company has developed custom computer vision models. The company needs a user-friendly interface for data labeling to minimize model mistakes on new real-world data. Which AWS service, feature, or tool meets these requirements?",Amazon SageMaker Ground Truth,Amazon SageMaker Canvas,Amazon Bedrock playground,Amazon Bedrock Agents,,,A,,0,1,0,0,2,1,,,,,1.3,Describe the ML development lifecycle.,,225,AIF-C01,AWS Certified AI Practitioner,,"Amazon SageMaker Ground Truth provides managed data labeling workflows and user-friendly interfaces for annotating datasets, especially for computer vision tasks. It supports human labeling workforces, built-in UIs, and active learning to reduce labeling effort and improve model accuracy on new data. Canvas, Bedrock playground, and Bedrock Agents do not provide data labeling functionality."
,,A company is integrating AI into its employee recruitment and hiring solution. The company wants to mitigate bias risks and ensure responsible AI practices while prioritizing equitable hiring decisions. Which core dimensions of responsible AI should the company consider? (Choose two.),Fairness,Tolerance,Flexibility,Open source,Transparency,,"A, E",1,0,0,0,1,1,,,,,,4.1,Explain the development of AI systems that are responsible.,,226,AIF-C01,AWS Certified AI Practitioner,,"Fairness targets bias mitigation and equitable outcomes in hiring decisions. Transparency enables explainability and auditability of model decisions, supporting accountability and trust. Tolerance, flexibility, and open source are not core responsible AI pillars for bias mitigation."
,,A ﬁnancial company has deployed an ML model to predict customer churn. The model has been running in production for 1 week. The company wants to evaluate how accurately the model predicts churn compared to actual customer behavior. Which metric meets these requirements?,Root mean squared error (RMSE),Return on investment (ROI),F1 score,Bilingual Evaluation Understudy (BLEU) score,,,C,,0,0,1,0,1,,,,,,1.1,Explain basic AI concepts and terminologies.,,227,AIF-C01,AWS Certified AI Practitioner,,"F1 score balances precision and recall, making it suitable for binary classification, especially with class imbalance common in churn prediction. RMSE is for regression, ROI is a business metric not a model accuracy metric, and BLEU evaluates machine translation quality. Therefore, F1 score best measures how well churn predictions match actual outcomes."
,,A company has a generative AI application that uses a pre-trained foundation model (FM) on Amazon Bedrock. The company wants the FM to include more context by using company information. Which solution meets these requirements MOST cost-effectively?,Use Amazon Bedrock Knowledge Bases.,Choose a different FM on Amazon Bedrock.,Use Amazon Bedrock Agents.,Deploy a custom model on Amazon Bedrock.,,,A,,0,0,0,1,1,,,,,,3.1,Describe design considerations for applications that use foundation models.,,228,AIF-C01,AWS Certified AI Practitioner,,"Amazon Bedrock Knowledge Bases provide retrieval-augmented generation to ground an FM on company data without expensive fine-tuning, making it the most cost-effective option. Switching FMs does not add private context, and deploying a custom model is costlier to build and maintain. Bedrock Agents are for task orchestration and may use knowledge bases, but the direct, cost-effective solution is Knowledge Bases."
,,HOTSPOT - A company is using Amazon SageMaker to develop AI models. Select the correct SageMaker feature or resource from the following list for each step in the AI model lifecycle workﬂow. Each SageMaker feature or resource should be selected one time or not at all. Answer : Question 230 ( Exam A) A food service company wants to collect a dataset to predict customer food preferences. The company wants to ensure that the food preferences of all demographics are included in the data. Which dataset characteristic does this scenario present?,Accuracy,Diversity,Recency bias,Reliability,,,B,,0,,,1,,,,,,,1.1,Explain basic AI concepts and terminologies.,,229,AIF-C01,AWS Certified AI Practitioner,,"Diversity refers to having a dataset that represents all relevant demographic groups so the model learns from varied examples and avoids bias. Accuracy and reliability describe model performance characteristics, not dataset composition, and recency bias is about time-skewed data rather than demographic coverage."
,,A company wants to create a chatbot that answers questions about human resources policies. The company is using a large language model (LLM) and has a large digital documentation base. Which technique should the company use to optimize the generated responses?,Use Retrieval Augmented Generation (RAG).,Use few-shot prompting.,Set the temperature to 1.,Decrease the token size.,,,A,,0,0,1,0,1,2,,,,,3.1,Describe design considerations for applications that use foundation models.,,231,AIF-C01,AWS Certified AI Practitioner,,"RAG retrieves relevant passages from the company’s HR documentation and injects them into the prompt, grounding the LLM’s answers in authoritative sources. This improves accuracy and reduces hallucinations for policy Q&A. Few-shot prompting and temperature tuning don’t ensure factual grounding, and changing token size doesn’t address knowledge retrieval."
,,An education company is building a chatbot whose target audience is teenagers. The company is training a custom large language model (LLM). The company wants the chatbot to speak in the target audience's language style by using creative spelling and shortened words. Which metric will assess the LLM's performance?,F1 score,BERTScore,Recall-Oriented Understudy for Gisting Evaluation (ROUGE),Bilingual Evaluation Understudy (BLEU) score,,,B,,0,0,1,0,1,1,,,,,3.4,Describe methods to evaluate foundation model performance.,,232,AIF-C01,AWS Certified AI Practitioner,,"BERTScore compares text using contextual embeddings, capturing semantic similarity even with paraphrasing, slang, or creative spelling. BLEU and ROUGE rely on n-gram overlap and penalize stylistic variations, and F1 is not suited for open-ended generation. Therefore, BERTScore is the most appropriate metric for assessing the chatbot’s stylistically varied outputs."
,,"A customer service team is developing an application to analyze customer feedback and automatically classify the feedback into different categories. The categories include product quality, customer service, and delivery experience. Which A1 concept does this scenario present?",Computer vision,Natural language processing (NLP),Recommendation systems,Fraud detection,,,B,,0,0,1,0,1,1,,,,,1.1,Explain basic AI concepts and terminologies.,,233,AIF-C01,AWS Certified AI Practitioner,,"Analyzing customer feedback text and classifying it into categories is a classic Natural Language Processing (NLP) task, specifically text classification. Computer vision deals with images, recommendation systems suggest items to users, and fraud detection focuses on anomalous transactions, not text understanding."
,,A ﬁnancial services company must ensure that its generative AI-powered chatbot provides factual responses for regulatory compliance. Which solution prevents the underlying foundation model (FM) from hallucinating?,Use AWS Conﬁg to query compliance metadata by using natural language.,Conﬁgure Amazon Bedrock Guardrails to evaluate user inputs and model responses.,Use Amazon Fraud Detector to detect potentially fraudulent online activities.,Use AWS Audit Manager to prepare IT audit and compliance reports.,,,B,,0,0,1,0,2,,,,,,2.3,Describe AWS infrastructure and technologies for building generative AI applications.,,234,AIF-C01,AWS Certified AI Practitioner,,"Amazon Bedrock Guardrails can evaluate and control both prompts and model outputs to enforce safety and factuality policies, reducing hallucinations and compliance risk. The other options (AWS Config, Fraud Detector, Audit Manager) do not moderate LLM inputs/outputs or address model hallucinations in chat responses."
,,"HOTSPOT - A company wants to develop a solution that uses generative AI to create content for product advertisements, including sample images and slogans. Select the correct model type from the following list for each action. Each model type should be selected one time. Answer : Question 236 ( Exam A) A company has created multiple ML models. The company needs a solution for storing, managing, and versioning the models. Which AWS service or feature meets these requirements?",AWS Audit Manager,Amazon SageMaker Model Monitor,Amazon SageMaker Model Registry,Amazon SageMaker Canvas,,,C,,0,,,1,,,,,,,1.3,Describe the ML development lifecycle.,,235,AIF-C01,AWS Certified AI Practitioner,,"Amazon SageMaker Model Registry is designed to store, manage, version, and approve models with lineage and integration into CI/CD pipelines. Audit Manager focuses on compliance evidence, not model artifacts. Model Monitor handles production monitoring, and SageMaker Canvas is a no-code ML tool, not a registry."
,,An AI practitioner is building an ML model. The AI practitioner wants to provide model transparency and explainability to stakeholders. Which solution will meet these requirements?,Present the model Shapley values.,Provide the model accuracy measure.,Provide the model confusion matrix.,Provide a secure model inference endpoint.,,,A,,0,0,1,0,1,1,,,,,4.2,Recognize the importance of transparent and explainable models.,,237,AIF-C01,AWS Certified AI Practitioner,,"Shapley values (e.g., SHAP) attribute each feature’s contribution to predictions, offering clear, instance-level explanations for stakeholders. Accuracy and confusion matrices are performance metrics, not explainability tools. A secure endpoint addresses security, not model transparency."
,,A company is developing an ML application. The application must automatically group similar customers and products based on their characteristics. Which ML strategy should the company use to meet these requirements?,Unsupervised learning,Supervised learning,Reinforcement learning,Semi-supervised learning,,,A,,0,0,1,0,2,,,,,,1.1,Explain basic AI concepts and terminologies.,,238,AIF-C01,AWS Certified AI Practitioner,,"Grouping similar customers and products without labeled outcomes is a clustering problem, which falls under unsupervised learning. Unsupervised methods (e.g., k-means, hierarchical clustering) discover structure in data based on feature similarity. Supervised and reinforcement learning require labels or rewards, which are not present here."
,,A news agency publishes articles in English. The agency wants to make articles available in other languages. Which solution meets these requirements?,Add Amazon Transcribe to the company’s website.,Use the Amazon Translate real-time translation feature.,Add Amazon Personalize to the company’s website.,Use the Amazon Textract real-time document processing feature.,,,B,,0,0,0,1,1,,,,,,1.2,Identify practical use cases for AI.,,239,AIF-C01,AWS Certified AI Practitioner,,"Amazon Translate provides neural machine translation to convert English articles into other languages in real time or batch. Amazon Transcribe is for speech-to-text, Amazon Personalize is for recommendations, and Amazon Textract extracts text from documents—none of which perform translation."
,,A bank is building a chatbot to answer customer questions about opening a bank account. The chatbot will use public bank documents to generate responses. The company will use Amazon Bedrock and prompt engineering to improve the chatbot’s responses. Which prompt engineering technique meets these requirements?,Complexity-based prompting,Zero-shot prompting,Few-shot prompting,Directional stimulus prompting,,,C,,0,0,1,0,1,2,,,,,3.2,Choose effective prompt engineering techniques.,,240,AIF-C01,AWS Certified AI Practitioner,,"Few-shot prompting provides the model with a few example Q&A pairs derived from the public bank documents to guide tone, structure, and content. This helps the model produce more accurate, consistent answers compared to zero-shot prompting. Other options like complexity-based or directional stimulus prompting are less standard for this use case on the exam."
,,A company wants to ﬁne-tune an ML model that is hosted on Amazon Bedrock. The company wants to use its own sensitive data that is stored in private databases in a VPC. The data needs to stay within the company’s private network. Which solution will meet these requirements?,Restrict access to Amazon Bedrock by using an AWS Identity and Access Management (IAM) service role.,Restrict access to Amazon Bedrock by using an AWS Identity and Access Management (IAM) resource policy.,Use AWS PrivateLink to connect the VPC and Amazon Bedrock.,Use AWS Key Management Service (AWS KMS) keys to encrypt the data.,,,C,,0,0,1,0,1,,,,,,2.3,Describe AWS infrastructure and technologies for building generative AI applications.,,241,AIF-C01,AWS Certified AI Practitioner,,"AWS PrivateLink enables private connectivity from a VPC to Amazon Bedrock without traversing the public internet, ensuring data remains within the private network. IAM roles or resource policies control authorization but do not provide private network paths. KMS encryption protects data at rest and in transit but does not enforce VPC-only access."
,,A documentary ﬁlmmaker wants to reach more viewers. The ﬁlmmaker wants to automatically add subtitles and voice-overs in multiple languages to their ﬁlms. Which combination of steps will meet these requirements? (Choose two.),Use Amazon Transcribe and Amazon Translate to generate subtitles in other languages.,Use Amazon Textract and Amazon Translate to generate subtitles in other languages.,Use Amazon Polly to generate voice-overs in other languages.,Use Amazon Translate to generate voice-overs in other languages.,Use Amazon Textract to generate voice-overs in other languages.,,"A, C",1,0,0,1,0,2,3,,,,,1.2,Identify practical use cases for AI.,,242,AIF-C01,AWS Certified AI Practitioner,,"Use Amazon Transcribe to convert speech in the films to text, then Amazon Translate to create subtitles in other languages (A). Use Amazon Polly to convert the translated text into natural-sounding speech for multilingual voice-overs (C). Textract is for document OCR, not audio, and Translate alone cannot produce voice output."
,,A company wants to create a chatbot to answer employee questions about company policies. Company policies are updated frequently. The chatbot must reﬂect the changes in near real time. The company wants to choose a large language model (LLM). Which solution meets these requirements?,Fine-tune an LLM on the company policy text by using Amazon SageMaker.,Select a foundation model (FM) from Amazon Bedrock to build an application.,Create a Retrieval Augmented Generation (RAG) workﬂow by using Amazon Bedrock Knowledge Bases.,Use Amazon Q Business to build a custom Q App.,,,C,,0,0,0,1,1,,,,,,3.1,Describe design considerations for applications that use foundation models.,,243,AIF-C01,AWS Certified AI Practitioner,,"Bedrock Knowledge Bases enable a RAG workflow that retrieves the latest policy content at query time, so the chatbot reflects updates in near real time. Fine-tuning is static and slow to update, and simply selecting an FM does not address data freshness. Amazon Q Business could help, but the question focuses on choosing an LLM solution pattern for dynamic content, which RAG via Bedrock KB best satisfies."
,,A company is using supervised learning to train an AI model on a small labeled dataset that is speciﬁc to a target task. Which step of the foundation model (FM) lifecycle does this describe?,Fine-tuning,Data selection,Pre-training,Evaluation,,,A,,0,0,1,0,1,1,,,,,3.3,Describe the training and fine-tuning process for foundation models.,,244,AIF-C01,AWS Certified AI Practitioner,,"Fine-tuning adapts a pre-trained foundation model to a specific downstream task using supervised learning on a smaller, task-specific labeled dataset. Pre-training uses large, general datasets, while data selection and evaluation are separate phases. Therefore, the described process matches fine-tuning."
,,HOTSPOT - A company is developing an AI application to help the company approve or deny personal loans. The application must follow the principles of responsible AI. Select the correct responsible AI principle from the following list for each action. Select each responsible AI principle one time or not at all. Answer : Question 246 ( Exam A) A company is introducing a new feature for its application. The feature will reﬁne the style of output messages. The company will ﬁne- tune a large language model (LLM) on Amazon Bedrock to implement the feature. Which type of data does the company need to meet these requirements?,Samples of only input messages,Samples of only output messages,Samples of pairs of input and output messages,Separate samples of input and output messages,,,C,,0,0,0,1,,1,,,,,3.3,Describe the training and fine-tuning process for foundation models.,,245,AIF-C01,AWS Certified AI Practitioner,,"Fine-tuning an LLM to refine output style uses supervised learning, which requires prompt–response pairs so the model learns the mapping from inputs to desired outputs. Input-only or output-only data cannot teach this mapping, and unpaired sets do not align examples. Amazon Bedrock fine-tuning expects paired records for effective supervised fine-tuning."
,,"A healthcare company is building an AI solution to predict patient readmission within 30 days of patient discharge. The company has trained a model on historical patient data including medical history, demographics, and treatment speciﬁcations, to provide readmission predictions in real time. Which task describes AI model inference in this scenario?",Gather historical patient readmission data.,Use appropriate metrics and assess model performance.,Use data to identify patient patterns and correlations.,Use a trained model to predict patient readmission.,,,D,,0,1,0,0,1,3,,,,,1.1,Explain basic AI concepts and terminologies.,,247,AIF-C01,AWS Certified AI Practitioner,,"Inference is the process of using a trained model to make predictions on new data, which is exactly what option D describes. Option A is data collection, option B is model evaluation, and option C refers to exploratory analysis or feature discovery, not inference."
,,A ﬁnancial company wants to build workﬂows for human review of ML predictions. The company wants to deﬁne conﬁdence thresholds for its use case and adjust the thresholds over time. Which AWS service meets these requirements?,Amazon Personalize,Amazon Augmented AI (Amazon A2I),Amazon Inspector,AWS Audit Manager,,,B,,0,0,1,0,1,,,,,,4.1,Explain the development of AI systems that are responsible.,,248,AIF-C01,AWS Certified AI Practitioner,,Amazon Augmented AI (A2I) enables managed human review workflows for ML predictions with configurable confidence thresholds that can be adjusted over time. It routes low-confidence inferences to human reviewers and integrates with AWS ML services and custom models. Other options do not provide human review workflows.
,,A company wants to develop an AI assistant for employees to query internal data. Which AWS service will meet this requirement?,Amazon Rekognition,Amazon Textract,Amazon Lex,Amazon Q Business,,,D,,0,0,1,0,1,1,,,,,2.3,Describe AWS infrastructure and technologies for building generative AI applications.,,249,AIF-C01,AWS Certified AI Practitioner,,"Amazon Q Business is a managed enterprise AI assistant that securely connects to internal data sources and lets employees query organizational content with built-in permissions and RAG. Rekognition and Textract handle vision and OCR tasks, and Lex builds chat interfaces but does not provide turnkey enterprise data retrieval and governance. Thus, Amazon Q Business best meets the requirement."
,,A company wants to build and deploy ML models on AWS without writing any code. Which AWS service or feature meets these requirements?,Amazon SageMaker Canvas,Amazon Rekognition,AWS DeepRacer,Amazon Comprehend,,,A,,0,0,0,1,3,,,,,,1.3,Describe the ML development lifecycle.,,250,AIF-C01,AWS Certified AI Practitioner,,"Amazon SageMaker Canvas provides a visual, no-code interface to prepare data, build, train, and deploy ML models. Rekognition and Comprehend are pre-trained managed AI services for specific modalities, not general no-code model building. AWS DeepRacer is an educational RL platform, not a production no-code ML service."
,,A design company is using a foundation model (FM) on Amazon Bedrock to generate images for various projects. The company wants to have control over how detailed or abstract each generated image appears Which model parameter should the company modify?,Model checkpoint,Batch size,Generation step,Token length,,,C,,0,0,0,1,4,1,,,,,2.1,Explain the basic concepts of generative AI.,,251,AIF-C01,AWS Certified AI Practitioner,,"In diffusion-based image generation, the generation (inference) steps control how many denoising iterations occur, with more steps yielding finer, more detailed images and fewer steps producing more abstract outputs. Model checkpoint selects which weights to use, batch size affects throughput, and token length pertains primarily to text models rather than image detail control."
,,A ﬁnancial company has oﬃces in different countries worldwide. The company requires that all API calls between generative AI applications and foundation models (FM) must not travel across the public internet. Which AWS service should the company use?,AWS PrivateLink,Amazon Q,Amazon CloudFront,AWS CloudTrail,,,A,,0,0,0,1,1,,,,,,5.1,Explain methods to secure AI systems.,,252,AIF-C01,AWS Certified AI Practitioner,,"AWS PrivateLink provides private connectivity to AWS services and SaaS endpoints through interface VPC endpoints, ensuring traffic does not traverse the public internet. This enables generative AI applications to call foundation models (e.g., via Amazon Bedrock or SageMaker) over the AWS network only. It supports compliance and data protection requirements by enforcing network isolation."
,,An ecommerce company is deploying a chatbot. The chatbot will give users the ability to ask questions about the company’s products and receive details on users’ orders. The company must implement safeguards for the chatbot to ﬁlter harmful content from the input prompts and chatbot responses. Which AWS feature or resource meets these requirements?,Amazon Bedrock Guardrails,Amazon Bedrock Agents,Amazon Bedrock inference APIs,Amazon Bedrock custom models,,,A,,0,0,1,0,1,,,,,,2.3,Describe AWS infrastructure and technologies for building generative AI applications.,,253,AIF-C01,AWS Certified AI Practitioner,,"Amazon Bedrock Guardrails provide configurable safety controls to filter harmful content in both user prompts and model responses, including toxicity, hate, and PII controls. Bedrock Agents orchestrate tool use and workflows, not safety filtering, and inference APIs or custom models do not inherently provide content moderation safeguards."
,,A company wants to learn about generative AI applications in an experimental environment. Which solution will meet this requirement MOST cost-effectively?,Amazon Q Developer,Amazon SageMaker JumpStart,Amazon Bedrock PartyRock,Amazon Q Business,,,C,,0,0,0,1,2,1,,,,,2.3,Describe AWS infrastructure and technologies for building generative AI applications.,,254,AIF-C01,AWS Certified AI Practitioner,,"Amazon Bedrock PartyRock is a no-code, low-cost playground to experiment with generative AI apps quickly without provisioning infrastructure. It’s designed for learning and prototyping, making it the most cost-effective option. Amazon Q Developer and Q Business serve different use cases, and SageMaker JumpStart can incur higher costs and complexity."
,,A company needs to collect a large dataset to train an AI assistant in a speciﬁc content area. Which dataset will meet this requirement?,Diverse conversations that use relevant terminology,Time series data of general purpose historical sales,Sentiment analysis of news articles,Unique product IDs and corresponding user IDs,,,A,,0,0,0,1,1,,,,,,1.3,Describe the ML development lifecycle.,,255,AIF-C01,AWS Certified AI Practitioner,,"An AI assistant trained for a specific domain needs conversational data that reflects the target terminology and context. Option A provides domain-relevant dialogues necessary for effective learning. The other options do not contain conversational, domain-specific content suitable for training an assistant."
,,A ﬁnancial company is developing a generative AI application for loan approval decisions. The company needs the application output to be responsible and fair. Which solution meets these requirements?,Review the training data to check for biases. Include data from all demographics in the training data.,Use a deep learning model with many hidden layers.,Keep the model’s decision-making process a secret to protect proprietary algorithms.,Continuously monitor the model’s performance on a static test dataset,,,A,,0,0,0,1,1,,,,,,4.1,Explain the development of AI systems that are responsible.,,256,AIF-C01,AWS Certified AI Practitioner,,"Ensuring responsible and fair outcomes starts with addressing bias in data by auditing and balancing training datasets across demographics. This directly mitigates discriminatory patterns the model could learn. The other options do not address fairness: model depth and secrecy are irrelevant or harmful, and static monitoring without bias checks will miss fairness issues."
,,HOTSPOT - Select the correct AWS service or tool from the following list for each use case. Select each AWS service or tool one time or not at all. Answer : Question 258 ( Exam A) An AI practitioner who has minimal ML knowledge wants to predict employee attrition without writing code. Which Amazon SageMaker feature meets this requirement?,SageMaker Canvas,SageMaker Clarify,SageMaker Model Monitor,SageMaker Data Wrangler,,,A,,0,,,1,,,,,,,1.3,Describe the ML development lifecycle.,,257,AIF-C01,AWS Certified AI Practitioner,,"SageMaker Canvas enables users with minimal ML expertise to build, train, and generate predictions for models without writing code, which fits the employee attrition prediction use case. SageMaker Clarify handles bias/explainability, Model Monitor monitors deployed models, and Data Wrangler focuses on data preparation rather than end-to-end no-code prediction."
,,A company is using AI to improve its services. The company needs to ensure that the AI system is fair and explainable. The company wants to require training for members of the AI system development team. Which training will meet these requirements?,Training on advanced coding skills,Training on data privacy and encryption protocols,Training on bias awareness and responsible AI,Training on advanced ML algorithms,,,C,,0,0,1,0,1,,,,,,4.1,Explain the development of AI systems that are responsible.,,259,AIF-C01,AWS Certified AI Practitioner,,"Training on bias awareness and responsible AI directly addresses fairness and transparency by teaching teams how to identify, mitigate, and monitor bias and apply explainability practices. Other options (coding skills, privacy/encryption, advanced algorithms) do not specifically target fairness and explainability requirements. Therefore, responsible AI training best meets the stated goals."
,,A company has an ML model. The company wants to know how the model makes predictions. Which term refers to understanding model predictions?,Model interpretability,Model training,Model interoperability,Model performance,,,A,,0,0,1,0,1,,,,,,4.2,Recognize the importance of transparent and explainable models.,,260,AIF-C01,AWS Certified AI Practitioner,,"Model interpretability refers to understanding how a model arrives at its predictions. It focuses on transparency and explainability techniques that make model decisions understandable to humans. Model training, interoperability, and performance measure different aspects unrelated to explaining predictions."
,,A company wants to identify groups for its customers based on the customers’ demographics and buying patterns. Which algorithm should the company use to meet this requirement?,K-nearest neighbors (k-NN),K-means,Decision tree,Support vector machine,,,B,,0,0,1,0,1,,,,,,1.1,Explain basic AI concepts and terminologies.,,261,AIF-C01,AWS Certified AI Practitioner,,"K-means is an unsupervised clustering algorithm that groups similar customers into clusters based on features like demographics and purchase behavior. This use case does not require labeled outcomes, making K-means appropriate. k-NN, decision trees, and SVM are supervised methods primarily for classification/regression, not clustering."
,,A company is working on a large language model (LLM) and noticed that the LLM’s outputs are not as diverse as expected. Which parameter should the company adjust?,Temperature,Batch size,Learning rate,Optimizer type,,,A,,0,0,0,1,1,,,,,,2.1,Explain the basic concepts of generative AI.,,262,AIF-C01,AWS Certified AI Practitioner,,"Temperature scales the logits before sampling; higher values increase randomness and diversity in LLM outputs, while lower values make outputs more deterministic. Batch size, learning rate, and optimizer type are training parameters and do not directly control output diversity at inference time. Therefore, adjusting temperature is the appropriate way to influence diversity."
,,A company is using an Amazon Nova Canvas model to generate images. The model generates images successfully. The company needs to prevent the model from including speciﬁc items in the generated images. Which solution will meet this requirement?,Use a higher temperature value.,Use a more detailed prompt.,Use a negative prompt.,Use another foundation model (FM).,,,C,,0,0,0,1,1,,,,,,3.2,Choose effective prompt engineering techniques.,,263,AIF-C01,AWS Certified AI Practitioner,,"Negative prompts explicitly tell the image model what to exclude, preventing specific items from appearing in generated images. Increasing temperature only adds randomness, and adding more detail does not guarantee exclusion. Switching models is unnecessary when prompt controls can achieve the requirement."
,,HOTSPOT - A company uses ML techniques to build applications. Select the correct ML technique from the following list for each task. Select each ML technique one time. Answer : Question 265 ( Exam A) A company wants to label training datasets by using human feedback to ﬁne-tune a foundation model (FM). The company does not want to develop labeling applications or manage a labeling workforce. Which AWS service or feature meets these requirements?,Amazon SageMaker Data Wrangler,Amazon SageMaker Ground Truth Plus,Amazon Transcribe,Amazon Macie,,,B,,0,,,1,,,,,,,3.3,Describe the training and fine-tuning process for foundation models.,,264,AIF-C01,AWS Certified AI Practitioner,,"Amazon SageMaker Ground Truth Plus provides a fully managed data labeling service with an expert workforce, so you don't need to build labeling apps or manage annotators. It delivers high-quality labeled datasets suitable for fine-tuning foundation models with human feedback. The other options address data preparation (Data Wrangler), speech-to-text (Transcribe), and data security (Macie), not managed labeling."
,,An online media streaming company wants to give its customers the ability to perform natural language-based image search and ﬁltering. The company needs a vector database that can help with similarity searches and nearest neighbor queries. Which AWS service meets these requirements?,Amazon Comprehend,Amazon Personalize,Amazon Polly,Amazon OpenSearch Service,,,D,,0,0,0,1,2,,,,,,2.3,Describe AWS infrastructure and technologies for building generative AI applications.,,266,AIF-C01,AWS Certified AI Practitioner,,"Amazon OpenSearch Service offers vector search (k-NN/ANN) and filtering, making it suitable for natural language-based similarity queries. Comprehend is for NLP analysis, Personalize is for recommendations, and Polly is for text-to-speech—none provide vector database functionality."
,,HOTSPOT - A company is building an AI solution by using Amazon SageMaker AI. The company wants to use SageMaker AI features to facilitate application development. Select the correct SageMaker AI feature from the following list for each use case. Select each feature one time. Answer : Question 268 ( Exam A) A company is building a generative AI tool. The company will use internal documents to customize a foundation model (FM). Which approach will meet this requirement?,Classiﬁcation,Continued pre-training,Distillation,Regression,,,B,,0,,,1,,,,,,,3.3,Describe the training and fine-tuning process for foundation models.,,267,AIF-C01,AWS Certified AI Practitioner,,"Continued pre-training (domain-adaptive pretraining) further trains the FM on a company’s internal corpus to adapt its knowledge and style to that domain. Classification and regression are supervised prediction tasks, not methods to adapt generative FMs, and distillation focuses on compressing models rather than incorporating new domain knowledge."
,,A company is monitoring a predictive model by using Amazon SageMaker Model Monitor. The company notices data drift beyond a deﬁned threshold. The company wants to mitigate a potentially adverse impact on the predictive model. Which solution will meet these requirements?,Restart the SageMaker AI endpoint.,Adjust the monitoring sensitivity.,Re-train the model with fresh data.,Set up experiments tracking.,,,C,,0,0,0,1,2,1,,,,,1.3,Describe the ML development lifecycle.,,269,AIF-C01,AWS Certified AI Practitioner,,"Data drift indicates the input distribution has changed, which can degrade model performance. The correct mitigation is to retrain the model with recent data so it reflects the new distribution. Restarting endpoints, tweaking sensitivity, or tracking experiments do not address the underlying drift."
,,A ﬁnancial company uses a generative AI model to assign credit limits to new customers. The company wants to make the decision-making process of the model more transparent to its customers. Which solution meets these requirements?,Use a rule-based system instead of an ML model.,Apply explainable AI techniques to show customers which factors inﬂuenced the model’s decision.,Develop an interactive UI for customers and provide clear technical explanations about the system.,Increase the accuracy of the model to reduce the need for transparency.,,,B,,0,0,0,1,1,,,,,,4.2,Recognize the importance of transparent and explainable models.,,270,AIF-C01,AWS Certified AI Practitioner,,"Explainable AI techniques (e.g., feature attribution like SHAP/LIME) show which inputs most influenced the credit limit decision, directly addressing transparency. Alternatives either change the system type, provide generic technical details without decision rationale, or improve accuracy without improving explainability."
,,"A company deployed a model to production. After 4 months, the model inference quality degraded. The company wants to receive a notiﬁcation if the model inference quality degrades. The company also wants to ensure that the problem does not happen again. Which solution will meet these requirements?",Retrain the model. Monitor model drift by using Amazon SageMaker Clarify.,Retrain the model. Monitor model drift by using Amazon SageMaker Model Monitor.,Build a new model. Monitor model drift by using Amazon SageMaker Feature Store.,Build a new model. Monitor model drift by using Amazon SageMaker JumpStart.,,,B,,0,0,1,0,3,,,,,,1.3,Describe the ML development lifecycle.,,271,AIF-C01,AWS Certified AI Practitioner,,"Amazon SageMaker Model Monitor tracks data and model quality drift in production and can trigger alerts when inference quality degrades. Retraining adapts the model to new data to restore performance and reduce future degradation. Clarify is for bias/explainability, Feature Store manages features, and JumpStart offers model templates, not drift monitoring with notifications."
,,Which option is an example of unsupervised learning?,A model that groups customers based on their purchase history,A model that classiﬁes images as dogs or cats,A model that predicts a house’s price based on various features,A model that learns to play chess by using trial and error,,,A,,0,0,1,0,1,,,,,,1.1,Explain basic AI concepts and terminologies.,,272,AIF-C01,AWS Certified AI Practitioner,,"Option A describes clustering customers based on purchase history, which is unsupervised learning because there are no labeled outcomes. Option B (dog/cat classification) and Option C (house price prediction) are supervised learning tasks with labeled data, and Option D is reinforcement learning via trial and error with rewards."
,,A company is evaluating several large language models (LLMs) for a text summarization task. The company needs to select a metric to evaluate the quality of the summaries that the LLMs generate. Which metric will meet this requirement?,Recall,Area under the ROC curve (AUC),Recall-Oriented Understudy for Gisting Evaluation (ROUGE),Mean squared error (MSE),,,C,,0,0,1,0,1,,,,,,3.4,Describe methods to evaluate foundation model performance.,,273,AIF-C01,AWS Certified AI Practitioner,,"ROUGE is a standard metric for text summarization that measures n-gram overlap between generated and reference summaries, emphasizing recall. Recall alone is too generic, AUC applies to classification, and MSE is for regression, not summarization quality. Therefore, ROUGE best meets the requirement."
,,A research group wants to test different generative AI models to create research papers. The research group has deﬁned a prompt and needs a method to assess the models’ output. The research group wants to use a team of scientists to perform the output assessments. Which solution will meet these requirements?,Use automatic evaluation on Amazon Personalize.,Use content moderation on Amazon Rekognition.,Use model evaluation on Amazon Bedrock.,Use sentiment analysis on Amazon Comprehend.,,,C,,0,0,1,0,1,1,,,,,3.4,Describe methods to evaluate foundation model performance.,,274,AIF-C01,AWS Certified AI Practitioner,,"Amazon Bedrock Model Evaluation supports both automated and human-based evaluations, enabling side-by-side comparisons of generative model outputs using a custom workforce such as a team of scientists. The other services do not provide generative model evaluation workflows: Personalize is for recommendations, Rekognition moderation targets images/video, and Comprehend sentiment analysis is not suited for structured human evaluation of LLM outputs."
,,HOTSPOT - An ecommerce company is developing a generative AI solution to create personalized product recommendations for its application users. The company wants to track how effectively the AI solution increases product sales and user engagement in the application. Select the correct business metric from the following list for each business goal. Each business metric should be selected one time. Answer : Question 276 ( Exam A) An AI practitioner wants to evaluate ML models. The AI practitioner wants to provide explanations of model predictions to customers and stakeholders. Which AWS service or feature will meet these requirements?,Amazon QuickSight,Amazon Comprehend,AWS Trusted Advisor,Amazon SageMaker Clarify,,,D,,0,,,1,,,,,,,4.2,Recognize the importance of transparent and explainable models.,,275,AIF-C01,AWS Certified AI Practitioner,,"Amazon SageMaker Clarify provides built-in model explainability using feature attributions (such as SHAP) and bias detection for datasets and models. It enables generating explanations of individual predictions and global feature importance for stakeholders, supporting transparent and accountable AI. The other options do not offer ML explainability features."
,,Sentiment analysis is a subset of which broader ﬁeld of AI?,Computer vision,Robotics,Natural language processing (NLP),Time series forecasting,,,C,,0,0,1,0,1,,,,,,1.1,Explain basic AI concepts and terminologies.,,277,AIF-C01,AWS Certified AI Practitioner,,"Sentiment analysis involves interpreting and classifying opinions or emotions in text, which is a core task in natural language processing (NLP). It uses linguistic features and models to determine sentiment polarity (e.g., positive/negative/neutral). Computer vision handles images, robotics focuses on physical agents, and time series forecasting predicts numerical sequences, not textual sentiment."
,,A company wants to set up private access to Amazon Bedrock APIs from the company’s AWS account. The company also wants to protect its data from internet exposure. Which solution meets these requirements?,Use Amazon CloudFront to restrict access to the company’s private content.,Use AWS Glue to set up data encryption across the company’s data catalog.,Use AWS Lake Formation to manage centralized data governance and cross-account data sharing.,Use AWS PrivateLink to conﬁgure a private connection between the company’s VPC and Amazon Bedrock.,,,D,,0,0,0,1,1,,,,,,2.3,Describe AWS infrastructure and technologies for building generative AI applications.,,278,AIF-C01,AWS Certified AI Practitioner,,"AWS PrivateLink provides private connectivity between a VPC and Amazon Bedrock without traversing the public internet, reducing data exposure risk. CloudFront is for content delivery, not private API access; AWS Glue encryption pertains to data catalogs; and Lake Formation handles data governance, not private network access to Bedrock APIs."
,,A company receives a large amount of unstructured user feedback in text format. The company wants to analyze the sentiment of the user feedback. Which solution will meet these requirements?,Use a large language model (LLM) to perform natural language processing (NLP) for sentiment analysis.,"Use a regression algorithm to classify the feedback based on predeﬁned categories. Then, analyze user sentiment.",Use a recommendation engine algorithm to detect user sentiment.,Use a time series algorithm to predict user sentiment based on past feedback.,,,A,,0,0,1,0,2,2,,,,,1.2,Identify practical use cases for AI.,,279,AIF-C01,AWS Certified AI Practitioner,,"LLMs can perform NLP classification tasks like sentiment analysis directly on unstructured text and scale well for varied language. Regression predicts continuous values, recommendation engines suggest items, and time series models forecast over time; none are suited to classifying text sentiment. Therefore, using an LLM for NLP sentiment analysis best meets the requirements."
,,HOTSPOT - A company wants to improve multiple ML models. Select the correct technique from the following list of use cases. Each technique should be selected one time or not at all. Answer : Question 281 ( Exam A) A company wants to create an AI solution to generate images and descriptions for a product catalog. The company needs to select a foundation model (FM) for this solution. The company must consider the output types of each FM. Which FM characteristic is the company evaluating?,Latency,Model size,Model customization,Modality,,,D,,0,,,1,,,,,,,3.1,Describe design considerations for applications that use foundation models.,,280,AIF-C01,AWS Certified AI Practitioner,,"Modality refers to the kinds of inputs and outputs a foundation model supports, such as text, images, or both (multimodal). Generating both product images and descriptions requires a model that supports image and text outputs. Latency, model size, and customization are important but do not determine the output types the model can produce."
,,"A company wants to use an ML model to analyze customer reviews on social media. The model must determine if each review has a neutral, positive, or negative sentiment. Which model evaluation strategy will meet these requirements?",Open-ended generation,Text summarization,Machine translation,Classiﬁcation,,,D,,0,0,1,0,1,,,,,,1.1,Explain basic AI concepts and terminologies.,,282,AIF-C01,AWS Certified AI Practitioner,,"Sentiment analysis that labels text as positive, neutral, or negative is a classic classification problem with discrete categories. Classification models output one of several predefined labels. The other options (generation, summarization, translation) produce free-form text or transform content, not categorical labels."
,,HOTSPOT - Select the correct AI term from the following list for each statement. Each AI term should be selected one time. Answer : Question 284 ( Exam A) Which option is an example of unsupervised learning?,Clustering data points into groups based on their similarity,Training a model to recognize images of animals,Predicting the price of a house based on the house’s features,Generating human-like text based on a given prompt,,,A,,0,0,0,1,1,,,,,,1.1,Explain basic AI concepts and terminologies.,,283,AIF-C01,AWS Certified AI Practitioner,,"Unsupervised learning discovers structure in unlabeled data, and clustering is a primary unsupervised technique that groups similar data points. Training to recognize animals and predicting house prices are supervised tasks requiring labeled data. Generating text from a prompt is a generative AI task, not a classic unsupervised learning example in this context."
,,An online learning company with large volumes of education materials wants to use enterprise search. Which AWS service meets these requirements?,Amazon Comprehend,Amazon Textract,Amazon Kendra,Amazon Personalize,,,C,,0,0,1,0,1,,,,,,2.3,Describe AWS infrastructure and technologies for building generative AI applications.,,285,AIF-C01,AWS Certified AI Practitioner,,"Amazon Kendra is a managed intelligent enterprise search service that enables natural-language search across large document repositories. It offers connectors, indexing, and relevance tuning suited for extensive educational materials. Comprehend performs text analytics, Textract extracts text from documents, and Personalize focuses on recommendations, not search."
,,A company creates video content. The company wants to use generative AI to generate new creative content and to reduce video creation time. Which solution will meet these requirements in the MOST operationally eﬃcient way?,Use the Amazon Titan Image Generator model on Amazon Bedrock to generate intermediate images. Use video editing software to create videos.,Use the Amazon Nova Canvas model on Amazon Bedrock to generate intermediate images. Use video editing software to create videos.,Use the Amazon Nova Reel model on Amazon Bedrock to generate videos.,Use the Amazon Nova Pro model on Amazon Bedrock to generate videos.,,,C,,0,0,0,1,2,1,,,,,2.3,Describe AWS infrastructure and technologies for building generative AI applications.,,286,AIF-C01,AWS Certified AI Practitioner,,"Amazon Nova Reel is purpose-built for video generation on Amazon Bedrock, directly producing videos and minimizing manual steps. Titan Image Generator and Nova Canvas are image-focused and would require separate video editing, increasing operational overhead. Nova Pro is a multimodal reasoning model, not specialized for generating videos."
,,A company is training ML models on datasets. The datasets contain some classes that have more examples than other classes. The company wants to measure how well the model balances detecting and labeling the classes. Which metric should the company use?,Accuracy,Recall,Precision,F1 score,,,D,,0,0,1,0,2,1,,,,,1.1,Explain basic AI concepts and terminologies.,,287,AIF-C01,AWS Certified AI Practitioner,,"F1 score is the harmonic mean of precision and recall, providing a single measure that balances both. This is particularly useful for imbalanced datasets where accuracy can be misleading and optimizing only precision or only recall is insufficient."
,,A company is analyzing ﬁnancial transaction records. The company categorizes the records as either personal or business. The company inserts the categories into the transaction records. Which data preparation step does this describe?,Data encoding,Data labeling,Data normalization,Data balancing,,,B,,0,0,0,1,1,,,,,,1.3,Describe the ML development lifecycle.,,288,AIF-C01,AWS Certified AI Practitioner,,"Adding 'personal' or 'business' tags to transaction records is data labeling used for supervised learning. Data encoding converts categories to numeric formats, normalization scales numeric values, and balancing adjusts class distributions—none of which describe inserting category labels."
,,A company wants to extract key insights from large policy documents to increase employee eﬃciency. Which generative AI strategy meets this requirement?,Regression,Clustering,Summarization,Classiﬁcation,,,C,,0,0,0,1,1,,,,,,2.2,Understand the capabilities and limitations of generative AI for solving business problems.,,289,AIF-C01,AWS Certified AI Practitioner,,"Summarization condenses lengthy documents into concise key points, directly boosting efficiency for readers. Regression, clustering, and classification are traditional ML techniques and do not generate condensed narratives or insights from long texts. Therefore, summarization is the appropriate generative AI strategy."
,,A company is using Amazon SageMaker to deploy a model that identiﬁes if social media posts contain certain topics. The company needs to show how different input features inﬂuence model behavior. Which SageMaker feature meets these requirements?,SageMaker Canvas,SageMaker Clarify,SageMaker Feature Store,SageMaker Ground Truth,,,B,,0,0,1,0,1,2,,,,,4.2,Recognize the importance of transparent and explainable models.,,290,AIF-C01,AWS Certified AI Practitioner,,"SageMaker Clarify provides explainability by generating feature attributions (e.g., SHAP values) to show how each input feature impacts model predictions. It also offers bias detection, supporting responsible AI needs. Canvas, Feature Store, and Ground Truth do not provide model explanation of feature influence."
,,HOTSPOT - An AI practitioner is determining the appropriate data type for various use cases. Select the correct data type from the following list for each use case. Select each data type one time. Answer : Question 292 ( Exam A) A company wants to assess internet quality in remote areas of the world. The company needs to collect internet speed data and store the data in Amazon RDS. The company will analyze internet speed variation throughout each day. The company wants to create an AI model to predict potential internet disruptions. Which type of data should the company collect for this task?,Tabular data,Text data,Time series data,Audio data,,,C,,0,,,1,,,,,,,1.1,Explain basic AI concepts and terminologies.,,291,AIF-C01,AWS Certified AI Practitioner,,"Internet speed measurements collected at regular intervals are time-stamped, making them time series data. This format enables analysis of daily variations and trend/seasonality patterns. Time series data is ideal for forecasting and detecting potential disruptions over time."
,,A company wants to build an ML model to detect abnormal patterns in sensor data. The company does not have labeled data for training. Which ML method will meet these requirements?,Linear regression,Classiﬁcation,Decision tree,Autoencoders,,,D,,0,0,1,0,1,2,,,,,1.1,Explain basic AI concepts and terminologies.,,293,AIF-C01,AWS Certified AI Practitioner,,"Autoencoders are unsupervised models that learn to reconstruct normal data patterns; anomalies produce higher reconstruction errors, enabling detection without labels. Linear regression, classification, and decision trees are supervised and require labeled data. Therefore, autoencoders best fit unlabeled anomaly detection in sensor data."
,,A company uses Amazon Bedrock to implement a generative AI assistant on a website. The AI assistant helps customers with product recommendations and purchasing decisions. The company wants to measure the direct impact of the AI assistant on sales performance. Which metric will meet these requirements?,The conversion rate of customers who purchase products after AI assistant interactions.,The number of customer interactions with the AI assistant,Sentiment analysis scores from customer feedback after AI assistant interactions,Natural language understanding accuracy rates,,,A,,0,0,0,1,1,,,,,,3.4,Describe methods to evaluate foundation model performance.,,294,AIF-C01,AWS Certified AI Practitioner,,"Conversion rate after AI assistant interactions directly links the assistant’s influence to purchase behavior, quantifying sales impact. Interaction counts, sentiment, or NLU accuracy reflect engagement, satisfaction, or model performance but do not measure revenue conversion outcomes."
,,Which AWS service or feature stores embeddings in a vector database for use with foundation models (FMs) and Retrieval Augmented Generation (RAG)?,Amazon SageMaker Ground Truth,Amazon OpenSearch Service,Amazon Transcribe,Amazon Textract,,,B,,0,0,1,0,2,1,,,,,2.3,Describe AWS infrastructure and technologies for building generative AI applications.,,295,AIF-C01,AWS Certified AI Practitioner,,"Amazon OpenSearch Service supports vector search (k-NN/ANN) and can store embeddings for similarity retrieval used in RAG workflows with foundation models. Ground Truth is for data labeling, Transcribe is for speech-to-text, and Textract is for document OCR, none of which provide vector database capabilities."
,,Which scenario represents a practical use case for generative AI?,Using an ML model to forecast product demand,Employing a chatbot to provide human-like responses to customer queries in real time,Using an analytics dashboard to track website traﬃc and user behavior,Implementing a rule-based recommendation engine to suggest products to customers,,,B,,0,0,0,1,1,,,,,,2.2,Understand the capabilities and limitations of generative AI for solving business problems.,,296,AIF-C01,AWS Certified AI Practitioner,,"Generative AI is well-suited for producing human-like text, making real-time conversational chatbots a prime use case. Option A is predictive forecasting, C is descriptive analytics, and D is a deterministic rule-based system—none of which involve text generation."
,,A company is using Amazon Bedrock for a generative AI solution. The solution must integrate a service with vector database storage and vector search capabilities. Which AWS service will meet these requirements?,Amazon DynamoDB,Amazon OpenSearch Service,Amazon ElastiCache,Amazon Redshift,,,B,,0,0,0,1,1,,,,,,2.3,Describe AWS infrastructure and technologies for building generative AI applications.,,297,AIF-C01,AWS Certified AI Practitioner,,"Amazon OpenSearch Service includes a vector engine with k-NN/ANN similarity search, enabling storage and retrieval of embeddings for RAG and generative AI use cases. DynamoDB, ElastiCache, and Redshift are not primary managed services for vector search in this context. Therefore, OpenSearch Service best meets the vector database and vector search requirements."
,,A media streaming platform wants to provide movie recommendations to users based on the users’ account history. Which AWS service meets these requirements?,Amazon Polly,Amazon Comprehend,Amazon Transcribe,Amazon Personalize,,,D,,0,0,0,1,3,,,,,,1.2,Identify practical use cases for AI.,,298,AIF-C01,AWS Certified AI Practitioner,,"Amazon Personalize is a fully managed service for building real-time personalized recommendations from user interaction history. Amazon Polly is for text-to-speech, Amazon Comprehend is for NLP text analysis, and Amazon Transcribe is for speech-to-text—none of which provide recommendation systems."
,,A company has developed an ML model to approve or reject loan applications. The model’s decision-making process must be transparent and explainable to comply with regulatory requirements. The company must document the decision-making process for audit purposes. Which solution will meet these requirements?,Amazon Textract,Amazon SageMaker Model Card,AWS Cloud Formation,Amazon Comprehend,,,B,,0,0,0,1,1,,,,,,4.2,Recognize the importance of transparent and explainable models.,,299,AIF-C01,AWS Certified AI Practitioner,,"Amazon SageMaker Model Card provides a standardized, governed way to document model purpose, data, training, evaluation metrics, and risk/mitigation details for auditability and explainability. This supports regulatory compliance by making the model’s decision-making process transparent. The other services do not provide model documentation or explainability capabilities."
,,HOTSPOT - A company is building a generative AI application and is reviewing foundation models (FMs). The company needs to consider multiple FM characteristics. Select the correct FM characteristic from the following list for each deﬁnition. Each FM characteristic should be selected one time. Answer : Question 301 ( Exam A) A company is using large language models (LLMs) to develop online tutoring applications. The company needs to apply conﬁgurable safeguards to the LLMs. These safeguards must ensure that the LLMs follow standard safety rules when creating applications. Which solution will meet these requirements with the LEAST effort?,Amazon Bedrock playgrounds,Amazon SageMaker Clarify,Amazon Bedrock Guardrails,Amazon SageMaker Jumpstart,,,C,,0,,,1,,,,,,,2.3,Describe AWS infrastructure and technologies for building generative AI applications.,,300,AIF-C01,AWS Certified AI Practitioner,,"Amazon Bedrock Guardrails provides configurable, managed safety controls (content filtering, topic blocking, PII redaction, prompt injection protections) that apply across Bedrock-hosted models with minimal setup. Bedrock playgrounds are for experimentation, not enforcement; SageMaker Clarify focuses on bias/explainability; and SageMaker JumpStart offers model/catalog templates but not runtime safety guardrails."
,,A company is exploring Amazon Nova models in Amazon Bedrock. The company needs a multimodal model that supports multiple languages. Which Nova model will meet these requirements MOST cost-effectively?,Nova Lite,Nova Pro,Nova Canvas,Nova Reel,,,A,,0,0,1,0,1,,,,,,2.3,Describe AWS infrastructure and technologies for building generative AI applications.,,302,AIF-C01,AWS Certified AI Practitioner,,"Nova Lite is a cost-effective, general-purpose multimodal model on Amazon Bedrock that supports multiple languages. Nova Pro offers higher capability at higher cost, Nova Canvas focuses on image generation/editing, and Nova Reel targets video generation. Therefore, Nova Lite best meets the multimodal and multilingual needs most economically."
,,"A company is building a new generative AI chatbot. The chatbot uses an Amazon Bedrock foundation model (FM) to generate responses. During testing, the company notices that the chatbot is prone to prompt injection attacks. What can the company do to secure the chatbot with the LEAST implementation effort?",Fine-tune the FM to avoid harmful responses.,Use Amazon Bedrock Guardrails content ﬁlters and denied topics.,Change the FM to a more secure FM.,Use chain-of-thought prompting to produce secure responses.,,,B,,0,0,0,1,1,,,,,,5.1,Explain methods to secure AI systems.,,303,AIF-C01,AWS Certified AI Practitioner,,"Amazon Bedrock Guardrails provide configurable content filters and denied topics to mitigate prompt injection with minimal implementation effort. Fine-tuning is costly and not a reliable defense against injection attacks, switching models offers no assurance, and chain-of-thought prompting is not a security control."
,,What does inference refer to in the context of AI?,The process of creating new AI algorithms,The use of a trained model to make predictions or decisions on unseen data,The process of combining multiple AI models into one model,The method of collecting training data for AI systems,,,B,,0,0,1,0,2,,,,,,1.1,Explain basic AI concepts and terminologies.,,304,AIF-C01,AWS Certified AI Practitioner,,"Inference is the phase where a trained model is applied to new, unseen data to generate predictions or decisions. This contrasts with training, which learns model parameters from labeled data. Options A, C, and D describe different activities not related to the prediction phase."
,,"A company wants to build an AI assistant to provide responses to user queries. The AI assistant must evaluate speciﬁc data sources, query external APIs, generate response options, and compare and prioritize response options. Which Amazon Bedrock feature or resource will meet these requirements?",Prompt Management,Response streaming,Knowledge Bases,Agents,,,D,,0,0,1,0,1,1,,,,,2.3,Describe AWS infrastructure and technologies for building generative AI applications.,,305,AIF-C01,AWS Certified AI Practitioner,,"Agents for Amazon Bedrock orchestrate multi-step tasks, plan and call external APIs/tools, access data sources (including knowledge bases), and synthesize results. They can evaluate candidate responses and decide next actions, satisfying the requirements to query, generate options, and prioritize them. Prompt Management, Response streaming, and Knowledge Bases alone do not provide this tool orchestration and decision-making."
,,An AI practitioner notices a large language model (LLM) is generating different responses for the same input across multiple invocations. Which risk of AI does this describe?,Hallucinations,Nondeterminism,Accuracy,Multimodality,,,B,,0,0,0,1,1,,,,,,2.2,Understand the capabilities and limitations of generative AI for solving business problems.,,306,AIF-C01,AWS Certified AI Practitioner,,"LLMs often use probabilistic decoding (e.g., temperature, top-p), which can produce different valid responses to the same input—this is nondeterminism. This behavior is distinct from hallucinations (fabricated facts), accuracy (correctness of outputs), and multimodality (handling multiple data types). Therefore, the described risk is nondeterminism."
,,A company is building a generative AI application on AWS. The application will help improve reading comprehension for students. The application must give students the ability to add illustrations to stories. Which solution will meet this requirement?,Use Amazon Bedrock Stable Diffusion 3.5 Large to generate images based on text inputs.,Use Amazon Polly to create an audiobook based on story texts.,Use Amazon Rekognition to analyze image contents and detect text attributes.,Create a standard prompt template. Use Amazon Q Business to illustrate stories.,,,A,,0,0,1,0,1,,,,,,2.3,Describe AWS infrastructure and technologies for building generative AI applications.,,307,AIF-C01,AWS Certified AI Practitioner,,"Amazon Bedrock provides access to image generation foundation models like Stable Diffusion to create illustrations from text prompts, meeting the requirement. Amazon Polly converts text to speech, not images; Amazon Rekognition analyzes existing images; and Amazon Q Business focuses on enterprise search and Q&A, not image generation."
,,A healthcare company wants to analyze patient data. The data was gathered over the previous year to detect patterns in disease outbreaks. The company needs to create a trend analysis report for each month to present to public health oﬃcials. The company must provide insights into patient data from the most recent month of the current year. Which inference method will meet these requirements MOST cost-effectively?,Real-time inference,Batch transform,Serverless inference,Asynchronous inference,,,B,,0,0,0,1,1,,,,,,1.3,Describe the ML development lifecycle.,,308,AIF-C01,AWS Certified AI Practitioner,,"Batch transform is ideal for periodic, large-scale, offline processing (e.g., monthly trend reports) and avoids the ongoing cost of hosting real-time endpoints. It processes data from S3 and writes results back to S3 on demand, making it cost-effective for monthly analyses including the most recent month's data. Real-time, serverless, and asynchronous inference maintain endpoints and are better suited for on-demand or latency-sensitive use cases, increasing cost unnecessarily here."
,,HOTSPOT - Select and order the steps from the following list to correctly describe the ML lifecycle for a new custom model. Select each step one time. Answer : Question 310 ( Exam A) A company acquires International Organization for Standardization (ISO) accreditation to manage AI risks and to use AI responsibly. What does this accreditation reﬂect about the company?,All members of the company are ISO certiﬁed.,All AI systems that the company uses are ISO certiﬁed.,All AI application team members are ISO certiﬁed.,The company’s development framework is ISO certiﬁed.,,,D,,0,,,1,,,,,,,5.2,Recognize governance and compliance regulations for AI systems.,,309,AIF-C01,AWS Certified AI Practitioner,,"ISO accreditation certifies that an organization’s management system or development framework meets defined standards for governance and risk management, not that individual people or specific AI systems are certified. It reflects that the company’s processes for building and managing AI are compliant with ISO standards. Therefore, the company’s development framework is ISO certified."
,,"HOTSPOT - Select the correct prompt engineering technique from the following list for each description. Select each prompt engineering technique one time or not at all. Answer : Question 312 ( Exam A) A company is developing an ML model to predict heart disease risk. The model uses patient data, such as age, cholesterol, blood pressure, smoking status, and exercise habits. The dataset includes a target value that indicates whether a patient has heart disease. Which ML technique will meet these requirements?",Unsupervised learning,Supervised learning,Reinforcement learning,Semi-supervised learning,,,B,,0,,,1,,,,,,,1.1,Explain basic AI concepts and terminologies.,,311,AIF-C01,AWS Certified AI Practitioner,,"Because the dataset contains a target label indicating heart disease presence, the appropriate approach is supervised learning. Unsupervised learning lacks labels, reinforcement learning relies on rewards, and semi-supervised requires mostly unlabeled data with a small labeled subset, which is not described here."
,,HOTSPOT - A company periodically updates its product database by manually uploading digital product guides. The product guides contain text and images. The company wants to automate this task by using generative AI. Select and order the steps from the following list to automate the database update task by using generative AI. Select each step one time. Answer : Question 314 ( Exam A) A company has guidelines for data storage and deletion. Which data governance strategy does this describe?,Data de-identiﬁcation,Data quality standards,Data retention,Log storage,,,C,,0,,,1,,,,,,,5.2,Recognize governance and compliance regulations for AI systems.,,313,AIF-C01,AWS Certified AI Practitioner,,"Data retention defines how long data is stored and when it must be deleted, matching guidelines for storage and deletion. Data de-identification is about anonymization, data quality standards address accuracy and completeness, and log storage pertains specifically to logs rather than overall retention policies."
,,A company needs to apply numerical transformations to a set of images to transpose and rotate the images. Which solution will meet these requirements in the MOST operationally eﬃcient way?,Create a deep neural network by using the images as input.,Create an AWS Lambda function to perform the transformations.,Use an Amazon Bedrock large language model (LLM) with a high temperature.,Use AWS Glue Data Quality to make corrections to each image.,,,B,,0,0,0,1,3,,,,,,1.2,Identify practical use cases for AI.,,315,AIF-C01,AWS Certified AI Practitioner,,"Lambda is a serverless, scalable, and cost-efficient way to run simple, deterministic image transformations like rotate and transpose. A deep neural network or an LLM is unnecessary and inefficient for this task, and AWS Glue Data Quality targets data quality on tabular data, not image manipulation."
,,An AI practitioner is writing software code. The AI practitioner wants to quickly develop a test case and create documentation for the code. Which solution will meet these requirements with the LEAST effort?,Upload the code to an online coding assistant.,Develop an application to use foundation models (FMs).,Use Amazon Q Developer in an integrated development environment (IDE).,"Research and write test cases. Then, create test cases and add documentation.",,,C,,0,0,1,0,2,,,,,,2.2,Understand the capabilities and limitations of generative AI for solving business problems.,,316,AIF-C01,AWS Certified AI Practitioner,,"Amazon Q Developer integrates into the IDE to automatically generate unit tests and documentation from code, minimizing developer effort. Building a custom FM app (B) is excessive, and manual creation (D) is time-consuming. A generic online assistant (A) lacks tight IDE integration and may introduce workflow and security frictions."
,,A company is developing a generative AI application to automatically generate product descriptions for an ecommerce website. The product descriptions must consist of paragraphs of text that are consistent in style and tone. The application must generate thousands of unique descriptions each day. Which type of generative model will meet these requirements?,A variational autoencoder (VAE) model,A transformer-based model,A diffusion model,A generative adversarial network (GAN) model,,,B,,0,0,1,0,2,3,,,,,2.1,Explain the basic concepts of generative AI.,,317,AIF-C01,AWS Certified AI Practitioner,,"Transformer-based models (LLMs) are state-of-the-art for generating coherent, stylistically consistent long-form text and scale well to high-throughput workloads. VAEs, GANs, and diffusion models are primarily used for images, audio, or latent representation learning rather than high-quality text generation. Therefore, a transformer-based model best fits the requirement to generate thousands of unique product descriptions daily."
,,"An AI practitioner has trained a model on a training dataset. The model performs well on the training data. However, the model does not perform well on evaluation data. What is the MOST likely cause of this issue?",The model is underﬁt.,The model requires prompt engineering.,The model is biased.,The model is overﬁt.,,,D,,0,0,1,0,1,1,,,,,1.1,Explain basic AI concepts and terminologies.,,318,AIF-C01,AWS Certified AI Practitioner,,"Overfitting occurs when a model learns training data too closely, capturing noise rather than general patterns, resulting in strong training performance but poor evaluation performance. Underfitting would show poor performance on both training and evaluation sets. Bias or prompt engineering are not the primary causes of the specific train/eval performance gap described."
,,Place these IAM policy evaluation steps in order from first to last:,Check resource-based policies,Check identity-based policies,Check SCPs,Check explicit deny,,,"D,C,B,A",2,0,0,1,0,5,4,,,,,5.2,Recognize governance and compliance regulations for AI systems.,,319,AIF-C01,AWS Certified AI Practitioner,,"AWS first short-circuits on any explicit Deny, which overrides all other policies. Then it evaluates Organizations SCPs as guardrails—if the action isn’t allowed by the SCP, it’s denied regardless of other policies. If still not denied, AWS checks for an Allow in identity-based policies and then in resource-based policies."
,,Which troubleshooting step should a security engineer take when Amazon CloudWatch Logs stop receiving data from an EC2 instance?,Verify that the CloudWatch agent is running on the instance,Increase the EBS volume size,Update the instance IAM role to add S3 access,Restart the EC2 instance,,,A,0,0,0,1,0,1,0,,,,,1.3,"Troubleshoot security monitoring, logging, and alerting",,1001,SCS-C03,AWS Security Specialist,,"CloudWatch Logs depend on the CloudWatch agent running and having correct permissions. If the agent is stopped, no logs are delivered."
,,A company notices no new log streams appearing in CloudWatch Logs from its payment-processing EC2 fleet. What is the most effective first diagnostic action?,Review CloudWatch agent logs on the instance,Resize the Auto Scaling group,Enable VPC Flow Logs,Update the CloudTrail trail name,,,A,0,0,0,1,0,1,0,,,,,1.3,"Troubleshoot security monitoring, logging, and alerting",,1002,SCS-C03,AWS Security Specialist,,Reviewing the agent logs helps identify misconfigurations or delivery failures at the source before checking downstream services.
,,A security team reports missing VPC Flow Logs for a production VPC. What should the engineer do first?,Use aws ec2 describe-flow-logs to confirm flow log configuration,Reboot all EC2 instances,Enable AWS Config recording,Rotate IAM keys,,,A,0,0,0,1,0,1,0,,,,,1.3,"Troubleshoot security monitoring, logging, and alerting",,1003,SCS-C03,AWS Security Specialist,,describe-flow-logs shows whether the flow logs are correctly configured and active. This is the fastest way to verify if logging is enabled.
,,A company reports that VPC Flow Log data stopped delivering to CloudWatch. Which diagnostic step is MOST appropriate?,Check whether the log group still exists in CloudWatch,Add a NAT Gateway to the VPC,Enable S3 versioning,Update EC2 instance metadata options,,,A,0,0,0,1,0,1,0,,,,,1.3,"Troubleshoot security monitoring, logging, and alerting",,1004,SCS-C03,AWS Security Specialist,,"If the target log group is deleted or misconfigured, flow logs cannot be delivered and will silently fail."
,,A SIEM tool shows no CloudTrail API events for 24 hours. What is the FIRST step the security engineer should take?,Run aws cloudtrail get-trail-status to confirm delivery,Create a new CloudTrail trail,Disable and re-enable SSE-KMS encryption,Change the S3 bucket lifecycle policy,,,A,0,0,0,1,0,1,0,,,,,1.3,"Troubleshoot security monitoring, logging, and alerting",,1005,SCS-C03,AWS Security Specialist,,get-trail-status quickly verifies whether CloudTrail is actively delivering events and identifies any failures.
,,A security team suspects that CloudTrail stopped sending events to its S3 bucket. What is the correct next action?,Check the S3 bucket policy for CloudTrail write permissions,Terminate unused EC2 instances,Enable AWS Backup on the bucket,Switch the SIEM ingestion format,,,A,0,0,1,0,0,0,1,,,,,1.3,"Troubleshoot security monitoring, logging, and alerting",,1006,SCS-C03,AWS Security Specialist,,"If CloudTrail loses S3 write access, it cannot deliver logs. Checking the bucket policy is essential."
,,"After fixing log delivery issues, what is the best verification step?",Confirm that new log streams appear in CloudWatch Logs,Resize the S3 bucket,Update IAM password policies,Rotate TLS certificates,,,A,0,0,0,1,0,1,0,,,,,1.3,"Troubleshoot security monitoring, logging, and alerting",,1007,SCS-C03,AWS Security Specialist,,Fresh log streams indicate that log ingestion has resumed successfully. This verifies that the pipeline is functioning.
,,A DevOps engineer wants to confirm whether CloudTrail events are now visible after a fix. What action should they take?,Trigger an API event and verify that it appears in CloudTrail,Modify the default VPC settings,Run EC2 run-instances without IAM roles,Upgrade the S3 storage class,,,A,0,0,0,1,0,1,0,,,,,1.3,"Troubleshoot security monitoring, logging, and alerting",,1008,SCS-C03,AWS Security Specialist,,Manually generating an API event provides a quick confirmation that CloudTrail logging has resumed.
,,How can an engineer prevent future misconfigurations that cause missing CloudWatch Logs or Flow Logs?,Create AWS Config rules to detect disabled or malformed logging,Use Auto Scaling scheduled actions,Enable Amazon Macie,Deploy an Elastic IP rotation schedule,,,A,0,0,0,1,0,1,0,,,,,1.3,"Troubleshoot security monitoring, logging, and alerting",,1009,SCS-C03,AWS Security Specialist,,AWS Config rules continuously check for configuration drift and can alert or remediate logging misconfigurations.
,,A company wants proactive alerting when CloudTrail stops delivering log files. What should they implement?,A CloudWatch alarm on the CloudTrail DeliveryErrors metric,An S3 access log bucket,A Lambda@Edge function,An Aurora read replica,,,A,0,0,0,1,0,1,0,,,,,1.3,"Troubleshoot security monitoring, logging, and alerting",,1010,SCS-C03,AWS Security Specialist,,The DeliveryErrors metric alerts the team immediately if CloudTrail cannot deliver logs to S3.
,,A security engineer is preparing documentation after a logging outage. Which action is MOST appropriate?,Write an incident report outlining root cause and remediation,Rebuild the entire VPC,Change all IAM policies to least privilege,Create a new KMS key,,,A,0,0,0,1,0,1,0,,,,,1.3,"Troubleshoot security monitoring, logging, and alerting",,1011,SCS-C03,AWS Security Specialist,,"Incident reports must clearly document what happened, why it happened, and what was done to fix it."
,,A DevOps manager requests communication to stakeholders after logs failed to deliver for several hours. What should the engineer provide?,"A summary of impact, resolution steps, and remaining risks",A new CloudFormation template,A purchasing request for more EC2 instances,A list of terminated spot instances,,,A,0,0,1,0,0,0,1,,,,,1.3,"Troubleshoot security monitoring, logging, and alerting",,1012,SCS-C03,AWS Security Specialist,,"Clear communication ensures stakeholders understand the outage impact, the fix, and any follow-up actions."
,,A security team notices that CloudWatch alarms for IAM activity have stopped triggering in a member account of an AWS Organization. The account recently onboarded a third-party SIEM ingestion Lambda that assumed a cross-account role with overly broad permissions. Which action will MOST likely restore correct monitoring without disrupting SIEM ingestion?,Reduce the SIEM Lambda’s IAM permissions by adding an IAM permissions boundary that explicitly denies iam:PutMetricFilter and iam:DeleteMetricFilter actions.,Apply a Service Control Policy (SCP) denying all Lambda updates in the member account to prevent further drift.,Enable AWS CloudTrail Insights in the account to automatically restore missing metric filters.,Migrate SIEM ingestion to an Organization-level delegated administrator to inherit centralized alarms.,,,A,0,0,1,0,0,0,1,,,,,1.3,"Troubleshoot security monitoring, logging, and alerting",,1013,SCS-C03,AWS Security Specialist,,"The Lambda’s excessive permissions likely modified or removed CloudWatch metric filters. A permissions boundary blocking filter changes restores guardrails without breaking the Lambda’s read-only ingestion needs. The SCP option is overly broad, CloudTrail Insights cannot restore filters, and migrating SIEM ingestion does not resolve the underlying permission risk."
,,A centralized logging account stops receiving VPC Flow Logs from one member account after an SCP update. The security team confirms the VPC Flow Logs are enabled. What is the MOST likely cause?,The SCP blocks the logs PutLogEvents action for CloudWatch Log Groups in the member account,The SCP blocks CreateLogGroup but not CreateLogStream,The SCP denies s3:ReplicateDelete on the destination bucket,The SCP denies ec2:CreateFlowLogs,,,A,0,1,,,,0,0,,,,,1.3,"Troubleshoot security monitoring, logging, and alerting",,1014,SCS-C03,AWS Security Specialist,,SCPs apply before IAM policies and can block PutLogEvents which prevents delivery to CloudWatch even when Flow Logs are enabled
,,"You enabled CloudTrail data events for several S3 buckets, but the logs never appear in the central S3 logging bucket. What is the MOST likely issue?",The central bucket lacks the required bucket policy allowing CloudTrail to write,The data events require CloudTrail Insights to be enabled,The S3 buckets must be encrypted with the same KMS key,CloudTrail can deliver data events only within the same Region,,,A,0,1,,,,0,0,,,,,1.3,"Troubleshoot security monitoring, logging, and alerting",,1015,SCS-C03,AWS Security Specialist,,Data event delivery requires a bucket policy explicitly allowing CloudTrail to write while Insights is optional and unrelated
,,Security Hub in the management account shows no findings from a specific member account. GuardDuty is enabled in that member account. What is the MOST likely root cause?,GuardDuty is not configured to send findings to the delegated administrator,Security Hub requires CloudTrail to be multi Region,The member account must have SCPs disabled,Macie must be enabled first,,,A,0,1,,,,0,0,,,,,1.3,"Troubleshoot security monitoring, logging, and alerting",,1016,SCS-C03,AWS Security Specialist,,GuardDuty must be configured to share findings with the delegated administrator or Security Hub cannot ingest them
,,A Lambda function running inside a VPC is not sending logs to CloudWatch. CloudWatch permissions are correct. What is the MOST likely issue?,The Lambda function lacks outbound access to the CloudWatch Logs endpoint,The VPC has DNS hostnames disabled,The Lambda has too little memory,The Lambda timeout is too short,,,A,0,1,,,,0,0,,,,,1.3,"Troubleshoot security monitoring, logging, and alerting",,1017,SCS-C03,AWS Security Specialist,,VPC Lambda requires either NAT Gateway or VPC endpoints to reach CloudWatch Logs otherwise logs never leave the subnet
,,A team reports missing WAF logs in their centralized S3 bucket. WAF logging is enabled. What is the MOST likely cause?,The S3 bucket policy does not allow delivery from the WAF service principal,WAF requires CloudTrail Insights to deliver logs,S3 versioning must be enabled for WAF logs,GuardDuty must be enabled in the same Region,,,A,0,1,,,,0,0,,,,,1.3,"Troubleshoot security monitoring, logging, and alerting",,1018,SCS-C03,AWS Security Specialist,,WAF uses a service principal that must be explicitly allowed in the bucket policy otherwise logs cannot be written
,,CloudWatch Agent on several EC2 instances stops sending logs after an OS patch. What is the MOST likely root cause?,The agent lost its IAM instance profile permissions during update,The OS patch disabled VPC DNS,A new SSH key was required,EBS encryption changed the instance metadata,,,A,0,1,,,,0,0,,,,,1.3,"Troubleshoot security monitoring, logging, and alerting",,1019,SCS-C03,AWS Security Specialist,,CloudWatch Agent requires IAM permissions which can be removed if the instance profile is modified during updates
,,Security Lake is missing CloudTrail logs but shows GuardDuty and VPC Flow Logs. What is the MOST likely cause?,CloudTrail event selectors exclude management events or data events,Security Lake cannot receive more than two log sources,AWS Config must be enabled first,Lake Formation is not authorized for Transport Layer Security,,,A,0,1,,,,0,0,,,,,1.3,"Troubleshoot security monitoring, logging, and alerting",,1020,SCS-C03,AWS Security Specialist,,Incorrect CloudTrail event selectors can prevent CloudTrail logs from being delivered even while other sources work normally
,,An organization enabled CloudTrail Insights but no anomalies appear. The environment generates significant API activity. What explains this?,CloudTrail Insights analyzes only management events and requires historical baseline,Insights requires VPC Flow Logs to be enabled,Insights needs Macie integration,CloudTrail needs S3 server access logging enabled first,,,A,0,1,,,,0,0,,,,,1.3,"Troubleshoot security monitoring, logging, and alerting",,1021,SCS-C03,AWS Security Specialist,,Insights detects unusual management event patterns after building a baseline so immediate anomalies may not appear
,,Security analysts report missing Route 53 Resolver query logs in S3. The logging configuration is correct. What is the MOST likely cause?,The VPC does not have DNS query logs enabled for the correct log destination,The S3 bucket must be in the same Region,Resolver logs require CloudFront,Resolver logs require DNSSEC,,,A,0,0,0,1,0,1,0,,,,,1.3,"Troubleshoot security monitoring, logging, and alerting",,1022,SCS-C03,AWS Security Specialist,,Resolver logs must be explicitly configured per VPC with the correct destination otherwise no logs will be sent
,,An API Gateway stage was recently updated and execution logs stopped appearing. What is the MOST likely reason?,The stage level CloudWatch Logs role ARN was removed during the update,X Ray must be enabled for logging,The API key rotation invalidated all logs,The resource policy blocked POST requests,,,A,0,1,,,,0,0,,,,,1.3,"Troubleshoot security monitoring, logging, and alerting",,1023,SCS-C03,AWS Security Specialist,,API Gateway requires a specific CloudWatch role ARN at the stage level which can be removed when redeploying stages
,,You are not receiving ELB access logs in a centralized S3 bucket. Logging is enabled on the ELB. What is the MOST likely cause?,The bucket policy does not allow the ELB service principal to write,The ELB is using TLS 1.3,The ELB is internal only,The ELB is cross zonal,,,A,0,1,,,,0,0,,,,,1.3,"Troubleshoot security monitoring, logging, and alerting",,1024,SCS-C03,AWS Security Specialist,,ELB access logging requires explicit bucket policy permissions for the ELB service principal to deliver log files
,,A GuardDuty finding references suspicious DNS activity but no corresponding VPC Flow Logs exist. What explains this?,Flow logs do not capture DNS queries across the Amazon Resolver,Flow logs are disabled by default for TGW,GuardDuty must be integrated with Inspector,Flow logs require CloudTrail data events,,,A,0,1,,,,0,0,,,,,1.3,"Troubleshoot security monitoring, logging, and alerting",,1025,SCS-C03,AWS Security Specialist,,VPC Flow Logs do not capture DNS traffic to the Amazon Resolver which can cause findings without matching flow records
,,CloudWatch Logs subscription filters fail to deliver logs to a Kinesis Firehose stream. What is the MOST likely issue?,The Firehose IAM role lacks PutRecordBatch permission,The log group retention is too long,The Firehose buffer interval is too short,KMS rotation disabled the stream,,,A,0,1,,,,0,0,,,,,1.3,"Troubleshoot security monitoring, logging, and alerting",,1026,SCS-C03,AWS Security Specialist,,Delivering logs from CloudWatch Logs to Firehose requires the Firehose role to have PutRecordBatch permissions
,,"After deploying a Config conformance pack, CloudWatch shows errors that remediation actions are failing. What is the MOST likely cause?",The remediation IAM role does not have permissions to modify affected resources,Macie must be enabled for remediation,Config requires S3 replication,The pack must be deployed in all Regions first,,,A,0,1,,,,0,0,,,,,1.3,"Troubleshoot security monitoring, logging, and alerting",,1027,SCS-C03,AWS Security Specialist,,Remediation actions run under a specific IAM role that must be allowed to update the target resource types
,,A Kinesis Firehose stream delivering CloudTrail logs to S3 shows frequent delivery failures. What should be checked FIRST?,Whether the S3 bucket policy allows the Firehose service principal,Whether CloudTrail Insights is enabled,Whether S3 versioning is enabled,Whether GuardDuty is enabled,,,A,0,1,,,,0,0,,,,,1.3,"Troubleshoot security monitoring, logging, and alerting",,1028,SCS-C03,AWS Security Specialist,,Firehose delivery failures often result from missing bucket policy permissions allowing the Firehose service to write to S3
,,Security Hub Insights dashboards show findings but QuickSight dashboards built on Athena show no data. What is the MOST likely issue?,Athena table partitions are not updated for new findings,The QuickSight user lacks IAM pass role permissions,QuickSight requires Macie,Athena requires data events,,,A,0,1,,,,0,0,,,,,1.3,"Troubleshoot security monitoring, logging, and alerting",,1029,SCS-C03,AWS Security Specialist,,When new findings arrive the underlying Athena tables require updated partitions otherwise queries return no results
,,Lambda based log processing fails after switching the destination bucket to SSE KMS encryption. What is the MOST likely reason?,The Lambda execution role lacks kms:Decrypt permission on the new key,The bucket must disable ACLs,Lambda requires VPC endpoints,The function must enable X Ray,,,A,0,1,,,,0,0,,,,,1.3,"Troubleshoot security monitoring, logging, and alerting",,1030,SCS-C03,AWS Security Specialist,,Reading or writing to an SSE KMS bucket requires the Lambda role to have kms:Decrypt and possibly kms:Encrypt permissions on the new key
,,A new Transit Gateway route table was added but Transit Gateway Flow Logs do not appear. What is the MOST likely explanation?,Flow Logs must be explicitly enabled for each Transit Gateway attachment,Flow Logs require GuardDuty,Flow Logs require CloudTrail Insights,The TGW must be peered,,,A,0,0,0,1,0,1,0,,,,,1.3,"Troubleshoot security monitoring, logging, and alerting",,1031,SCS-C03,AWS Security Specialist,,Transit Gateway Flow Logs are not global and must be enabled for each attachment where monitoring is required
,,CloudTrail suddenly stops delivering logs to an encrypted S3 bucket using SSE KMS. What is the MOST likely reason?,The KMS key policy was changed and no longer allows the CloudTrail service principal,CloudTrail requires CloudFront,CloudTrail cannot deliver to encrypted buckets,CloudTrail must use CloudWatch first,,,A,0,1,,,,0,0,,,,,1.3,"Troubleshoot security monitoring, logging, and alerting",,1032,SCS-C03,AWS Security Specialist,,If the key policy does not include CloudTrail the service cannot encrypt objects and delivery stops immediately
,,An organization sees Inspector findings in member accounts but none appear in the central Security Hub dashboard. What is the MOST likely reason?,Inspector is not configured to send findings to the delegated administrator account,Inspector requires KMS multi Region keys,Security Hub requires VPC Flow Logs,Security Hub requires S3 Object Lock,,,A,0,1,,,,0,0,,,,,1.3,"Troubleshoot security monitoring, logging, and alerting",,1033,SCS-C03,AWS Security Specialist,,Inspector findings must be forwarded to the Security Hub delegated administrator before they appear in the central dashboard